#pragma once
#include "../../dlc-intrinsics.h"
#include "../../typehint.h"

#ifndef _TANF_H_X86_
#define _TANF_H_X86_

#include "cosf.h"

inline float8_128 __dlc_tanf(float8_128 x)
{
    int8_128 y0, y1;
    int8_128 n = rem_pio2f(x, &y0, &y1);

    // if n is odd
    bool8_128 cmp_n = v_s32_cmp(EQ, n & v_u32_move_i(1), 1);
    
    bool8_128 cmp_x = v_f32_cmp(LS, _$F(_$S(x) & v_u32_move_i(0x7fffffff)), v_u32_move_b(0x3f490fdb));
    float8_128 f_y = v_f32_sel(cmp_x, _$F(y0), x);

    float8_128 f_y_3 = v_f32_mul_b(f_y, v_f32_mul_b(f_y, f_y));
    float8_128 f_y_5 = v_f32_mul_b(f_y_3, v_f32_mul_b(f_y, f_y));
    float8_128 f_y_7 = v_f32_mul_b(f_y_5, v_f32_mul_b(f_y, f_y));
    float8_128 f_y_9 = v_f32_mul_b(f_y_7, v_f32_mul_b(f_y, f_y));
    float8_128 f_y_11 = v_f32_mul_b(f_y_9, v_f32_mul_b(f_y, f_y));
    float8_128 f_y_13 = v_f32_mul_b(f_y_11, v_f32_mul_b(f_y, f_y));
    float8_128 f_y_15 = v_f32_mul_b(f_y_13, v_f32_mul_b(f_y, f_y));
    float8_128 f_y_17 = v_f32_mul_b(f_y_15, v_f32_mul_b(f_y, f_y));
    float8_128 f_y_19 = v_f32_mul_b(f_y_17, v_f32_mul_b(f_y, f_y));
    float8_128 f_y_21 = v_f32_mul_b(f_y_19, v_f32_mul_b(f_y, f_y));
    float8_128 f_y_23 = v_f32_mul_b(f_y_21, v_f32_mul_b(f_y, f_y));
    float8_128 f_y_25 = v_f32_mul_b(f_y_23, v_f32_mul_b(f_y, f_y));
    float8_128 f_y_27 = v_f32_mul_b(f_y_25, v_f32_mul_b(f_y, f_y));
    float8_128 res_taylor = f_y;
    res_taylor = v_f32_add_b(res_taylor, v_f32_mul_b(f_y_3, v_u32_move_b(0x3eaaaaab)));
    res_taylor = v_f32_add_b(res_taylor, v_f32_mul_b(f_y_5, v_u32_move_b(0x3e088889)));
    res_taylor = v_f32_add_b(res_taylor, v_f32_mul_b(f_y_7, v_u32_move_b(0x3d5d0dd1)));
    res_taylor = v_f32_add_b(res_taylor, v_f32_mul_b(f_y_9, v_u32_move_b(0x3cb327a4)));
    res_taylor = v_f32_add_b(res_taylor, v_f32_mul_b(f_y_11, v_u32_move_b(0x3c11371b)));
    res_taylor = v_f32_add_b(res_taylor, v_f32_mul_b(f_y_13, v_u32_move_b(0x3b6b69e8)));
    res_taylor = v_f32_add_b(res_taylor, v_f32_mul_b(f_y_15, v_u32_move_b(0x3abed1b2)));
    res_taylor = v_f32_add_b(res_taylor, v_f32_mul_b(f_y_17, v_u32_move_b(0x3a1aac12)));
    res_taylor = v_f32_add_b(res_taylor, v_f32_mul_b(f_y_19, v_u32_move_b(0x397abebc)));
    res_taylor = v_f32_add_b(res_taylor, v_f32_mul_b(f_y_21, v_u32_move_b(0x38cb3f0c)));
    res_taylor = v_f32_add_b(res_taylor, v_f32_mul_b(f_y_23, v_u32_move_b(0x3824bec7)));
    res_taylor = v_f32_add_b(res_taylor, v_f32_mul_b(f_y_25, v_u32_move_b(0x37858997)));
    res_taylor = v_f32_add_b(res_taylor, v_f32_mul_b(f_y_27, v_u32_move_b(0x36b87b98)));

    float8_128 res_taylor_rcp = v_f32_mul_b(v_u32_move_f(-1.0), __dlc_frcp_rd_without_unary(res_taylor));
    float8_128 res = v_f32_sel(cmp_n, res_taylor, res_taylor_rcp);

    bool8_128 mask = v_s32_cmp(LS, _$S(x) & v_u32_move_i(0x7fffffff), v_u32_move_i(0x39000000));
    res = v_f32_sel(mask, res, x);

    res = res + (v_u32_move_f(1.0) + res * res) * _$F(y1);
    
    bool8_128 infnan = v_f32_infnan(x);
    res = v_f32_sel(infnan, res, v_u32_move_b(0x7fc00000));
    return res;
}

inline float8_128 __dlc_tanf_libdevice(float8_128 a)
{
    float8_128 result0;
    asm (
        "{V0@(pr0)  vr10 = mov.u32 %[input0];}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr11;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 17136;"
        "V1@(pr0)	vmsk4 = ls.f32 vr2, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr10;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 29;"
        "V1@(pr0)	vr11 = shl.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 49152;"
        "pseudo@0	@pseudo imm_2 = 3;"
        "V0@(pr0)	vr1 = and.u32 vr10, r44;"
        "V1@(pr0)	vr2 = shr.u32 vr10, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 2047;"
        "V0@(pr0)	vr2 = and.u32 vr2, r44;"
        "V1@(pr0)	vr1 = or.u32 vr2, vr1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 2048;"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 14336;"
        "V0@(pr0)	vr2 = and.u32 vr10, r44;"
        "V1@(pr0)	vr3 = or.u32 vr1, r45;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = eq.s32 vr2, r46;"
        "V1@(pr0)	vr1 = sel vmsk1 vr3, vr1;"
        "}"
        "{"
        "V1@(pr0)	vr10 = mov.u32 vr1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr11;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 24368;"
        "pseudo@0	@pseudo imm_1 = 16740;"
        "pseudo@0	@pseudo imm_2 = 51331;"
        "pseudo@0	@pseudo imm_3 = 28105;"
        "V0@(pr0)	vr12 = mov.u32 r44;"
        "V1@(pr0)	vr13 = mov.u32 r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr13;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr1 = and.u32 vr10, r44;"
        "V1@(pr0)	vr2 = and.u32 vr12, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "V0@(pr0)	vmsk1 = ls.f32 vr1, r44;"
        "V1@(pr0)	vmsk2 = ls.f32 vr2, r44;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, r46;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, r46;"
        "}"
        "{"
        "V0@(pr0)	vr12 = sel vmsk2 vr12, r46;"
        "V1@(pr0)	vr13 = sel vmsk2 vr13, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr6 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr7 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 15;"
        "V0@(pr0)	vr10 = and.u32 vr10, r44;"
        "V1@(pr0)	vr12 = and.u32 vr12, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "V0@(pr0)	vr10 = or.u32 vr10, r44;"
        "V1@(pr0)	vr12 = or.u32 vr12, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 11;"
        "V1@(pr0)	vr12 = shl.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 21;"
        "V1@(pr0)	vr1 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 11;"
        "V0@(pr0)	vr12 = or.u32 vr12, vr1;"
        "V1@(pr0)	vr13 = shl.u32 vr13, r32;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr13, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = and.u32 vr12, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr13, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr12, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = and.u32 vr12, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr12, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 r46;"
        "V1@(pr0)	vr10 = mov.u32 vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr15 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr14 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr10;"
        "V1@(pr0)	vr17 = mov.u32 vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = or.u32 vr1, vr2;"
        "V1@(pr0)	vr14 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr15 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr14 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr15 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr14 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr12 = mov.u32 r46;"
        "V1@(pr0)	vr13 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "V1@(pr0)	vr15 = shr.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr15 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr14 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 vr15;"
        "V1@(pr0)	vr29 = mov.u32 vr14;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr15;"
        "V1@(pr0)	vr15 = mov.u32 vr0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr16;"
        "V1@(pr0)	vr11 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr12;"
        "V1@(pr0)	vr17 = mov.u32 vr13;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr29;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr15 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr14 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 vr15;"
        "V1@(pr0)	vr29 = mov.u32 vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr16;"
        "V1@(pr0)	vr13 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr10;"
        "V1@(pr0)	vr17 = mov.u32 vr11;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr28;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr16;"
        "V1@(pr0)	vr11 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr29 = shl.u32 vr29, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr15 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr14 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr12;"
        "V1@(pr0)	vr17 = mov.u32 vr13;"
        "}"
        "{"
        "V0@(pr0)	vr30 = mov.u32 vr14;"
        "V1@(pr0)	vr14 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr31 = mov.u32 vr17;"
        "V1@(pr0)	vr17 = mov.u32 vr16;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr11;"
        "V1@(pr0)	vr15 = mov.u32 vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr16;"
        "V1@(pr0)	vr11 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr13;"
        "V1@(pr0)	vr13 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr28;"
        "V1@(pr0)	vr17 = mov.u32 vr29;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr30;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr13;"
        "V1@(pr0)	vr15 = mov.u32 vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr13 = mov.u32 vr17;"
        "V1@(pr0)	vr12 = mov.u32 vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "V0@(pr0)	vr3 = xor.u32 vr6, vr7;"
        "V1@(pr0)	vr3 = and.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr3 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 2047;"
        "pseudo@0	@pseudo imm_3 = 0;"
        "V0@(pr0)	vr7 = and.u32 vr7, r45;"
        "V1@(pr0)	vr6 = and.u32 vr6, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 1023;"
        "pseudo@0	@pseudo imm_3 = 0;"
        "V0@(pr0)	vr6 = add.s32 vr6, vr7;"
        "V1@(pr0)	vr6 = sub.s32 vr6, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 16;"
        "V0@(pr0)	vr1 = and.u32 vr10, r45;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr6, r48;"
        "V1@(pr0)	vmsk1 = eq.s32 vr1, r46;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr2, vr6;"
        "}"
        "{"
        "V1@(pr0)	vr2 = shl.u32 vr10, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V1@(pr0)	vr5 = shr.u32 vr11, r32;"
        "}"
        "{"
        "V0@(pr0)	vr2 = or.u32 vr2, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V1@(pr0)	vr5 = shl.u32 vr13, r32;"
        "}"
        "{"
        "V1@(pr0)	vr4 = shl.u32 vr11, r48;"
        "}"
        "{"
        "V0@(pr0)	vr5 = or.u32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr2;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr5;"
        "}"
        "{"
        "V1@(pr0)	vr2 = shl.u32 vr12, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V1@(pr0)	vr5 = shr.u32 vr13, r32;"
        "}"
        "{"
        "V0@(pr0)	vr2 = or.u32 vr2, vr5;"
        "}"
        "{"
        "V1@(pr0)	vr5 = shl.u32 vr13, r48;"
        "}"
        "{"
        "V0@(pr0)	vr12 = sel vmsk1 vr12, vr2;"
        "V1@(pr0)	vr13 = sel vmsk1 vr13, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 15;"
        "V0@(pr0)	vr1 = and.u32 vr10, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr2 = shl.u32 vr6, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = gt.s32 vr6, r46;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr10, vr3;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32768;"
        "V0@(pr0)	vr1 = add.s32 vr11, r48;"
        "V1@(pr0)	vmsk1 = gt.s32 vr12, r44;"
        "}"
        "{"
        "V0@(pr0)	vr11 = sel vmsk1 vr11, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr1 = and.u32 vr11, r48;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr11;"
        "V1@(pr0)	vr15 = mov.u32 vr10;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr1;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32768;"
        "V1@(pr0)	vmsk1 = eq.s32 vr12, r45;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr16;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr17;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr28 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr29 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr30 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr31 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr1 = and.u32 vr28, r44;"
        "V1@(pr0)	vr2 = and.u32 vr30, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = lseq.s32 vr1, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 8;"
        "V1@(pr0)	vr1 = or.u32 vr28, r45;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr1, vr10;"
        "V1@(pr0)	vr11 = sel vmsk1 vr29, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = lseq.s32 vr2, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 8;"
        "V1@(pr0)	vr1 = or.u32 vr30, r45;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr1, vr10;"
        "V1@(pr0)	vr11 = sel vmsk1 vr31, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32768;"
        "V0@(pr0)	vr3 = xor.u32 vr28, vr30;"
        "V1@(pr0)	vr3 = and.u32 vr3, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = eq.f32 vr1, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32760;"
        "V0@(pr0)	vr5 = mov.u32 r44;"
        "}"
        "{"
        "V0@(pr0)	vr4 = xor.u32 vr1, vr3;"
        "}"
        "{"
        "V1@(pr0)	vmsk2 = eq.s32 vr2, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr5;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = eq.f32 vr2, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32760;"
        "V0@(pr0)	vr5 = mov.u32 r44;"
        "}"
        "{"
        "V0@(pr0)	vr4 = xor.u32 vr2, vr3;"
        "}"
        "{"
        "V1@(pr0)	vmsk2 = eq.s32 vr1, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr5;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr11;"
        "}"
        "{"
        "V1@(pr0)	vmsk1 = eq.f32 vr1, r46;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr3;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, r46;"
        "}"
        "{"
        "V1@(pr0)	vmsk1 = eq.f32 vr2, r46;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr3;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 49152;"
        "pseudo@0	@pseudo imm_2 = 3;"
        "V0@(pr0)	vr13 = and.u32 vr10, r44;"
        "V1@(pr0)	vr1 = shl.u32 vr10, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 16383;"
        "V0@(pr0)	vr1 = and.u32 vr1, r44;"
        "V1@(pr0)	vr13 = or.u32 vr1, vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 29;"
        "V1@(pr0)	vr1 = shr.u32 vr11, r32;"
        "}"
        "{"
        "V0@(pr0)	vr13 = or.u32 vr1, vr13;"
        "}"
        "{"
        "V0@(pr0)	vr13 = cvtftoint.s32 vr13, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 128;"
        "V0@(pr0)	vr13 = add.s32 vr13, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 24;"
        "V1@(pr0)	vr13 = shra.s32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr13;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr1 = cvtinttof.f32 vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 29;"
        "V1@(pr0)	vr11 = shl.u32 vr1, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 49152;"
        "pseudo@0	@pseudo imm_2 = 3;"
        "V0@(pr0)	vr10 = and.u32 vr1, r44;"
        "V1@(pr0)	vr2 = shr.u32 vr1, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 2047;"
        "V0@(pr0)	vr2 = and.u32 vr2, r44;"
        "V1@(pr0)	vr10 = or.u32 vr2, vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 2048;"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 14336;"
        "V0@(pr0)	vr2 = and.u32 vr1, r44;"
        "V1@(pr0)	vr3 = or.u32 vr10, r45;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = eq.s32 vr2, r46;"
        "V1@(pr0)	vr10 = sel vmsk1 vr3, vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8699;"
        "pseudo@0	@pseudo imm_1 = 16377;"
        "pseudo@0	@pseudo imm_2 = 11544;"
        "pseudo@0	@pseudo imm_3 = 21572;"
        "V0@(pr0)	vr12 = mov.u32 r44;"
        "V1@(pr0)	vr13 = mov.u32 r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr13;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr1 = and.u32 vr10, r44;"
        "V1@(pr0)	vr2 = and.u32 vr12, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "V0@(pr0)	vmsk1 = ls.f32 vr1, r44;"
        "V1@(pr0)	vmsk2 = ls.f32 vr2, r44;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, r46;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, r46;"
        "}"
        "{"
        "V0@(pr0)	vr12 = sel vmsk2 vr12, r46;"
        "V1@(pr0)	vr13 = sel vmsk2 vr13, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr6 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr7 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 15;"
        "V0@(pr0)	vr10 = and.u32 vr10, r44;"
        "V1@(pr0)	vr12 = and.u32 vr12, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "V0@(pr0)	vr10 = or.u32 vr10, r44;"
        "V1@(pr0)	vr12 = or.u32 vr12, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 11;"
        "V1@(pr0)	vr12 = shl.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 21;"
        "V1@(pr0)	vr1 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 11;"
        "V0@(pr0)	vr12 = or.u32 vr12, vr1;"
        "V1@(pr0)	vr13 = shl.u32 vr13, r32;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr13, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = and.u32 vr12, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr13, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr12, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = and.u32 vr12, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr12, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 r46;"
        "V1@(pr0)	vr10 = mov.u32 vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr15 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr14 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr10;"
        "V1@(pr0)	vr17 = mov.u32 vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = or.u32 vr1, vr2;"
        "V1@(pr0)	vr14 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr15 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr14 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr15 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr14 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr12 = mov.u32 r46;"
        "V1@(pr0)	vr13 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "V1@(pr0)	vr15 = shr.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr15 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr14 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 vr15;"
        "V1@(pr0)	vr29 = mov.u32 vr14;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr15;"
        "V1@(pr0)	vr15 = mov.u32 vr0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr16;"
        "V1@(pr0)	vr11 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr12;"
        "V1@(pr0)	vr17 = mov.u32 vr13;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr29;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr15 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr14 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 vr15;"
        "V1@(pr0)	vr29 = mov.u32 vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr16;"
        "V1@(pr0)	vr13 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr10;"
        "V1@(pr0)	vr17 = mov.u32 vr11;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr28;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr16;"
        "V1@(pr0)	vr11 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr29 = shl.u32 vr29, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr15 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr14 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr12;"
        "V1@(pr0)	vr17 = mov.u32 vr13;"
        "}"
        "{"
        "V0@(pr0)	vr30 = mov.u32 vr14;"
        "V1@(pr0)	vr14 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr31 = mov.u32 vr17;"
        "V1@(pr0)	vr17 = mov.u32 vr16;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr11;"
        "V1@(pr0)	vr15 = mov.u32 vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr16;"
        "V1@(pr0)	vr11 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr13;"
        "V1@(pr0)	vr13 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr28;"
        "V1@(pr0)	vr17 = mov.u32 vr29;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr30;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr13;"
        "V1@(pr0)	vr15 = mov.u32 vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr13 = mov.u32 vr17;"
        "V1@(pr0)	vr12 = mov.u32 vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "V0@(pr0)	vr3 = xor.u32 vr6, vr7;"
        "V1@(pr0)	vr3 = and.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr3 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 2047;"
        "pseudo@0	@pseudo imm_3 = 0;"
        "V0@(pr0)	vr7 = and.u32 vr7, r45;"
        "V1@(pr0)	vr6 = and.u32 vr6, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 1023;"
        "pseudo@0	@pseudo imm_3 = 0;"
        "V0@(pr0)	vr6 = add.s32 vr6, vr7;"
        "V1@(pr0)	vr6 = sub.s32 vr6, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 16;"
        "V0@(pr0)	vr1 = and.u32 vr10, r45;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr6, r48;"
        "V1@(pr0)	vmsk1 = eq.s32 vr1, r46;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr2, vr6;"
        "}"
        "{"
        "V1@(pr0)	vr2 = shl.u32 vr10, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V1@(pr0)	vr5 = shr.u32 vr11, r32;"
        "}"
        "{"
        "V0@(pr0)	vr2 = or.u32 vr2, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V1@(pr0)	vr5 = shl.u32 vr13, r32;"
        "}"
        "{"
        "V1@(pr0)	vr4 = shl.u32 vr11, r48;"
        "}"
        "{"
        "V0@(pr0)	vr5 = or.u32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr2;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr5;"
        "}"
        "{"
        "V1@(pr0)	vr2 = shl.u32 vr12, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V1@(pr0)	vr5 = shr.u32 vr13, r32;"
        "}"
        "{"
        "V0@(pr0)	vr2 = or.u32 vr2, vr5;"
        "}"
        "{"
        "V1@(pr0)	vr5 = shl.u32 vr13, r48;"
        "}"
        "{"
        "V0@(pr0)	vr12 = sel vmsk1 vr12, vr2;"
        "V1@(pr0)	vr13 = sel vmsk1 vr13, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 15;"
        "V0@(pr0)	vr1 = and.u32 vr10, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr2 = shl.u32 vr6, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = gt.s32 vr6, r46;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr10, vr3;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32768;"
        "V0@(pr0)	vr1 = add.s32 vr11, r48;"
        "V1@(pr0)	vmsk1 = gt.s32 vr12, r44;"
        "}"
        "{"
        "V0@(pr0)	vr11 = sel vmsk1 vr11, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr1 = and.u32 vr11, r48;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr11;"
        "V1@(pr0)	vr15 = mov.u32 vr10;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr1;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32768;"
        "V1@(pr0)	vmsk1 = eq.s32 vr12, r45;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr16;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr17;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr28 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr29 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr30 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr31 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr1 = and.u32 vr28, r44;"
        "V1@(pr0)	vr2 = and.u32 vr30, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = lseq.s32 vr1, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 8;"
        "V1@(pr0)	vr1 = or.u32 vr28, r45;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr1, vr10;"
        "V1@(pr0)	vr11 = sel vmsk1 vr29, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = lseq.s32 vr2, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 8;"
        "V1@(pr0)	vr1 = or.u32 vr30, r45;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr1, vr10;"
        "V1@(pr0)	vr11 = sel vmsk1 vr31, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32768;"
        "V0@(pr0)	vr3 = xor.u32 vr28, vr30;"
        "V1@(pr0)	vr3 = and.u32 vr3, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = eq.f32 vr1, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32760;"
        "V0@(pr0)	vr5 = mov.u32 r44;"
        "}"
        "{"
        "V0@(pr0)	vr4 = xor.u32 vr1, vr3;"
        "}"
        "{"
        "V1@(pr0)	vmsk2 = eq.s32 vr2, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr5;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = eq.f32 vr2, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32760;"
        "V0@(pr0)	vr5 = mov.u32 r44;"
        "}"
        "{"
        "V0@(pr0)	vr4 = xor.u32 vr2, vr3;"
        "}"
        "{"
        "V1@(pr0)	vmsk2 = eq.s32 vr1, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr5;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr11;"
        "}"
        "{"
        "V1@(pr0)	vmsk1 = eq.f32 vr1, r46;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr3;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, r46;"
        "}"
        "{"
        "V1@(pr0)	vmsk1 = eq.f32 vr2, r46;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr3;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32768;"
        "V0@(pr0)	vr12 = mov.u32 vr11;"
        "V1@(pr0)	vr13 = xor.u32 vr10, r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr1 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr11 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr10 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr1;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 32767;"
        "V0@(pr0)	vr14 = and.u32 vr10, r44;"
        "V1@(pr0)	vr15 = and.u32 vr11, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 32767;"
        "V0@(pr0)	vr16 = and.u32 vr12, r44;"
        "V1@(pr0)	vr17 = and.u32 vr13, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8192;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 5120;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr15;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 6144;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 7168;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = gt.s32 vr17, vr15;"
        "V1@(pr0)	vmsk1 = eq.s32 vr17, vr15;"
        "}"
        "{"
        "V0@(pr0)	vmsk2 = gt.s32 vr16, vr14;"
        "}"
        "{"
        "V0@(pr0)	vr1 = sel vmsk2 vr13, vr11;"
        "V1@(pr0)	vr0 = sel vmsk2 vr12, vr10;"
        "}"
        "{"
        "V0@(pr0)	vr1 = sel vmsk1 vr13, vr1;"
        "V1@(pr0)	vr0 = sel vmsk1 vr12, vr0;"
        "}"
        "{"
        "V0@(pr0)	vr1 = sel vmsk0 vr1, vr11;"
        "V1@(pr0)	vr0 = sel vmsk0 vr0, vr10;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = eq.s32 vr1, vr11;"
        "V1@(pr0)	vmsk1 = eq.s32 vr0, vr10;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr7 = sel vmsk0 vr5, r48;"
        "V1@(pr0)	vr6 = sel vmsk1 vr5, r48;"
        "}"
        "{"
        "V0@(pr0)	vr7 = and.u32 vr6, vr7;"
        "V1@(pr0)	vmsk0 = eq.s32 vr7, r48;"
        "}"
        "{"
        "V0@(pr0)	vr15 = sel vmsk0 vr11, vr13;"
        "V1@(pr0)	vr14 = sel vmsk0 vr10, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr1;"
        "V1@(pr0)	vr16 = mov.u32 vr0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2047;"
        "pseudo@0	@pseudo imm_1 = 20;"
        "V0@(pr0)	vr7 = mov.u32 r32;"
        "V1@(pr0)	vr6 = mov.u32 r33;"
        "}"
        "{"
        "V1@(pr0)	vr0 = shr.u32 vr15, vr6;"
        "}"
        "{"
        "V0@(pr0)	vr0 = and.u32 vr0, vr7;"
        "}"
        "{"
        "V1@(pr0)	vr1 = shr.u32 vr17, vr6;"
        "}"
        "{"
        "V0@(pr0)	vr1 = and.u32 vr1, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 15;"
        "V0@(pr0)	vr2 = and.u32 vr14, r44;"
        "V1@(pr0)	vr3 = and.u32 vr15, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 15;"
        "V0@(pr0)	vr4 = and.u32 vr16, r44;"
        "V1@(pr0)	vr5 = and.u32 vr17, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32768;"
        "V0@(pr0)	vr6 = and.u32 vr15, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32768;"
        "V0@(pr0)	vr7 = xor.u32 vr15, vr17;"
        "V1@(pr0)	vr7 = and.u32 vr7, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "pseudo@0	@pseudo imm_2 = 3;"
        "V0@(pr0)	vr3 = or.u32 vr3, r44;"
        "V1@(pr0)	vr3 = shl.u32 vr3, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 57344;"
        "pseudo@0	@pseudo imm_2 = 29;"
        "V0@(pr0)	vr28 = and.u32 vr2, r44;"
        "V1@(pr0)	vr28 = shr.u32 vr28, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3;"
        "V0@(pr0)	vr3 = or.u32 vr3, vr28;"
        "V1@(pr0)	vr2 = shl.u32 vr2, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "pseudo@0	@pseudo imm_2 = 3;"
        "V0@(pr0)	vr5 = or.u32 vr5, r44;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 57344;"
        "pseudo@0	@pseudo imm_2 = 29;"
        "V0@(pr0)	vr28 = and.u32 vr4, r44;"
        "V1@(pr0)	vr28 = shr.u32 vr28, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3;"
        "V0@(pr0)	vr5 = or.u32 vr5, vr28;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 64;"
        "V0@(pr0)	vr28 = sub.s32 vr0, vr1;"
        "V1@(pr0)	vr29 = mov.u32 r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr29 = sub.s32 vr29, vr28;"
        "V1@(pr0)	vmsk0 = gt.s32 vr29, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr31 = mov.u32 r32;"
        "V1@(pr0)	vr31 = sub.s32 vr31, vr29;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr30 = mov.u32 r44;"
        "}"
        "{"
        "V1@(pr0)	vr30 = shl.u32 vr30, vr31;"
        "}"
        "{"
        "V0@(pr0)	vr30 = and.u32 vr4, vr30;"
        "V1@(pr0)	vr30 = shr.u32 vr30, vr31;"
        "}"
        "{"
        "V1@(pr0)	vr11 = shl.u32 vr5, vr29;"
        "}"
        "{"
        "V0@(pr0)	vr11 = or.u32 vr11, vr30;"
        "V1@(pr0)	vr10 = shl.u32 vr4, vr29;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V1@(pr0)	vr30 = sub.s32 vr29, r32;"
        "}"
        "{"
        "V1@(pr0)	vr12 = shl.u32 vr4, vr30;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk0 vr10, r46;"
        "V1@(pr0)	vr11 = sel vmsk0 vr11, vr12;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = eq.s32 vr10, r46;"
        "V1@(pr0)	vmsk1 = eq.s32 vr11, r46;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 r46;"
        "V1@(pr0)	vr17 = sel vmsk0 vr11, r48;"
        "}"
        "{"
        "V0@(pr0)	vr16 = sel vmsk1 vr11, r48;"
        "V1@(pr0)	vr17 = add.s32 vr16, vr17;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vmsk0 = eq.s32 vr17, r32;"
        "V1@(pr0)	vr17 = sel vmsk0 vr11, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V1@(pr0)	vmsk0 = gt.s32 vr28, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 65535;"
        "V0@(pr0)	vr31 = mov.u32 r45;"
        "V1@(pr0)	vr30 = mov.u32 r32;"
        "}"
        "{"
        "V0@(pr0)	vr30 = sub.s32 vr30, vr28;"
        "V1@(pr0)	vr31 = shr.u32 vr31, vr30;"
        "}"
        "{"
        "V0@(pr0)	vr31 = and.u32 vr31, vr5;"
        "V1@(pr0)	vr31 = shl.u32 vr31, vr30;"
        "}"
        "{"
        "V1@(pr0)	vr10 = shr.u32 vr4, vr28;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr10, vr31;"
        "V1@(pr0)	vr11 = shr.u32 vr5, vr28;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr31 = sub.s32 vr28, r32;"
        "V1@(pr0)	vr12 = shr.u32 vr5, vr31;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk0 vr10, vr12;"
        "V1@(pr0)	vr11 = sel vmsk0 vr11, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 64;"
        "V0@(pr0)	vr10 = or.u32 vr10, vr17;"
        "V1@(pr0)	vmsk0 = ls.s32 vr28, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 r46;"
        "V1@(pr0)	vr11 = sel vmsk0 vr17, vr11;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 r48;"
        "V1@(pr0)	vr10 = sel vmsk0 vr17, vr10;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = eq.s32 vr28, r36;"
        "V1@(pr0)	vr4 = sel vmsk0 vr10, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "V0@(pr0)	vr5 = sel vmsk0 vr11, vr5;"
        "V1@(pr0)	vr10 = and.u32 vr2, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr12 = and.u32 vr4, r33;"
        "V1@(pr0)	vr11 = shr.u32 vr2, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr13 = shr.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = gteq.s32 vr10, vr12;"
        "V1@(pr0)	vr28 = mov.u32 r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr17 = sel vmsk0 vr28, r46;"
        "V1@(pr0)	vr17 = shl.u32 vr17, r32;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr10, vr17;"
        "V1@(pr0)	vr14 = sub.s32 vr10, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr15 = add.s32 vr13, r48;"
        "V1@(pr0)	vr13 = sel vmsk0 vr15, vr13;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = gteq.s32 vr11, vr13;"
        "V1@(pr0)	vr17 = sel vmsk0 vr28, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr17 = shl.u32 vr17, r32;"
        "}"
        "{"
        "V0@(pr0)	vr11 = or.u32 vr11, vr17;"
        "V1@(pr0)	vr15 = sub.s32 vr11, vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr14, vr15;"
        "}"
        "{"
        "V0@(pr0)	vr17 = add.s32 vr5, r48;"
        "V1@(pr0)	vr17 = sel vmsk0 vr17, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr11 = sub.s32 vr3, vr17;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = eq.s32 vr11, r46;"
        "V1@(pr0)	vr31 = count.u32 vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr30 = count.u32 vr10;"
        "V1@(pr0)	vr30 = add.s32 vr30, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr31 = sel vmsk0 vr31, vr30;"
        "V1@(pr0)	vr31 = sub.s32 vr31, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vmsk6 = ls.s32 vr31, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr30 = mov.u32 r32;"
        "V1@(pr0)	vr30 = sub.s32 vr30, vr31;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr29 = mov.u32 r44;"
        "V1@(pr0)	vr29 = shl.u32 vr29, vr30;"
        "}"
        "{"
        "V0@(pr0)	vr29 = and.u32 vr29, vr10;"
        "V1@(pr0)	vr29 = shr.u32 vr29, vr30;"
        "}"
        "{"
        "V1@(pr0)	vr13 = shl.u32 vr11, vr31;"
        "}"
        "{"
        "V0@(pr0)	vr13 = or.u32 vr13, vr29;"
        "V1@(pr0)	vr12 = shl.u32 vr10, vr31;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr30 = mov.u32 r32;"
        "V1@(pr0)	vr30 = sub.s32 vr31, vr30;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr14 = shl.u32 vr10, vr30;"
        "}"
        "{"
        "V0@(pr0)	vr12 = sel vmsk6 vr15, vr12;"
        "V1@(pr0)	vr13 = sel vmsk6 vr14, vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 128;"
        "V0@(pr0)	vmsk0 = ls.s32 vr11, r36;"
        "V1@(pr0)	vr10 = sel vmsk0 vr10, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr15 = sub.s32 vr0, vr31;"
        "}"
        "{"
        "V0@(pr0)	vr11 = sel vmsk0 vr11, vr13;"
        "V1@(pr0)	vr15 = sel vmsk0 vr0, vr15;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr12 = and.u32 vr2, r33;"
        "V1@(pr0)	vr13 = shr.u32 vr2, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr16 = and.u32 vr4, r33;"
        "V1@(pr0)	vr17 = shr.u32 vr4, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr28 = add.s32 vr12, vr16;"
        "V1@(pr0)	vr14 = shr.u32 vr28, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "V0@(pr0)	vr28 = and.u32 vr28, r32;"
        "}"
        "{"
        "V0@(pr0)	vr29 = add.s32 vr13, vr17;"
        "V1@(pr0)	vr29 = add.s32 vr29, vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr29, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "V0@(pr0)	vr29 = and.u32 vr29, r32;"
        "V1@(pr0)	vr29 = shl.u32 vr29, r33;"
        "}"
        "{"
        "V0@(pr0)	vr12 = or.u32 vr29, vr28;"
        "V1@(pr0)	vr13 = add.s32 vr3, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr13 = add.s32 vr13, vr14;"
        "V1@(pr0)	vr28 = and.u32 vr12, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V0@(pr0)	vr14 = and.u32 vr13, r48;"
        "V1@(pr0)	vr14 = shl.u32 vr14, r32;"
        "}"
        "{"
        "V1@(pr0)	vr16 = shr.u32 vr12, r48;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr16, vr14;"
        "V1@(pr0)	vr17 = shr.u32 vr13, r48;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr16, vr28;"
        "V1@(pr0)	vr14 = add.s32 vr0, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 256;"
        "V0@(pr0)	vr28 = and.u32 vr13, r36;"
        "V1@(pr0)	vmsk0 = eq.s32 vr28, r46;"
        "}"
        "{"
        "V0@(pr0)	vr12 = sel vmsk0 vr16, vr12;"
        "V1@(pr0)	vr13 = sel vmsk0 vr17, vr13;"
        "}"
        "{"
        "V0@(pr0)	vr14 = sel vmsk0 vr14, vr0;"
        "V1@(pr0)	vmsk1 = eq.s32 vr7, r46;"
        "}"
        "{"
        "V0@(pr0)	vr2 = sel vmsk1 vr10, vr12;"
        "V1@(pr0)	vr3 = sel vmsk1 vr11, vr13;"
        "}"
        "{"
        "V0@(pr0)	vr0 = sel vmsk1 vr15, vr14;"
        "V1@(pr0)	vr31 = mov.u32 r48;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr2, r48;"
        "V1@(pr0)	vmsk0 = lseq.s32 vr0, r46;"
        "}"
        "{"
        "V0@(pr0)	vr2 = sel vmsk0 vr2, vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 7;"
        "pseudo@0	@pseudo imm_1 = 3;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 65535;"
        "V0@(pr0)	vr17 = and.u32 vr2, r32;"
        "V1@(pr0)	vr31 = mov.u32 r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 29;"
        "V1@(pr0)	vr31 = shr.u32 vr31, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 29;"
        "V0@(pr0)	vr31 = and.u32 vr3, vr31;"
        "V1@(pr0)	vr31 = shl.u32 vr31, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3;"
        "V1@(pr0)	vr10 = shr.u32 vr2, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3;"
        "V0@(pr0)	vr10 = or.u32 vr10, vr31;"
        "V1@(pr0)	vr11 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 15;"
        "V0@(pr0)	vr10 = and.u32 vr10, r44;"
        "V1@(pr0)	vr11 = and.u32 vr11, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr28 = shl.u32 vr0, r32;"
        "}"
        "{"
        "V0@(pr0)	vr11 = or.u32 vr11, vr28;"
        "V1@(pr0)	vr11 = or.u32 vr11, vr6;"
        "}"
        "{"
        "V0@(pr0)	vr12 = add.s32 vr10, r48;"
        "V1@(pr0)	vr13 = and.u32 vr10, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "V0@(pr0)	vmsk0 = gt.s32 vr17, r32;"
        "V1@(pr0)	vmsk1 = eq.s32 vr17, r32;"
        "}"
        "{"
        "V0@(pr0)	vr13 = add.s32 vr10, vr13;"
        "V1@(pr0)	vr13 = sel vmsk1 vr10, vr13;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk0 vr13, vr12;"
        "V1@(pr0)	vr0 = mov.u32 vr10;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mov.u32 vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr10 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr11 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr12 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr13 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr14 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 5120;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr15 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 6144;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr16 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 7168;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr17 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8192;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = eq.s32 vr17, r46;"
        "V1@(pr0)	vmsk1 = eq.s32 vr16, r46;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 r46;"
        "V1@(pr0)	vr7 = sel vmsk0 vr28, r48;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr28, r48;"
        "V1@(pr0)	vr7 = add.s32 vr6, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vmsk0 = eq.s32 vr7, r32;"
        "V1@(pr0)	vr30 = sel vmsk0 vr0, vr10;"
        "}"
        "{"
        "V0@(pr0)	vr31 = sel vmsk0 vr1, vr11;"
        "V1@(pr0)	vr6 = and.u32 vr10, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr7 = and.u32 vr11, vr13;"
        "V1@(pr0)	vr5 = sel vmsk0 vr13, vr7;"
        "}"
        "{"
        "V0@(pr0)	vr4 = sel vmsk0 vr12, vr6;"
        "V1@(pr0)	vmsk0 = eq.s32 vr14, r46;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = eq.s32 vr15, r46;"
        "V1@(pr0)	vr6 = sel vmsk0 vr28, r48;"
        "}"
        "{"
        "V0@(pr0)	vr7 = sel vmsk1 vr28, r48;"
        "V1@(pr0)	vr7 = add.s32 vr6, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vmsk0 = eq.s32 vr7, r32;"
        "V1@(pr0)	vr31 = sel vmsk0 vr31, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr30 = sel vmsk0 vr30, r34;"
        "V1@(pr0)	vr7 = mov.u32 r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 15;"
        "V0@(pr0)	vr6 = mov.u32 r44;"
        "V1@(pr0)	vr7 = xor.u32 vr7, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr6 = xor.u32 vr6, r44;"
        "V1@(pr0)	vmsk0 = eq.s32 vr16, vr6;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mov.u32 vr7;"
        "V1@(pr0)	vr4 = mov.u32 vr6;"
        "}"
        "{"
        "V0@(pr0)	vmsk6 = eq.s32 vr14, vr6;"
        "V1@(pr0)	vmsk7 = eq.s32 vr15, vr7;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = eq.s32 vr17, vr7;"
        "V1@(pr0)	vr7 = sel vmsk0 vr28, r48;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr28, r48;"
        "V1@(pr0)	vr7 = add.s32 vr6, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vmsk0 = eq.s32 vr7, r32;"
        "V1@(pr0)	vr31 = sel vmsk0 vr31, vr13;"
        "}"
        "{"
        "V0@(pr0)	vr30 = sel vmsk0 vr30, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr7 = sel vmsk7 vr28, r48;"
        "V1@(pr0)	vr6 = sel vmsk6 vr28, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vr7 = add.s32 vr6, vr7;"
        "V1@(pr0)	vmsk7 = eq.s32 vr7, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr3 = or.u32 vr5, r36;"
        "V1@(pr0)	vr6 = xor.u32 vr10, vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32768;"
        "V0@(pr0)	vr7 = xor.u32 vr11, vr13;"
        "V1@(pr0)	vmsk0 = eq.s32 vr7, r36;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = eq.s32 vr6, r46;"
        "V1@(pr0)	vr7 = sel vmsk0 vr28, r48;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr28, r48;"
        "V1@(pr0)	vr7 = add.s32 vr6, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vmsk0 = eq.s32 vr7, r32;"
        "V1@(pr0)	vr7 = sel vmsk0 vr11, vr3;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk0 vr10, vr4;"
        "V1@(pr0)	vr30 = sel vmsk7 vr30, vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr31 = sel vmsk7 vr31, vr7;"
        "V1@(pr0)	vr7 = or.u32 vr13, r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk0 = eq.s32 vr16, r46;"
        "V1@(pr0)	vr6 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "V0@(pr0)	vr28 = and.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32752;"
        "V0@(pr0)	vmsk1 = gt.s32 vr6, r32;"
        "V1@(pr0)	vmsk2 = eq.s32 vr28, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk0 vr28, r48;"
        "V1@(pr0)	vr4 = sel vmsk2 vr28, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vr5 = add.s32 vr4, vr5;"
        "V1@(pr0)	vmsk2 = eq.s32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr7, vr31;"
        "V1@(pr0)	vr4 = sel vmsk2 vr12, vr30;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32752;"
        "V0@(pr0)	vmsk2 = eq.s32 vr6, r32;"
        "V1@(pr0)	vr4 = sel vmsk2 vr30, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr31, vr5;"
        "V1@(pr0)	vr31 = sel vmsk1 vr5, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr30 = sel vmsk1 vr4, vr12;"
        "V1@(pr0)	vr7 = or.u32 vr11, r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk0 = eq.s32 vr14, r46;"
        "V1@(pr0)	vr6 = shr.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "V0@(pr0)	vr28 = and.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32752;"
        "V0@(pr0)	vmsk1 = gt.s32 vr6, r32;"
        "V1@(pr0)	vmsk2 = eq.s32 vr28, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk0 vr28, r48;"
        "V1@(pr0)	vr4 = sel vmsk2 vr28, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vr5 = add.s32 vr4, vr5;"
        "V1@(pr0)	vmsk2 = eq.s32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr7, vr31;"
        "V1@(pr0)	vr4 = sel vmsk2 vr10, vr30;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32752;"
        "V0@(pr0)	vmsk2 = eq.s32 vr6, r32;"
        "V1@(pr0)	vr4 = sel vmsk2 vr30, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr31, vr5;"
        "V1@(pr0)	vr11 = sel vmsk1 vr5, vr7;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr4, vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr12 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr1 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V1@(pr0)	vr2 = shr.u32 vr1, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 5120;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr12;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 26;"
        "V1@(pr0)	vr1 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 15;"
        "V1@(pr0)	vr1 = and.u32 vr1, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 162;"
        "pseudo@0	@pseudo imm_3 = 0;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 28238;"
        "pseudo@0	@pseudo imm_1 = 63875;"
        "pseudo@0	@pseudo imm_2 = 10748;"
        "pseudo@0	@pseudo imm_3 = 17429;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "pseudo@0	@pseudo imm_2 = 41721;"
        "pseudo@0	@pseudo imm_3 = 0;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20036;"
        "pseudo@0	@pseudo imm_1 = 33646;"
        "pseudo@0	@pseudo imm_2 = 64551;"
        "pseudo@0	@pseudo imm_3 = 5417;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "pseudo@0	@pseudo imm_2 = 63875;"
        "pseudo@0	@pseudo imm_3 = 162;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 17429;"
        "pseudo@0	@pseudo imm_1 = 28238;"
        "pseudo@0	@pseudo imm_2 = 10071;"
        "pseudo@0	@pseudo imm_3 = 10748;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3;"
        "pseudo@0	@pseudo imm_2 = 33646;"
        "pseudo@0	@pseudo imm_3 = 41721;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 5417;"
        "pseudo@0	@pseudo imm_1 = 20036;"
        "pseudo@0	@pseudo imm_2 = 22481;"
        "pseudo@0	@pseudo imm_3 = 64551;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "pseudo@0	@pseudo imm_2 = 28238;"
        "pseudo@0	@pseudo imm_3 = 63875;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 10748;"
        "pseudo@0	@pseudo imm_1 = 17429;"
        "pseudo@0	@pseudo imm_2 = 53749;"
        "pseudo@0	@pseudo imm_3 = 10071;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 5;"
        "pseudo@0	@pseudo imm_2 = 20036;"
        "pseudo@0	@pseudo imm_3 = 33646;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 64551;"
        "pseudo@0	@pseudo imm_1 = 5417;"
        "pseudo@0	@pseudo imm_2 = 62772;"
        "pseudo@0	@pseudo imm_3 = 22481;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 6;"
        "pseudo@0	@pseudo imm_2 = 17429;"
        "pseudo@0	@pseudo imm_3 = 28238;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 10071;"
        "pseudo@0	@pseudo imm_1 = 10748;"
        "pseudo@0	@pseudo imm_2 = 13533;"
        "pseudo@0	@pseudo imm_3 = 53749;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 7;"
        "pseudo@0	@pseudo imm_2 = 5417;"
        "pseudo@0	@pseudo imm_3 = 20036;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 22481;"
        "pseudo@0	@pseudo imm_1 = 64551;"
        "pseudo@0	@pseudo imm_2 = 56768;"
        "pseudo@0	@pseudo imm_3 = 62772;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "pseudo@0	@pseudo imm_2 = 10748;"
        "pseudo@0	@pseudo imm_3 = 17429;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 53749;"
        "pseudo@0	@pseudo imm_1 = 10071;"
        "pseudo@0	@pseudo imm_2 = 49371;"
        "pseudo@0	@pseudo imm_3 = 13533;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 9;"
        "pseudo@0	@pseudo imm_2 = 64551;"
        "pseudo@0	@pseudo imm_3 = 5417;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 62772;"
        "pseudo@0	@pseudo imm_1 = 22481;"
        "pseudo@0	@pseudo imm_2 = 56162;"
        "pseudo@0	@pseudo imm_3 = 56768;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 10;"
        "pseudo@0	@pseudo imm_2 = 10071;"
        "pseudo@0	@pseudo imm_3 = 10748;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 13533;"
        "pseudo@0	@pseudo imm_1 = 53749;"
        "pseudo@0	@pseudo imm_2 = 25237;"
        "pseudo@0	@pseudo imm_3 = 49371;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 11;"
        "pseudo@0	@pseudo imm_2 = 22481;"
        "pseudo@0	@pseudo imm_3 = 64551;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 56768;"
        "pseudo@0	@pseudo imm_1 = 62772;"
        "pseudo@0	@pseudo imm_2 = 38297;"
        "pseudo@0	@pseudo imm_3 = 56162;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 12;"
        "pseudo@0	@pseudo imm_2 = 53749;"
        "pseudo@0	@pseudo imm_3 = 10071;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 49371;"
        "pseudo@0	@pseudo imm_1 = 13533;"
        "pseudo@0	@pseudo imm_2 = 39228;"
        "pseudo@0	@pseudo imm_3 = 25237;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 13;"
        "pseudo@0	@pseudo imm_2 = 62772;"
        "pseudo@0	@pseudo imm_3 = 22481;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 56162;"
        "pseudo@0	@pseudo imm_1 = 56768;"
        "pseudo@0	@pseudo imm_2 = 15427;"
        "pseudo@0	@pseudo imm_3 = 38297;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 14;"
        "pseudo@0	@pseudo imm_2 = 13533;"
        "pseudo@0	@pseudo imm_3 = 53749;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 25237;"
        "pseudo@0	@pseudo imm_1 = 49371;"
        "pseudo@0	@pseudo imm_2 = 17296;"
        "pseudo@0	@pseudo imm_3 = 39228;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 23;"
        "V1@(pr0)	vr1 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 7;"
        "V1@(pr0)	vr1 = and.u32 vr1, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 255;"
        "pseudo@0	@pseudo imm_2 = 128;"
        "V0@(pr0)	vr10 = and.u32 vr10, r44;"
        "V1@(pr0)	vr10 = or.u32 vr10, r38;"
        "}"
        "{"
        "V1@(pr0)	vr10 = shl.u32 vr10, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr10;"
        "V1@(pr0)	vr13 = mov.u32 vr28;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr14;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr12, r54;"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr12, r32;"
        "}"
        "{"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr13, r32;"
        "}"
        "{"
        "V1@(pr0)	vr15 = and.u32 vr12, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr16;"
        "V1@(pr0)	vr13 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 vr13;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr10;"
        "V1@(pr0)	vr13 = mov.u32 vr30;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr14;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr12, r54;"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr12, r32;"
        "}"
        "{"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr13, r32;"
        "}"
        "{"
        "V1@(pr0)	vr15 = and.u32 vr12, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr16;"
        "V1@(pr0)	vr13 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr30 = mov.u32 vr12;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr10;"
        "V1@(pr0)	vr13 = mov.u32 vr29;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr14;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr12, r54;"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr12, r32;"
        "}"
        "{"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr13, r32;"
        "}"
        "{"
        "V1@(pr0)	vr15 = and.u32 vr12, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr16;"
        "V1@(pr0)	vr13 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr12;"
        "V1@(pr0)	vr17 = mov.u32 vr13;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 vr28;"
        "V1@(pr0)	vr14 = mov.u32 vr30;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8192;"
        "V0@(pr0)	vr1 = add.s32 vr2, r32;"
        "V1@(pr0)	vr1 = and.u32 vr1, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 14;"
        "V1@(pr0)	vr1 = shr.u32 vr1, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 14;"
        "V1@(pr0)	vr3 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V1@(pr0)	vr2 = sub.s32 vr2, vr3;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr16 = and.u32 vr16, r54;"
        "V1@(pr0)	vr2 = shl.u32 vr2, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr16, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr1;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32768;"
        "V0@(pr0)	vr12 = mov.u32 r44;"
        "V1@(pr0)	vmsk7 = gteq.f32 vr16, r46;"
        "}"
        "{"
        "V1@(pr0)	vr12 = sel vmsk7 vr12, r46;"
        "}"
        "{"
        "V0@(pr0)	vmsk3 = eq.s32 vr17, r46;"
        "V1@(pr0)	vmsk2 = eq.s32 vr16, r46;"
        "}"
        "{"
        "V0@(pr0)	vr6 = mov.u32 r46;"
        "V1@(pr0)	vr6 = sel vmsk2 vr6, r48;"
        "}"
        "{"
        "V0@(pr0)	vr7 = add.s32 vr6, r48;"
        "V1@(pr0)	vr6 = sel vmsk3 vr6, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V1@(pr0)	vmsk2 = eq.s32 vr6, r32;"
        "}"
        "{"
        "V1@(pr0)	vr6 = sub.s32 vr17, r48;"
        "}"
        "{"
        "V1@(pr0)	vr7 = sub.s32 vr16, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr6 = sel vmsk3 vr6, r44;"
        "V1@(pr0)	vr7 = sel vmsk3 vr16, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr6 = xor.u32 vr6, r44;"
        "V1@(pr0)	vr7 = xor.u32 vr7, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr17 = sel vmsk7 vr6, vr17;"
        "V1@(pr0)	vr16 = sel vmsk7 vr7, vr16;"
        "}"
        "{"
        "V0@(pr0)	vr2 = count.u32 vr16;"
        "V1@(pr0)	vr1 = count.u32 vr17;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vmsk6 = ls.s32 vr2, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V0@(pr0)	vr6 = mov.u32 r32;"
        "V1@(pr0)	vr1 = sub.s32 vr6, vr1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 63;"
        "V0@(pr0)	vr6 = mov.u32 r32;"
        "V1@(pr0)	vr2 = sub.s32 vr6, vr2;"
        "}"
        "{"
        "V1@(pr0)	vr13 = sel vmsk6 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr0 = mov.u32 vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1023;"
        "V0@(pr0)	vr13 = add.s32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr13 = shl.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 20;"
        "V0@(pr0)	vmsk5 = lseq.s32 vr0, r32;"
        "V1@(pr0)	vmsk4 = lseq.s32 vr0, r33;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr6 = sub.s32 vr0, r32;"
        "V1@(pr0)	vr7 = mov.u32 r48;"
        "}"
        "{"
        "V1@(pr0)	vr6 = shl.u32 vr7, vr6;"
        "}"
        "{"
        "V1@(pr0)	vr6 = sub.s32 vr16, vr6;"
        "}"
        "{"
        "V1@(pr0)	vr7 = shl.u32 vr7, vr0;"
        "}"
        "{"
        "V1@(pr0)	vr7 = sub.s32 vr17, vr7;"
        "}"
        "{"
        "V1@(pr0)	vr5 = sel vmsk4 vr6, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "V0@(pr0)	vr6 = sub.s32 vr0, r32;"
        "V1@(pr0)	vr3 = shr.u32 vr5, vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 84;"
        "V0@(pr0)	vr6 = mov.u32 r32;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sub.s32 vr6, vr0;"
        "V1@(pr0)	vr4 = shl.u32 vr5, vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "V0@(pr0)	vr6 = sub.s32 vr0, r32;"
        "V1@(pr0)	vr6 = shr.u32 vr17, vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V0@(pr0)	vr4 = add.s32 vr4, vr6;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 vr3;"
        "V1@(pr0)	vr14 = mov.u32 vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "V0@(pr0)	vr6 = mov.u32 r32;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sub.s32 vr6, vr0;"
        "V1@(pr0)	vr3 = shl.u32 vr5, vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V0@(pr0)	vr6 = sub.s32 vr0, r32;"
        "V1@(pr0)	vr6 = shr.u32 vr17, vr6;"
        "}"
        "{"
        "V1@(pr0)	vr3 = add.s32 vr3, vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "V0@(pr0)	vr7 = mov.u32 r32;"
        "}"
        "{"
        "V0@(pr0)	vr7 = sub.s32 vr7, vr0;"
        "V1@(pr0)	vr4 = shl.u32 vr17, vr7;"
        "}"
        "{"
        "V0@(pr0)	vr15 = sel vmsk5 vr15, vr3;"
        "V1@(pr0)	vr14 = sel vmsk5 vr14, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V0@(pr0)	vr6 = mov.u32 r32;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sub.s32 vr6, vr0;"
        "V1@(pr0)	vr3 = shl.u32 vr5, vr6;"
        "}"
        "{"
        "V0@(pr0)	vr15 = sel vmsk4 vr15, vr3;"
        "V1@(pr0)	vr14 = sel vmsk4 vr14, r46;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr12, vr13;"
        "V1@(pr0)	vr10 = or.u32 vr10, vr15;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 vr14;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk2 vr10, r46;"
        "V1@(pr0)	vr11 = sel vmsk2 vr11, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8699;"
        "pseudo@0	@pseudo imm_1 = 15385;"
        "pseudo@0	@pseudo imm_2 = 11544;"
        "pseudo@0	@pseudo imm_3 = 21572;"
        "V0@(pr0)	vr12 = mov.u32 r44;"
        "V1@(pr0)	vr13 = mov.u32 r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr13;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr1 = and.u32 vr10, r44;"
        "V1@(pr0)	vr2 = and.u32 vr12, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "V0@(pr0)	vmsk1 = ls.f32 vr1, r44;"
        "V1@(pr0)	vmsk2 = ls.f32 vr2, r44;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, r46;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, r46;"
        "}"
        "{"
        "V0@(pr0)	vr12 = sel vmsk2 vr12, r46;"
        "V1@(pr0)	vr13 = sel vmsk2 vr13, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr6 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr7 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 15;"
        "V0@(pr0)	vr10 = and.u32 vr10, r44;"
        "V1@(pr0)	vr12 = and.u32 vr12, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "V0@(pr0)	vr10 = or.u32 vr10, r44;"
        "V1@(pr0)	vr12 = or.u32 vr12, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 11;"
        "V1@(pr0)	vr12 = shl.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 21;"
        "V1@(pr0)	vr1 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 11;"
        "V0@(pr0)	vr12 = or.u32 vr12, vr1;"
        "V1@(pr0)	vr13 = shl.u32 vr13, r32;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr13, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = and.u32 vr12, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr13, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr12, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = and.u32 vr12, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr12, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 r46;"
        "V1@(pr0)	vr10 = mov.u32 vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr15 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr14 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr10;"
        "V1@(pr0)	vr17 = mov.u32 vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = or.u32 vr1, vr2;"
        "V1@(pr0)	vr14 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr15 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr14 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr15 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr14 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr12 = mov.u32 r46;"
        "V1@(pr0)	vr13 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "V1@(pr0)	vr15 = shr.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr15 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr14 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 vr15;"
        "V1@(pr0)	vr29 = mov.u32 vr14;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr15;"
        "V1@(pr0)	vr15 = mov.u32 vr0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr16;"
        "V1@(pr0)	vr11 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr12;"
        "V1@(pr0)	vr17 = mov.u32 vr13;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr29;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr15 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr14 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 vr15;"
        "V1@(pr0)	vr29 = mov.u32 vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr16;"
        "V1@(pr0)	vr13 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr10;"
        "V1@(pr0)	vr17 = mov.u32 vr11;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr28;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr16;"
        "V1@(pr0)	vr11 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr29 = shl.u32 vr29, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr15 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr14 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr12;"
        "V1@(pr0)	vr17 = mov.u32 vr13;"
        "}"
        "{"
        "V0@(pr0)	vr30 = mov.u32 vr14;"
        "V1@(pr0)	vr14 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr31 = mov.u32 vr17;"
        "V1@(pr0)	vr17 = mov.u32 vr16;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr11;"
        "V1@(pr0)	vr15 = mov.u32 vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr16;"
        "V1@(pr0)	vr11 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr13;"
        "V1@(pr0)	vr13 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr28;"
        "V1@(pr0)	vr17 = mov.u32 vr29;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr30;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr13;"
        "V1@(pr0)	vr15 = mov.u32 vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr13 = mov.u32 vr17;"
        "V1@(pr0)	vr12 = mov.u32 vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "V0@(pr0)	vr3 = xor.u32 vr6, vr7;"
        "V1@(pr0)	vr3 = and.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr3 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 2047;"
        "pseudo@0	@pseudo imm_3 = 0;"
        "V0@(pr0)	vr7 = and.u32 vr7, r45;"
        "V1@(pr0)	vr6 = and.u32 vr6, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 1023;"
        "pseudo@0	@pseudo imm_3 = 0;"
        "V0@(pr0)	vr6 = add.s32 vr6, vr7;"
        "V1@(pr0)	vr6 = sub.s32 vr6, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 16;"
        "V0@(pr0)	vr1 = and.u32 vr10, r45;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr6, r48;"
        "V1@(pr0)	vmsk1 = eq.s32 vr1, r46;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr2, vr6;"
        "}"
        "{"
        "V1@(pr0)	vr2 = shl.u32 vr10, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V1@(pr0)	vr5 = shr.u32 vr11, r32;"
        "}"
        "{"
        "V0@(pr0)	vr2 = or.u32 vr2, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V1@(pr0)	vr5 = shl.u32 vr13, r32;"
        "}"
        "{"
        "V1@(pr0)	vr4 = shl.u32 vr11, r48;"
        "}"
        "{"
        "V0@(pr0)	vr5 = or.u32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr2;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr5;"
        "}"
        "{"
        "V1@(pr0)	vr2 = shl.u32 vr12, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V1@(pr0)	vr5 = shr.u32 vr13, r32;"
        "}"
        "{"
        "V0@(pr0)	vr2 = or.u32 vr2, vr5;"
        "}"
        "{"
        "V1@(pr0)	vr5 = shl.u32 vr13, r48;"
        "}"
        "{"
        "V0@(pr0)	vr12 = sel vmsk1 vr12, vr2;"
        "V1@(pr0)	vr13 = sel vmsk1 vr13, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 15;"
        "V0@(pr0)	vr1 = and.u32 vr10, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr2 = shl.u32 vr6, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = gt.s32 vr6, r46;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr10, vr3;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32768;"
        "V0@(pr0)	vr1 = add.s32 vr11, r48;"
        "V1@(pr0)	vmsk1 = gt.s32 vr12, r44;"
        "}"
        "{"
        "V0@(pr0)	vr11 = sel vmsk1 vr11, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr1 = and.u32 vr11, r48;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr11;"
        "V1@(pr0)	vr15 = mov.u32 vr10;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr1;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32768;"
        "V1@(pr0)	vmsk1 = eq.s32 vr12, r45;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr16;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr17;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr28 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr29 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr30 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr31 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr1 = and.u32 vr28, r44;"
        "V1@(pr0)	vr2 = and.u32 vr30, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = lseq.s32 vr1, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 8;"
        "V1@(pr0)	vr1 = or.u32 vr28, r45;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr1, vr10;"
        "V1@(pr0)	vr11 = sel vmsk1 vr29, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = lseq.s32 vr2, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 8;"
        "V1@(pr0)	vr1 = or.u32 vr30, r45;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr1, vr10;"
        "V1@(pr0)	vr11 = sel vmsk1 vr31, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32768;"
        "V0@(pr0)	vr3 = xor.u32 vr28, vr30;"
        "V1@(pr0)	vr3 = and.u32 vr3, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = eq.f32 vr1, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32760;"
        "V0@(pr0)	vr5 = mov.u32 r44;"
        "}"
        "{"
        "V0@(pr0)	vr4 = xor.u32 vr1, vr3;"
        "}"
        "{"
        "V1@(pr0)	vmsk2 = eq.s32 vr2, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr5;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = eq.f32 vr2, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32760;"
        "V0@(pr0)	vr5 = mov.u32 r44;"
        "}"
        "{"
        "V0@(pr0)	vr4 = xor.u32 vr2, vr3;"
        "}"
        "{"
        "V1@(pr0)	vmsk2 = eq.s32 vr1, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr5;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr11;"
        "}"
        "{"
        "V1@(pr0)	vmsk1 = eq.f32 vr1, r46;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr3;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, r46;"
        "}"
        "{"
        "V1@(pr0)	vmsk1 = eq.f32 vr2, r46;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr3;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr12 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr5 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr4 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr1 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr2 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr3 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 5120;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32768;"
        "V0@(pr0)	vmsk1 = eq.s32 vr4, r48;"
        "V1@(pr0)	vr6 = xor.u32 vr10, r36;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 17136;"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 20224;"
        "V0@(pr0)	vr6 = and.u32 vr5, r45;"
        "V1@(pr0)	vmsk4 = ls.f32 vr6, r44;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk4 vr10, vr2;"
        "V1@(pr0)	vr11 = sel vmsk4 vr11, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr13 = sel vmsk4 vr12, vr3;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 49152;"
        "pseudo@0	@pseudo imm_2 = 3;"
        "V0@(pr0)	vr4 = and.u32 vr10, r44;"
        "V1@(pr0)	vr1 = shl.u32 vr10, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 16383;"
        "V0@(pr0)	vr1 = and.u32 vr1, r44;"
        "V1@(pr0)	vr4 = or.u32 vr1, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 29;"
        "V1@(pr0)	vr1 = shr.u32 vr11, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = or.u32 vr1, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr13;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 29;"
        "V1@(pr0)	vr12 = shl.u32 vr4, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 49152;"
        "pseudo@0	@pseudo imm_2 = 3;"
        "V0@(pr0)	vr6 = and.u32 vr4, r44;"
        "V1@(pr0)	vr2 = shr.u32 vr4, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 2047;"
        "V0@(pr0)	vr2 = and.u32 vr2, r44;"
        "V1@(pr0)	vr6 = or.u32 vr2, vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 2048;"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 14336;"
        "V0@(pr0)	vr2 = and.u32 vr4, r44;"
        "V1@(pr0)	vr3 = or.u32 vr6, r45;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = eq.s32 vr2, r46;"
        "V1@(pr0)	vr6 = sel vmsk1 vr3, vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32768;"
        "V1@(pr0)	vr13 = xor.u32 vr6, r36;"
        "}"
        "{"
        "V0@(pr0)	vr2 = mov.u32 vr10;"
        "V1@(pr0)	vr10 = mov.u32 vr11;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 32767;"
        "V0@(pr0)	vr14 = and.u32 vr10, r44;"
        "V1@(pr0)	vr15 = and.u32 vr11, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 32767;"
        "V0@(pr0)	vr16 = and.u32 vr12, r44;"
        "V1@(pr0)	vr17 = and.u32 vr13, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8192;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 5120;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr15;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 6144;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 7168;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = gt.s32 vr17, vr15;"
        "V1@(pr0)	vmsk1 = eq.s32 vr17, vr15;"
        "}"
        "{"
        "V0@(pr0)	vmsk2 = gt.s32 vr16, vr14;"
        "}"
        "{"
        "V0@(pr0)	vr1 = sel vmsk2 vr13, vr11;"
        "V1@(pr0)	vr0 = sel vmsk2 vr12, vr10;"
        "}"
        "{"
        "V0@(pr0)	vr1 = sel vmsk1 vr13, vr1;"
        "V1@(pr0)	vr0 = sel vmsk1 vr12, vr0;"
        "}"
        "{"
        "V0@(pr0)	vr1 = sel vmsk0 vr1, vr11;"
        "V1@(pr0)	vr0 = sel vmsk0 vr0, vr10;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = eq.s32 vr1, vr11;"
        "V1@(pr0)	vmsk1 = eq.s32 vr0, vr10;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr7 = sel vmsk0 vr5, r48;"
        "V1@(pr0)	vr6 = sel vmsk1 vr5, r48;"
        "}"
        "{"
        "V0@(pr0)	vr7 = and.u32 vr6, vr7;"
        "V1@(pr0)	vmsk0 = eq.s32 vr7, r48;"
        "}"
        "{"
        "V0@(pr0)	vr15 = sel vmsk0 vr11, vr13;"
        "V1@(pr0)	vr14 = sel vmsk0 vr10, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr1;"
        "V1@(pr0)	vr16 = mov.u32 vr0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2047;"
        "pseudo@0	@pseudo imm_1 = 20;"
        "V0@(pr0)	vr7 = mov.u32 r32;"
        "V1@(pr0)	vr6 = mov.u32 r33;"
        "}"
        "{"
        "V1@(pr0)	vr0 = shr.u32 vr15, vr6;"
        "}"
        "{"
        "V0@(pr0)	vr0 = and.u32 vr0, vr7;"
        "}"
        "{"
        "V1@(pr0)	vr1 = shr.u32 vr17, vr6;"
        "}"
        "{"
        "V0@(pr0)	vr1 = and.u32 vr1, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 15;"
        "V0@(pr0)	vr2 = and.u32 vr14, r44;"
        "V1@(pr0)	vr3 = and.u32 vr15, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 15;"
        "V0@(pr0)	vr4 = and.u32 vr16, r44;"
        "V1@(pr0)	vr5 = and.u32 vr17, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32768;"
        "V0@(pr0)	vr6 = and.u32 vr15, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32768;"
        "V0@(pr0)	vr7 = xor.u32 vr15, vr17;"
        "V1@(pr0)	vr7 = and.u32 vr7, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "pseudo@0	@pseudo imm_2 = 3;"
        "V0@(pr0)	vr3 = or.u32 vr3, r44;"
        "V1@(pr0)	vr3 = shl.u32 vr3, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 57344;"
        "pseudo@0	@pseudo imm_2 = 29;"
        "V0@(pr0)	vr28 = and.u32 vr2, r44;"
        "V1@(pr0)	vr28 = shr.u32 vr28, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3;"
        "V0@(pr0)	vr3 = or.u32 vr3, vr28;"
        "V1@(pr0)	vr2 = shl.u32 vr2, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "pseudo@0	@pseudo imm_2 = 3;"
        "V0@(pr0)	vr5 = or.u32 vr5, r44;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 57344;"
        "pseudo@0	@pseudo imm_2 = 29;"
        "V0@(pr0)	vr28 = and.u32 vr4, r44;"
        "V1@(pr0)	vr28 = shr.u32 vr28, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3;"
        "V0@(pr0)	vr5 = or.u32 vr5, vr28;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 64;"
        "V0@(pr0)	vr28 = sub.s32 vr0, vr1;"
        "V1@(pr0)	vr29 = mov.u32 r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr29 = sub.s32 vr29, vr28;"
        "V1@(pr0)	vmsk0 = gt.s32 vr29, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr31 = mov.u32 r32;"
        "V1@(pr0)	vr31 = sub.s32 vr31, vr29;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr30 = mov.u32 r44;"
        "}"
        "{"
        "V1@(pr0)	vr30 = shl.u32 vr30, vr31;"
        "}"
        "{"
        "V0@(pr0)	vr30 = and.u32 vr4, vr30;"
        "V1@(pr0)	vr30 = shr.u32 vr30, vr31;"
        "}"
        "{"
        "V1@(pr0)	vr11 = shl.u32 vr5, vr29;"
        "}"
        "{"
        "V0@(pr0)	vr11 = or.u32 vr11, vr30;"
        "V1@(pr0)	vr10 = shl.u32 vr4, vr29;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V1@(pr0)	vr30 = sub.s32 vr29, r32;"
        "}"
        "{"
        "V1@(pr0)	vr12 = shl.u32 vr4, vr30;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk0 vr10, r46;"
        "V1@(pr0)	vr11 = sel vmsk0 vr11, vr12;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = eq.s32 vr10, r46;"
        "V1@(pr0)	vmsk1 = eq.s32 vr11, r46;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 r46;"
        "V1@(pr0)	vr17 = sel vmsk0 vr11, r48;"
        "}"
        "{"
        "V0@(pr0)	vr16 = sel vmsk1 vr11, r48;"
        "V1@(pr0)	vr17 = add.s32 vr16, vr17;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vmsk0 = eq.s32 vr17, r32;"
        "V1@(pr0)	vr17 = sel vmsk0 vr11, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V1@(pr0)	vmsk0 = gt.s32 vr28, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 65535;"
        "V0@(pr0)	vr31 = mov.u32 r45;"
        "V1@(pr0)	vr30 = mov.u32 r32;"
        "}"
        "{"
        "V0@(pr0)	vr30 = sub.s32 vr30, vr28;"
        "V1@(pr0)	vr31 = shr.u32 vr31, vr30;"
        "}"
        "{"
        "V0@(pr0)	vr31 = and.u32 vr31, vr5;"
        "V1@(pr0)	vr31 = shl.u32 vr31, vr30;"
        "}"
        "{"
        "V1@(pr0)	vr10 = shr.u32 vr4, vr28;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr10, vr31;"
        "V1@(pr0)	vr11 = shr.u32 vr5, vr28;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr31 = sub.s32 vr28, r32;"
        "V1@(pr0)	vr12 = shr.u32 vr5, vr31;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk0 vr10, vr12;"
        "V1@(pr0)	vr11 = sel vmsk0 vr11, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 64;"
        "V0@(pr0)	vr10 = or.u32 vr10, vr17;"
        "V1@(pr0)	vmsk0 = ls.s32 vr28, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 r46;"
        "V1@(pr0)	vr11 = sel vmsk0 vr17, vr11;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 r48;"
        "V1@(pr0)	vr10 = sel vmsk0 vr17, vr10;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = eq.s32 vr28, r36;"
        "V1@(pr0)	vr4 = sel vmsk0 vr10, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "V0@(pr0)	vr5 = sel vmsk0 vr11, vr5;"
        "V1@(pr0)	vr10 = and.u32 vr2, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr12 = and.u32 vr4, r33;"
        "V1@(pr0)	vr11 = shr.u32 vr2, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr13 = shr.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = gteq.s32 vr10, vr12;"
        "V1@(pr0)	vr28 = mov.u32 r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr17 = sel vmsk0 vr28, r46;"
        "V1@(pr0)	vr17 = shl.u32 vr17, r32;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr10, vr17;"
        "V1@(pr0)	vr14 = sub.s32 vr10, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr15 = add.s32 vr13, r48;"
        "V1@(pr0)	vr13 = sel vmsk0 vr15, vr13;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = gteq.s32 vr11, vr13;"
        "V1@(pr0)	vr17 = sel vmsk0 vr28, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr17 = shl.u32 vr17, r32;"
        "}"
        "{"
        "V0@(pr0)	vr11 = or.u32 vr11, vr17;"
        "V1@(pr0)	vr15 = sub.s32 vr11, vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr14, vr15;"
        "}"
        "{"
        "V0@(pr0)	vr17 = add.s32 vr5, r48;"
        "V1@(pr0)	vr17 = sel vmsk0 vr17, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr11 = sub.s32 vr3, vr17;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = eq.s32 vr11, r46;"
        "V1@(pr0)	vr31 = count.u32 vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr30 = count.u32 vr10;"
        "V1@(pr0)	vr30 = add.s32 vr30, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr31 = sel vmsk0 vr31, vr30;"
        "V1@(pr0)	vr31 = sub.s32 vr31, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vmsk6 = ls.s32 vr31, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr30 = mov.u32 r32;"
        "V1@(pr0)	vr30 = sub.s32 vr30, vr31;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr29 = mov.u32 r44;"
        "V1@(pr0)	vr29 = shl.u32 vr29, vr30;"
        "}"
        "{"
        "V0@(pr0)	vr29 = and.u32 vr29, vr10;"
        "V1@(pr0)	vr29 = shr.u32 vr29, vr30;"
        "}"
        "{"
        "V1@(pr0)	vr13 = shl.u32 vr11, vr31;"
        "}"
        "{"
        "V0@(pr0)	vr13 = or.u32 vr13, vr29;"
        "V1@(pr0)	vr12 = shl.u32 vr10, vr31;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr30 = mov.u32 r32;"
        "V1@(pr0)	vr30 = sub.s32 vr31, vr30;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr14 = shl.u32 vr10, vr30;"
        "}"
        "{"
        "V0@(pr0)	vr12 = sel vmsk6 vr15, vr12;"
        "V1@(pr0)	vr13 = sel vmsk6 vr14, vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 128;"
        "V0@(pr0)	vmsk0 = ls.s32 vr11, r36;"
        "V1@(pr0)	vr10 = sel vmsk0 vr10, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr15 = sub.s32 vr0, vr31;"
        "}"
        "{"
        "V0@(pr0)	vr11 = sel vmsk0 vr11, vr13;"
        "V1@(pr0)	vr15 = sel vmsk0 vr0, vr15;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr12 = and.u32 vr2, r33;"
        "V1@(pr0)	vr13 = shr.u32 vr2, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr16 = and.u32 vr4, r33;"
        "V1@(pr0)	vr17 = shr.u32 vr4, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr28 = add.s32 vr12, vr16;"
        "V1@(pr0)	vr14 = shr.u32 vr28, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "V0@(pr0)	vr28 = and.u32 vr28, r32;"
        "}"
        "{"
        "V0@(pr0)	vr29 = add.s32 vr13, vr17;"
        "V1@(pr0)	vr29 = add.s32 vr29, vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr29, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "V0@(pr0)	vr29 = and.u32 vr29, r32;"
        "V1@(pr0)	vr29 = shl.u32 vr29, r33;"
        "}"
        "{"
        "V0@(pr0)	vr12 = or.u32 vr29, vr28;"
        "V1@(pr0)	vr13 = add.s32 vr3, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr13 = add.s32 vr13, vr14;"
        "V1@(pr0)	vr28 = and.u32 vr12, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V0@(pr0)	vr14 = and.u32 vr13, r48;"
        "V1@(pr0)	vr14 = shl.u32 vr14, r32;"
        "}"
        "{"
        "V1@(pr0)	vr16 = shr.u32 vr12, r48;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr16, vr14;"
        "V1@(pr0)	vr17 = shr.u32 vr13, r48;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr16, vr28;"
        "V1@(pr0)	vr14 = add.s32 vr0, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 256;"
        "V0@(pr0)	vr28 = and.u32 vr13, r36;"
        "V1@(pr0)	vmsk0 = eq.s32 vr28, r46;"
        "}"
        "{"
        "V0@(pr0)	vr12 = sel vmsk0 vr16, vr12;"
        "V1@(pr0)	vr13 = sel vmsk0 vr17, vr13;"
        "}"
        "{"
        "V0@(pr0)	vr14 = sel vmsk0 vr14, vr0;"
        "V1@(pr0)	vmsk1 = eq.s32 vr7, r46;"
        "}"
        "{"
        "V0@(pr0)	vr2 = sel vmsk1 vr10, vr12;"
        "V1@(pr0)	vr3 = sel vmsk1 vr11, vr13;"
        "}"
        "{"
        "V0@(pr0)	vr0 = sel vmsk1 vr15, vr14;"
        "V1@(pr0)	vr31 = mov.u32 r48;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr2, r48;"
        "V1@(pr0)	vmsk0 = lseq.s32 vr0, r46;"
        "}"
        "{"
        "V0@(pr0)	vr2 = sel vmsk0 vr2, vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 7;"
        "pseudo@0	@pseudo imm_1 = 3;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 65535;"
        "V0@(pr0)	vr17 = and.u32 vr2, r32;"
        "V1@(pr0)	vr31 = mov.u32 r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 29;"
        "V1@(pr0)	vr31 = shr.u32 vr31, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 29;"
        "V0@(pr0)	vr31 = and.u32 vr3, vr31;"
        "V1@(pr0)	vr31 = shl.u32 vr31, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3;"
        "V1@(pr0)	vr10 = shr.u32 vr2, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3;"
        "V0@(pr0)	vr10 = or.u32 vr10, vr31;"
        "V1@(pr0)	vr11 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 15;"
        "V0@(pr0)	vr10 = and.u32 vr10, r44;"
        "V1@(pr0)	vr11 = and.u32 vr11, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr28 = shl.u32 vr0, r32;"
        "}"
        "{"
        "V0@(pr0)	vr11 = or.u32 vr11, vr28;"
        "V1@(pr0)	vr11 = or.u32 vr11, vr6;"
        "}"
        "{"
        "V0@(pr0)	vr12 = add.s32 vr10, r48;"
        "V1@(pr0)	vr13 = and.u32 vr10, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "V0@(pr0)	vmsk0 = gt.s32 vr17, r32;"
        "V1@(pr0)	vmsk1 = eq.s32 vr17, r32;"
        "}"
        "{"
        "V0@(pr0)	vr13 = add.s32 vr10, vr13;"
        "V1@(pr0)	vr13 = sel vmsk1 vr10, vr13;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk0 vr13, vr12;"
        "V1@(pr0)	vr0 = mov.u32 vr10;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mov.u32 vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr10 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr11 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr12 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr13 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr14 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 5120;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr15 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 6144;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr16 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 7168;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr17 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8192;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = eq.s32 vr17, r46;"
        "V1@(pr0)	vmsk1 = eq.s32 vr16, r46;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 r46;"
        "V1@(pr0)	vr7 = sel vmsk0 vr28, r48;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr28, r48;"
        "V1@(pr0)	vr7 = add.s32 vr6, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vmsk0 = eq.s32 vr7, r32;"
        "V1@(pr0)	vr30 = sel vmsk0 vr0, vr10;"
        "}"
        "{"
        "V0@(pr0)	vr31 = sel vmsk0 vr1, vr11;"
        "V1@(pr0)	vr6 = and.u32 vr10, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr7 = and.u32 vr11, vr13;"
        "V1@(pr0)	vr5 = sel vmsk0 vr13, vr7;"
        "}"
        "{"
        "V0@(pr0)	vr4 = sel vmsk0 vr12, vr6;"
        "V1@(pr0)	vmsk0 = eq.s32 vr14, r46;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = eq.s32 vr15, r46;"
        "V1@(pr0)	vr6 = sel vmsk0 vr28, r48;"
        "}"
        "{"
        "V0@(pr0)	vr7 = sel vmsk1 vr28, r48;"
        "V1@(pr0)	vr7 = add.s32 vr6, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vmsk0 = eq.s32 vr7, r32;"
        "V1@(pr0)	vr31 = sel vmsk0 vr31, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr30 = sel vmsk0 vr30, r34;"
        "V1@(pr0)	vr7 = mov.u32 r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 15;"
        "V0@(pr0)	vr6 = mov.u32 r44;"
        "V1@(pr0)	vr7 = xor.u32 vr7, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr6 = xor.u32 vr6, r44;"
        "V1@(pr0)	vmsk0 = eq.s32 vr16, vr6;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mov.u32 vr7;"
        "V1@(pr0)	vr4 = mov.u32 vr6;"
        "}"
        "{"
        "V0@(pr0)	vmsk6 = eq.s32 vr14, vr6;"
        "V1@(pr0)	vmsk7 = eq.s32 vr15, vr7;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = eq.s32 vr17, vr7;"
        "V1@(pr0)	vr7 = sel vmsk0 vr28, r48;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr28, r48;"
        "V1@(pr0)	vr7 = add.s32 vr6, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vmsk0 = eq.s32 vr7, r32;"
        "V1@(pr0)	vr31 = sel vmsk0 vr31, vr13;"
        "}"
        "{"
        "V0@(pr0)	vr30 = sel vmsk0 vr30, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr7 = sel vmsk7 vr28, r48;"
        "V1@(pr0)	vr6 = sel vmsk6 vr28, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vr7 = add.s32 vr6, vr7;"
        "V1@(pr0)	vmsk7 = eq.s32 vr7, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr3 = or.u32 vr5, r36;"
        "V1@(pr0)	vr6 = xor.u32 vr10, vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32768;"
        "V0@(pr0)	vr7 = xor.u32 vr11, vr13;"
        "V1@(pr0)	vmsk0 = eq.s32 vr7, r36;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = eq.s32 vr6, r46;"
        "V1@(pr0)	vr7 = sel vmsk0 vr28, r48;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr28, r48;"
        "V1@(pr0)	vr7 = add.s32 vr6, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vmsk0 = eq.s32 vr7, r32;"
        "V1@(pr0)	vr7 = sel vmsk0 vr11, vr3;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk0 vr10, vr4;"
        "V1@(pr0)	vr30 = sel vmsk7 vr30, vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr31 = sel vmsk7 vr31, vr7;"
        "V1@(pr0)	vr7 = or.u32 vr13, r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk0 = eq.s32 vr16, r46;"
        "V1@(pr0)	vr6 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "V0@(pr0)	vr28 = and.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32752;"
        "V0@(pr0)	vmsk1 = gt.s32 vr6, r32;"
        "V1@(pr0)	vmsk2 = eq.s32 vr28, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk0 vr28, r48;"
        "V1@(pr0)	vr4 = sel vmsk2 vr28, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vr5 = add.s32 vr4, vr5;"
        "V1@(pr0)	vmsk2 = eq.s32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr7, vr31;"
        "V1@(pr0)	vr4 = sel vmsk2 vr12, vr30;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32752;"
        "V0@(pr0)	vmsk2 = eq.s32 vr6, r32;"
        "V1@(pr0)	vr4 = sel vmsk2 vr30, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr31, vr5;"
        "V1@(pr0)	vr31 = sel vmsk1 vr5, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr30 = sel vmsk1 vr4, vr12;"
        "V1@(pr0)	vr7 = or.u32 vr11, r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk0 = eq.s32 vr14, r46;"
        "V1@(pr0)	vr6 = shr.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "V0@(pr0)	vr28 = and.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32752;"
        "V0@(pr0)	vmsk1 = gt.s32 vr6, r32;"
        "V1@(pr0)	vmsk2 = eq.s32 vr28, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk0 vr28, r48;"
        "V1@(pr0)	vr4 = sel vmsk2 vr28, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vr5 = add.s32 vr4, vr5;"
        "V1@(pr0)	vmsk2 = eq.s32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr7, vr31;"
        "V1@(pr0)	vr4 = sel vmsk2 vr10, vr30;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32752;"
        "V0@(pr0)	vmsk2 = eq.s32 vr6, r32;"
        "V1@(pr0)	vr4 = sel vmsk2 vr30, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr31, vr5;"
        "V1@(pr0)	vr11 = sel vmsk1 vr5, vr7;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr4, vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 49152;"
        "pseudo@0	@pseudo imm_2 = 3;"
        "V0@(pr0)	vr5 = and.u32 vr11, r44;"
        "V1@(pr0)	vr1 = shl.u32 vr11, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 16383;"
        "V0@(pr0)	vr1 = and.u32 vr1, r44;"
        "V1@(pr0)	vr5 = or.u32 vr1, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 29;"
        "V1@(pr0)	vr1 = shr.u32 vr10, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = or.u32 vr1, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr4 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr13 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr11 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr13, r48;"
        "V1@(pr0)	vmsk7 = eq.s32 vr14, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "pseudo@0	@pseudo imm_2 = 4059;"
        "pseudo@0	@pseudo imm_3 = 16201;"
        "V0@(pr0)	vr17 = and.u32 vr11, r44;"
        "V1@(pr0)	vmsk6 = ls.f32 vr17, r45;"
        "}"
        "{"
        "V1@(pr0)	vr12 = sel vmsk6 vr12, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 14592;"
        "V0@(pr0)	vr17 = and.u32 vr12, r44;"
        "V1@(pr0)	vmsk5 = ls.f32 vr17, r45;"
        "}"
        "{"
        "V0@(pr0)	vr13 = mul.f32 vr12, vr12;"
        "V1@(pr0)	vr16 = mov.u32 vr12;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mul.f32 vr12, vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 43691;"
        "pseudo@0	@pseudo imm_1 = 16042;"
        "V0@(pr0)	vr15 = mul.f32 vr14, r44;"
        "V1@(pr0)	vr16 = add.f32 vr16, vr15;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mul.f32 vr14, vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 34953;"
        "pseudo@0	@pseudo imm_1 = 15880;"
        "V0@(pr0)	vr15 = mul.f32 vr14, r44;"
        "V1@(pr0)	vr16 = add.f32 vr16, vr15;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mul.f32 vr14, vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3537;"
        "pseudo@0	@pseudo imm_1 = 15709;"
        "V0@(pr0)	vr15 = mul.f32 vr14, r44;"
        "V1@(pr0)	vr16 = add.f32 vr16, vr15;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mul.f32 vr14, vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 10148;"
        "pseudo@0	@pseudo imm_1 = 15539;"
        "V0@(pr0)	vr15 = mul.f32 vr14, r44;"
        "V1@(pr0)	vr16 = add.f32 vr16, vr15;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mul.f32 vr14, vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 14107;"
        "pseudo@0	@pseudo imm_1 = 15377;"
        "V0@(pr0)	vr15 = mul.f32 vr14, r44;"
        "V1@(pr0)	vr16 = add.f32 vr16, vr15;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mul.f32 vr14, vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 27112;"
        "pseudo@0	@pseudo imm_1 = 15211;"
        "V0@(pr0)	vr15 = mul.f32 vr14, r44;"
        "V1@(pr0)	vr16 = add.f32 vr16, vr15;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mul.f32 vr14, vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 53682;"
        "pseudo@0	@pseudo imm_1 = 15038;"
        "V0@(pr0)	vr15 = mul.f32 vr14, r44;"
        "V1@(pr0)	vr16 = add.f32 vr16, vr15;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mul.f32 vr14, vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 44050;"
        "pseudo@0	@pseudo imm_1 = 14874;"
        "V0@(pr0)	vr15 = mul.f32 vr14, r44;"
        "V1@(pr0)	vr16 = add.f32 vr16, vr15;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mul.f32 vr14, vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48828;"
        "pseudo@0	@pseudo imm_1 = 14714;"
        "V0@(pr0)	vr15 = mul.f32 vr14, r44;"
        "V1@(pr0)	vr16 = add.f32 vr16, vr15;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mul.f32 vr14, vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16140;"
        "pseudo@0	@pseudo imm_1 = 14539;"
        "V0@(pr0)	vr15 = mul.f32 vr14, r44;"
        "V1@(pr0)	vr16 = add.f32 vr16, vr15;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mul.f32 vr14, vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48839;"
        "pseudo@0	@pseudo imm_1 = 14372;"
        "V0@(pr0)	vr15 = mul.f32 vr14, r44;"
        "V1@(pr0)	vr16 = add.f32 vr16, vr15;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mul.f32 vr14, vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 35223;"
        "pseudo@0	@pseudo imm_1 = 14213;"
        "V0@(pr0)	vr15 = mul.f32 vr14, r44;"
        "V1@(pr0)	vr16 = add.f32 vr16, vr15;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mul.f32 vr14, vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31640;"
        "pseudo@0	@pseudo imm_1 = 14040;"
        "V0@(pr0)	vr15 = mul.f32 vr14, r44;"
        "V1@(pr0)	vr16 = add.f32 vr16, vr15;"
        "}"
        "{"
        "V1@(pr0)	vr16 = sel vmsk5 vr16, vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 7168;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 5120;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr15;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 6144;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr16;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32640;"
        "V0@(pr0)	vr0 = and.u32 vr10, r44;"
        "V1@(pr0)	vmsk1 = eq.s32 vr0, r45;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr0, r50;"
        "V1@(pr0)	vr2 = mov.u32 vr0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 23007;"
        "pseudo@0	@pseudo imm_1 = 24375;"
        "V0@(pr0)	vr3 = mov.u32 r44;"
        "V1@(pr0)	vr2 = shr.u32 vr2, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16320;"
        "V0@(pr0)	vr2 = sub.s32 vr3, vr2;"
        "V1@(pr0)	vr4 = mov.u32 r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32768;"
        "V0@(pr0)	vr3 = mul.f32 vr2, vr2;"
        "V1@(pr0)	vr6 = and.u32 vr10, r36;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr1, vr3;"
        "V1@(pr0)	vr5 = sub.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr5, vr2;"
        "V1@(pr0)	vr2 = mov.u32 vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr2, vr2;"
        "V1@(pr0)	vmsk0 = eq.f32 vr10, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr1, vr3;"
        "V1@(pr0)	vr5 = sub.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr5, vr2;"
        "V1@(pr0)	vr2 = mov.u32 vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32640;"
        "V0@(pr0)	vr3 = mul.f32 vr2, vr2;"
        "V1@(pr0)	vr7 = or.u32 vr6, r44;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr1, vr3;"
        "V1@(pr0)	vr5 = sub.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr5, vr2;"
        "V1@(pr0)	vr2 = or.u32 vr6, r46;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr5, vr5;"
        "V1@(pr0)	vr1 = or.u32 vr6, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk0 vr1, vr7;"
        "V1@(pr0)	vr10 = sel vmsk1 vr10, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr10 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr11 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr12 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr13 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr14 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 5120;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr15 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 6144;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr16 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 7168;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr17 = sub.f32 vr0, vr17;"
        "}"
        "{"
        "V1@(pr0)	vr10 = sel vmsk7 vr16, vr17;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32704;"
        "V0@(pr0)	vmsk4 = infnan.f32 vr11;"
        "V1@(pr0)	vr10 = sel vmsk4 vr10, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4059;"
        "pseudo@0	@pseudo imm_1 = 48969;"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32704;"
        "V0@(pr0)	vmsk3 = lseq.f32 vr12, r44;"
        "V1@(pr0)	vr10 = sel vmsk3 vr10, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4059;"
        "pseudo@0	@pseudo imm_1 = 16201;"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32704;"
        "V0@(pr0)	vmsk2 = gteq.f32 vr12, r44;"
        "V1@(pr0)	%[res0] = sel vmsk2 vr10, r45;"
        "}"
        : [res0] "=x" (result0)
        : [input0] "x" (a)
        :"vr4", "vr30", "vr10", "vr6", "vr12", "vr7", "vr15", "vr31", "vr1", "vr5", "vr0", "vr28", "vr13", "vr29", "vr2", "vr17", "vr16", "vr11", "vr3", "vr14", "r1", "r0", "r3", "r9", "vmsk3", "vmsk0", "vmsk5", "vmsk4", "vmsk7", "vmsk1", "vmsk2", "vmsk6"
        );
    return result0;
}

#endif // _TANF_H_
