#pragma once
#include "../../dlc-intrinsics.h"
#include "../../typehint.h"

#ifndef _COSF_H_X86_
#define _COSF_H_X86_

// #include "typehint.h"
#include "function.h"

static inline int8_128 _$S(float8_128 a) {
    int8_128 result0 = *(int8_128*)(&a);
    return result0;
}

static inline float8_128 _$F(int8_128 a) {
    float8_128 result0 = *(float8_128*)(&a);
    return result0;
}

#define Word_LoMask   (0x0000ffff)
#define Word_H_X86iMask   (0xffff0000)
#define Word_FullMask (0xffffffff)

#define Word_hi(a) (v_u32_shr(a, v_u32_move_i(16)) & 0xffff)
#define Word_lo(a) ((a) & Word_LoMask)

#define typeWidth           uint64_t(64)
#define exponentBits        uint64_t(11)
#define maxExponent         ((uint64_t(1)<<11) - 1)// 0x111 11111111
#define significandBits     uint64_t(52)
#define implicitBit         (uint64_t(1) << significandBits)
#define exponentBias        (maxExponent >> 1)
#define signBit             (uint64_t(1) << 63) //0x80000000 符号位为1
#define absMask             (signBit - 1)
#define significandMask     (implicitBit - 1U)
#define exponentMask        (absMask ^ significandMask)
#define quietBit            (implicitBit >> 1)
#define qnanRep             (exponentMask | quietBit)
#define infRep              exponentMask

typedef struct {
    int8_128 hi;
    int8_128 lo;
} double8_128, longlong8_128;

typedef struct {
    double8_128 hi;
    double8_128 lo;
} Twodouble8_128;

/*  默认输入前16位为0, e.g. 0x0000ffff
        ffff
    × ffff
    --------
    feff01 
    feff01
    -----------
    fffe0001
*/
inline int8_128 mul_i16(int8_128 a, int8_128 b)
{
    float8_128 b_lo = v_cvt_itof(b & 0xff);
    float8_128 b_hi = v_cvt_itof(v_u32_shr(b, v_u32_move_i(8)));
    float8_128 f_a = v_cvt_itof(a);

    int8_128 res1 = v_cvt_ftoi(f_a * b_lo, 0);
    int8_128 res2 = v_u32_shl(v_cvt_ftoi(f_a * b_hi, 0), 8);

    int8_128 res1_lo = v_u32_shl(res1, v_u32_move_i(16));
    int8_128 res2_lo = v_u32_shl(res2, v_u32_move_i(16));
    int8_128 res1_hi = v_u32_shr(res1, v_u32_move_i(16));
    int8_128 res2_hi = v_u32_shr(res2, v_u32_move_i(16));
    bool8_128 con = v_u32_carry(res1_lo, res2_lo);
    int8_128 carry = v_s32_sel(con, v_u32_move_i(0), v_u32_move_i(1));

    res1_hi = v_s32_add(res1_hi, carry);
    res1_hi = v_s32_add(res1_hi, res2_hi);
    res1_hi = v_u32_shl(res1_hi, v_u32_move_i(16));

    res1_lo = v_s32_add(res1 & v_u32_move_i(0xffff), res2 & v_u32_move_i(0xffff));
    res1_lo = res1_lo & v_u32_move_i(0xffff);
    return res1_hi | res1_lo;
}

inline double8_128 add_i32(int8_128 a, int8_128 b) {
    double8_128 res;

    int8_128 a_lo = v_u32_and(a, v_u32_move_i(0xffff));
    int8_128 b_lo = v_u32_and(b, v_u32_move_i(0xffff));
    int8_128 res_lo = v_s32_add(a_lo, b_lo);
    bool8_128 con = v_s32_cmp(GTEQ, res_lo, v_u32_move_i(0x10000));
    int8_128 carry = v_s32_sel(con, v_u32_move_i(0), v_u32_move_i(1));
    res_lo = v_u32_and(res_lo, 0xffff);

    int8_128 a_hi = v_u32_shr(a, v_u32_move_i(16));
    int8_128 b_hi = v_u32_shr(b, v_u32_move_i(16));
    int8_128 res_hi = v_s32_add(a_hi, b_hi);
    res_hi = v_s32_add(res_hi, carry);
    res.lo = v_u32_shl(res_hi, v_u32_move_i(16));
    res.hi = v_u32_shr(res_hi, v_u32_move_i(16));

    res.lo = res.lo | res_lo;
    return res;
}

inline double8_128 add_i64_i32(double8_128 a, int8_128 b)
{
    double8_128 res;
    int8_128 b_lo = b & v_u32_move_i(0xffff);
    int8_128 b_hi = v_u32_shr(b, v_u32_move_i(16));

    int8_128 a_lo_hi = v_u32_shr(a.lo, v_u32_move_i(16));
    int8_128 a_lo_lo = a.lo & v_u32_move_i(0xffff);

    bool8_128 con = v_u32_carry(v_u32_shl(a.lo, 16), v_u32_shl(b, 16));
    int8_128 carry = v_s32_sel(con, v_u32_move_i(0), v_u32_move_i(1));

    int8_128 res_lo = v_s32_add(b_lo, a_lo_lo) & v_u32_move_i(0xffff);

    int8_128 res_hi = v_s32_add(v_s32_add(b_hi, a_lo_hi), carry);
    res.lo = v_u32_shl(res_hi, v_u32_move_i(16)) | res_lo;

    int8_128 hi_carry = v_u32_shr(res_hi, v_u32_move_i(16));
    int8_128 a_hi_hi = v_u32_shr(a.hi, v_u32_move_i(16));
    int8_128 a_hi_lo = a.hi & v_u32_move_i(0xffff);
    con = v_u32_carry(a_hi_lo, hi_carry);
    carry = v_s32_sel(con, v_u32_move_i(0), v_u32_move_i(1));

    res_lo = v_s32_add(carry, a_hi_lo) & v_u32_move_i(0xffff);
    res_hi = v_s32_add(a_hi_hi, carry);
    res.hi = v_u32_shl(res_hi, v_u32_move_i(16)) | res_lo;
    return res;
}

inline double8_128 add_i64(double8_128 a, double8_128 b)
{
    double8_128 res;

    bool8_128 con = v_u32_carry(v_u32_shl(a.lo, 16), v_u32_shl(b.lo, 16));
    int8_128 carry = v_s32_sel(con, v_u32_move_i(0), v_u32_move_i(1));

    int8_128 a_lo_hi = v_u32_shr(a.lo, v_u32_move_i(16));
    int8_128 b_lo_hi = v_u32_shr(b.lo, v_u32_move_i(16));
    a_lo_hi = v_s32_add(a_lo_hi, carry);
    int8_128 res_lo_hi = v_s32_add(a_lo_hi, b_lo_hi);
    
    con = v_s32_cmp(GTEQ, res_lo_hi, v_u32_move_i(0x10000));
    int8_128 res_lo_carry = v_s32_sel(con, v_u32_move_i(0), v_u32_move_i(1));

    int8_128 a_lo_lo = a.lo & v_u32_move_i(0xffff);
    int8_128 b_lo_lo = b.lo & v_u32_move_i(0xffff);
    a_lo_lo = v_s32_add(a_lo_lo, b_lo_lo);
    int8_128 res_lo_lo = v_u32_and(a_lo_lo, v_u32_move_i(0xffff));
    res.lo = res_lo_lo | v_u32_shl(res_lo_hi, v_u32_move_i(16));

    int8_128 a_hi_lo = a.hi & v_u32_move_i(0xffff);
    int8_128 b_hi_lo = b.hi & v_u32_move_i(0xffff);
    a_hi_lo = v_s32_add(a_hi_lo, res_lo_carry);
    a_hi_lo = v_s32_add(a_hi_lo, b_hi_lo);
    int8_128 res_hi_lo = a_hi_lo & v_u32_move_i(0xffff);

    con = v_u32_carry(v_u32_shl(a.hi, v_u32_move_i(16)), v_u32_shl(b.hi, v_u32_move_i(16)));
    carry = v_s32_sel(con, v_u32_move_i(0), v_u32_move_i(1));

    int8_128 a_hi_hi = v_u32_shr(a.hi, v_u32_move_i(16));
    int8_128 b_hi_hi = v_u32_shr(b.hi, v_u32_move_i(16));
    a_hi_hi = v_s32_add(a_hi_hi, carry);
    int8_128 res_hi_hi = v_s32_add(a_hi_hi, b_hi_hi);

    res.hi = res_hi_lo | v_u32_shl(res_hi_hi, v_u32_move_i(16));
    return res;
}

inline double8_128 mul_i32(int8_128 a, int8_128 b)
{
    int8_128 a_hi = v_u32_shr(a, v_u32_move_i(16));
    int8_128 b_hi = v_u32_shr(b, v_u32_move_i(16));

    int8_128 hi_hi = mul_i16(a_hi, b_hi);

    int8_128 a_lo = a & v_u32_move_i(0xffff);
    int8_128 b_lo = b & v_u32_move_i(0xffff);
    int8_128 lo_lo = mul_i16(a_lo, b_lo);

    int8_128 hi_lo = mul_i16(a_hi, b_lo);
    double8_128 temp1 = {v_u32_shr(hi_lo, v_u32_move_i(16)), v_u32_shl(hi_lo, v_u32_move_i(16))};
    double8_128 temp2 = {hi_hi, lo_lo};
    double8_128 res = add_i64(temp1, temp2);

    int8_128 lo_hi = mul_i16(a_lo, b_hi);
    double8_128 temp3 = {v_u32_shr(lo_hi, v_u32_move_i(16)), v_u32_shl(lo_hi, v_u32_move_i(16))};
    res = add_i64(temp3, res);

    return res;
}

inline double8_128 float2double(int8_128 a)
{
    double8_128 res;
    res.lo = v_u32_shl(a, 29);
    res.hi = v_u32_shr(a, 3) & v_u32_move_i(0x7FFFFFF);
    res.hi = v_u32_or(res.hi, a & v_u32_move_i(0x80000000));

    int8_128 Exponent = v_u32_shr(a & 0x7f800000, v_u32_move_i(23));
    Exponent = v_s32_add(Exponent, v_u32_move_i(896));
    Exponent = v_u32_shl(Exponent, v_u32_move_i(20));
    res.hi = v_u32_or(res.hi, Exponent);
    return res;
}

inline int8_128 double2float(double8_128 a)
{
    int8_128 signbit = a.hi & v_u32_move_i(0x80000000);
    int8_128 Exponent = v_u32_shr(a.hi & v_u32_move_i(0x7ff00000), v_u32_move_i(20));
    Exponent = v_s32_sub(Exponent, v_u32_move_i(896));
    Exponent = v_u32_shl(Exponent, 23) & v_u32_move_i(0x7f800000);
    int8_128 Mantissa = v_u32_shl(a.hi, v_u32_move_i(3));
    bool8_128 cmp = v_s32_cmp(GTEQ, a.lo & v_u32_move_i(0x1fffffff), v_u32_move_i(0x10000000));
    int8_128 add_temp = v_s32_sel(cmp, v_u32_move_i(0), v_u32_move_i(1));
    int8_128 Mantissa_back = v_u32_shr(a.lo, v_u32_move_i(29));
    Mantissa = v_u32_or(Mantissa, v_s32_add(Mantissa_back, add_temp));
    Mantissa = Mantissa & v_u32_move_i(0x7FFFFF);
    int8_128 res = Exponent | Mantissa;
    return res | signbit;
}

inline double8_128 ll2double(longlong8_128 x){
    int8_128 int64_lo_reg = x.lo;
    int8_128 int64_hi_reg = x.hi;
    
    int8_128 expo_reg; // double的第1位~第11位代表指数位
    int8_128 frac_lo_reg; // double的第12位~63位代表尾数位，这里是第32位~63位
    int8_128 frac_hi_reg; // double的第12位~63位代表尾数位，这里是第12位~31位
    int8_128 double_lo_reg ;
    int8_128 double_hi_reg ;

    int8_128 const_temp_reg1; // 暂存
    int8_128 const_temp_reg0; // 暂存
    int8_128 sub_first_1_reg; // （正数）去掉x最高位的1，剩下都是尾数部分
    int8_128 frac_temp_lo_reg; // double的第12位~63位代表尾数位，这里是第32位~63位
    int8_128 frac_temp_hi_reg; // double的第12位~63位代表尾数位，这里是第12位~31位
    int8_128 clz_temp_hi_reg; // x的高32位的前导0个数
    int8_128 clz_temp_lo_reg; // x的低32位的前导0个数
    int8_128 frac_count_reg; // 有效的指数位有多少位

    //sinf.cpp中的是用f32比较的，这里改成s32的，应该不会有问题
    bool8_128 is_pos = v_s32_cmp(GTEQ, int64_hi_reg, 0);
    int8_128 sign_reg = v_u32_and(int64_hi_reg, v_u32_move_i(0x80000000)); // double的第0位代表符号位
    // // Print("sign_reg : %h", sign_reg);
    // 把负数变为正数，正数=~(负数-1)
    /*实际值 -280453848227002
    int64(bin)  11111111111111110000000011101101   11000000000000000010001101000110
    int64(hex)  ffff00ed c0002346
    double(bin) 1 10000101110 11111110001001000111 11111111111110111001011101000000
    double(hex) c2efe247 fffb9740*/
    bool8_128 minus1_need_borrow = v_f32_cmp(EQ, *(float8_128 *)(&int64_lo_reg), 0);
    const_temp_reg0 = int64_lo_reg - 1;
    const_temp_reg1 = int64_hi_reg - 1; 
    // 如果x-1需要借位，int64_lo从0x00000000变成0xffffffff，int64_hi-1
    const_temp_reg0 = v_s32_sel(minus1_need_borrow, const_temp_reg0, v_u32_move_i(0xffffffff));
    const_temp_reg1 = v_s32_sel(minus1_need_borrow, int64_hi_reg, const_temp_reg1);
    // 再对x-1按位取反
    const_temp_reg0 = v_u32_xor(const_temp_reg0, v_u32_move_i(0xffffffff)); 
    const_temp_reg1 = v_u32_xor(const_temp_reg1, v_u32_move_i(0xffffffff));
    // 如果x不是正数，则x变成x-1再按位取反
    int64_lo_reg = v_s32_sel(is_pos, const_temp_reg0, int64_lo_reg);
    int64_hi_reg = v_s32_sel(is_pos, const_temp_reg1, int64_hi_reg);
    // clz 为了找到第一个1
    clz_temp_hi_reg = v_u32_clz(int64_hi_reg);
    clz_temp_lo_reg = v_u32_clz(int64_lo_reg);
    bool8_128 clz_temp = v_s32_cmp(LS, clz_temp_hi_reg, 32);
    clz_temp_lo_reg = 31 - clz_temp_lo_reg;
    clz_temp_hi_reg = 63 - clz_temp_hi_reg;
    expo_reg = v_s32_sel(clz_temp, clz_temp_lo_reg, clz_temp_hi_reg);
    frac_count_reg = expo_reg;
    // exponent = biased_exponent(expo_reg) + bias(2^(k-1) - 1), and k(int64)=11
    expo_reg = v_u32_shl((expo_reg + 1023), 20);
    // 得到 frac_count_less_52_vmask 和 frac_count_less_20_vmask
    bool8_128 frac_count_less_52 = v_s32_cmp(LSEQ, frac_count_reg, 52);
    bool8_128 frac_count_less_20 = v_s32_cmp(LSEQ, frac_count_reg, 20);
    // 三个分支的公共部分 sub_first_1_reg = （正数）去掉x最高位的1，剩下都是尾数部分
    // 前两个分支 const_temp_reg0 = (int64_hi_reg - (1 << (frac_count_reg - 32)));
    const_temp_reg0 = frac_count_reg - 32;
    const_temp_reg0 = int64_hi_reg - v_u32_shl(1, const_temp_reg0); 
    // 第三个分支 const_temp_reg1 = int64_lo_reg - (1 << (frac_count_reg));
    const_temp_reg1 = int64_lo_reg - v_u32_shl(1, frac_count_reg);
    sub_first_1_reg = v_s32_sel(frac_count_less_20, const_temp_reg0, const_temp_reg1);
    // 1. 有效尾数位大于52位，舍去多余的尾数位；
    /*实际值 72042572800079432
    int64(bin)  00000000111111111111001001010110   10011000011101000011011001001000
    int64(hex)  00fff256 98743648
    double(bin) 0 10000110110 11111111111001001010 11010011000011101000011011001001
    double(hex) 436ffe4a d30e86c9*/
    // frac_hi_reg算上x的高32位的尾数
    frac_hi_reg = v_u32_shr(sub_first_1_reg, (frac_count_reg - 52));
    frac_temp_lo_reg = v_u32_shl(sub_first_1_reg, (84 - frac_count_reg));
    // frac_lo_reg算上x的低32位的尾数
    frac_lo_reg = frac_temp_lo_reg + v_u32_shr(int64_lo_reg, (frac_count_reg - 52));
    // frac_hi_reg = frac_temp_hi_reg;
    // frac_lo_reg = frac_temp_lo_reg;
    // 2. 有效尾数位小于等于52位，不足要补零；
    /*实际值 280453848227002
    int64(bin)  00000000000000001111111100010010   00111111111111111101110010111010
    int64(hex)  0000ff12 3fffdcba
    double(bin) 0 10000101110 11111110001001000111 11111111111110111001011101000000
    double(hex) 42efe247 fffb9740*/
    // frac_hi_reg算上x的高32位的尾数
    frac_temp_hi_reg = v_u32_shl(sub_first_1_reg, (52 - frac_count_reg));
    // frac_hi_reg算上x的低32位的尾数
    frac_temp_hi_reg = frac_temp_hi_reg + v_u32_shr(int64_lo_reg, (frac_count_reg - 20));
    // frac_lo_reg算上x低32位的尾数
    frac_temp_lo_reg = v_u32_shl(int64_lo_reg, (52 - frac_count_reg));
    frac_hi_reg = v_s32_sel(frac_count_less_52, frac_hi_reg, frac_temp_hi_reg);
    frac_lo_reg = v_s32_sel(frac_count_less_52, frac_lo_reg, frac_temp_lo_reg);
    // 3. 有效尾数位小于等于20位，用不到frac_lo_reg；
    /*实际值 6144
    int64(bin)  00000000000000000000000000000000   00000000000000000001100000000000
    int64(hex)  00000000 00001800
    double(bin) 0 10000001011 10000000000000000000 00000000000000000000000000000000
    double(hex) 40b8000000000000*/
    frac_temp_hi_reg = v_u32_shl(sub_first_1_reg, (20 - frac_count_reg));
    frac_hi_reg = v_s32_sel(frac_count_less_20, frac_hi_reg, frac_temp_hi_reg);
    double_lo_reg = v_s32_sel(frac_count_less_20, frac_lo_reg, 0);
    // double_hi_reg = (sign_reg | expo_reg) | frac_hi_reg;
    double_hi_reg = (sign_reg | expo_reg) | frac_hi_reg;
    double8_128 res;
    res.hi = double_hi_reg;
    res.lo = double_lo_reg;
    return res;
}

inline double8_128 double_add(double8_128 a, double8_128 b){
    int8_128 vr10_src = a.lo;  //a的低32位
    int8_128 vr11_src = a.hi;  //a的高32位
    int8_128 vr12_src = b.lo;  //b的低32位
    int8_128 vr13_src = b.hi;  //b的高32位


    int8_128 vr14_src = vr10_src & v_u32_move_i(0x80000000);
    int8_128 vr16_src = vr12_src & v_u32_move_i(0x80000000); 
    bool8_128 same_sign = v_s32_cmp(EQ, vr14_src, vr16_src);
    int8_128 vr15_src = vr11_src & v_u32_move_i(0x7fffffff);
    int8_128 vr17_src = vr13_src & v_u32_move_i(0x7fffffff);

    //取a和b中的较小值，res = (aAbs > bAbs ? b : a) vr1 = res.hi vr0 = res.lo
    //if(a.hi > b.hi) vr1 = b.hi vr0 = b.lo; else vr1 = a.hi, vr0 = a.lo 
    //if(a.hi == b.hi && a.lo > b.lo) vr0 = b.lo; else vr0 = a.lo
    bool8_128 vmask0 = v_s32_cmp(GT, vr17_src, vr15_src);
    bool8_128 vmask1 = v_s32_cmp(EQ, vr17_src, vr15_src);
    bool8_128 vmask2 = v_s32_cmp(GT, vr12_src, vr10_src);
    vmask2 = !(vmask2 ^ same_sign);

    int8_128 vr1 = v_s32_sel(vmask2, vr13_src, vr11_src);
    int8_128 vr0 = v_s32_sel(vmask2, vr12_src, vr10_src);

    vr1 = v_s32_sel(vmask1, vr13_src, vr1);
    vr0 = v_s32_sel(vmask1, vr12_src, vr0);

    vr1 = v_s32_sel(vmask0, vr1, vr11_src);
    vr0 = v_s32_sel(vmask0, vr0, vr10_src); 

    vmask0 = v_s32_cmp(EQ, vr1, vr11_src);
    vmask1 = v_s32_cmp(EQ, vr0, vr10_src);

    /* vr17 bRep(aAbs和bAbs比出来的较小值)高位    vr16 bRep低位*/
    int8_128 vr17 = vr1;
    int8_128 vr16 = vr0;  

    int8_128 vr7 = v_s32_sel(vmask0, 0, 1);
    int8_128 vr6 = v_s32_sel(vmask1, 0, 1);
    vr7 = vr6 & vr7; 
    vmask0 = v_s32_cmp(EQ, vr7, 1);
    /*vr15 aRep(aAbs和bAbs比出来的较大值)高位   vr14 aRep低位*/
    int8_128 vr15 = v_s32_sel(vmask0, vr11_src, vr13_src);
    int8_128 vr14 = v_s32_sel(vmask0, vr10_src, vr12_src);
    /*vr0 aExponent（aRep的指数部分）  vr1 bExponent（bRep的指数部分）*/
    vr0 = v_u32_shr(vr15, 0x14) & v_u32_move_i(0x7ff);
    vr1 = v_u32_shr(vr17, 0x14) & v_u32_move_i(0x7ff);
    /*vr2 aSignificand 低位    vr3 aSignificand 高位*/
    int8_128 vr2 = vr14;
    int8_128 vr3 = vr15 & v_u32_move_i(0xfffff);
    /*vr4 bSignificand 低位 vr5 bSignificand 高位*/
    int8_128 vr4 = vr16;
    int8_128 vr5 = vr17 & v_u32_move_i(0xfffff);
    /*符号位   vr6 resultSign  vr7 subtraction*/
    vr6 = vr15 & v_u32_move_i(0x80000000);
    vr7 = v_u32_xor(vr15, vr17) & v_u32_move_i(0x80000000);
    /*vr2 aSignificand 低位    vr3 aSignificand 高位*/
    vr3 = v_u32_shl(vr3 | v_u32_move_i(0x100000), 3);
    int8_128 vr28 = v_u32_shr(vr2 & v_u32_move_i(0xe0000000), 0x1d);
    vr3 = vr3 | vr28;
    vr2 = v_u32_shl(vr2, 3);
    /*vr4 bSignificand 低位    vr5 bSignificand 高位*/
    vr5 = v_u32_shl(vr5 | v_u32_move_i(0x100000), 3);
    vr28 = v_u32_shr(vr4 & v_u32_move_i(0xe0000000), 0x1d);
    vr5 = vr5 | vr28;
    vr4 = v_u32_shl(vr4, 3);    

    /*vr28 align*/
    vr28 = vr0 - vr1; 
    /*vr29 (typeWidth - align)*/
    int8_128 vr29 = v_u32_move_i(0x40) - vr28;  
    vmask0 = v_s32_cmp(GT, vr29, v_u32_move_i(0x20)); 
    int8_128 vr31 = v_u32_move_i(0x20) - vr29; //31
    int8_128 vr30 = v_u32_shl(v_u32_move_i(0xffffffff), vr31);//30
    vr30 = v_u32_shr(vr4 & vr30, vr31);
    // /*第一种可能 vr10 sticky 低位    vr11 sticky 高位*/
    int8_128 vr11 = v_u32_shl(vr5, vr29);
    vr11 = vr11 | vr30;
    int8_128 vr10 = v_u32_shl(vr4, vr29); 
    vr30 = vr29 - v_u32_move_i(0x20);
    /*第二种情况 vr12 sticky 高位*/
    int8_128 vr12 = v_u32_shl(vr4, vr30);
    /*vr10 sticky 低位    vr11 sticky 高位*/
    vr10 = v_s32_sel(vmask0, vr10, 0);
    vr11 = v_s32_sel(vmask0, vr11, vr12);
    /*判断 sticky bool*/
    // bool sticky = (aSignificand << (typeWidth - shift)) != 0;
    vmask0 = v_s32_cmp(EQ, vr10, 0);
    vmask1 = v_s32_cmp(EQ, vr11, 0);
    vr17 = v_s32_sel(vmask0 & vmask1 , 1, 0); //这里感觉错了！！！！ 应该是46 -> 48
    
    vmask0 = v_s32_cmp(GT, vr28, 0x20);  //vmask0
    vr30 = 0x20 - vr28; //30
    vr31 = v_u32_shr(v_u32_move_i(0xffffffff), vr30);//31
    vr31 = v_u32_shl(vr31 & vr5, vr30);

    // /*第一种情况 vr10 lo    vr11 hi*/
    vr10 = v_u32_shr(vr4, vr28) | vr31; // 10
    vr11 = v_u32_shr(vr5, vr28); //11
    vr31 = vr28 - 0x20;
    vr12 = v_u32_shr(vr5, vr31);
    vr10 = v_s32_sel(vmask0, vr10, vr12);
    vr11 = v_s32_sel(vmask0, vr11, 0);
    vr10 = vr10 | vr17;
    vmask0 = v_s32_cmp(LS, vr28, 0x40); //vmask0
    vr11 = v_s32_sel(vmask0, 0, vr11);
    vr10 = v_s32_sel(vmask0, 1, vr10);
    vmask0 = v_s32_cmp(EQ, vr28, 0); //vmask0
    vr4 = v_s32_sel(vmask0, vr10, vr4);
    vr5 = v_s32_sel(vmask0, vr11, vr5);
    /*获得低位32的减法  vr5 是经过align中的vr5     vr10 是减法中aSignificand的低32位的低16位*/
    vr10 = vr2 & 0xffff; //10
    /*vr12 是减法中bSignificand的低32位的低16位    vr11 是减法中aSignificand的低32位的高16位*/
    vr12 = vr4 & 0xffff;//12
    vr11 = v_u32_shr(vr2, 0x10);//11
    int8_128 vr13 = v_u32_shr(vr4, 0x10);//13
    // /*需要先判断谁大谁小 */
    vmask0 = v_s32_cmp(GTEQ, vr10, vr12); //vmask0
    vr17 = v_s32_sel(vmask0, 1, 0);  //17
    vr17 = v_u32_shl(vr17, 0x10);
    vr10 = vr10 | vr17;
    vr14 = vr10 - vr12; //14
    /*低32位的高16位借位与否的结果*/
    vr13 = v_s32_sel(vmask0, vr13 + 1, vr13);
    /*判断低32位的高16位大小*/
    vmask0 = v_s32_cmp(GTEQ, vr11, vr13); //vmask0
    vr17 = v_s32_sel(vmask0, 1, 0);  //17
    vr17 = v_u32_shl(vr17, 0x10);
    /*r15 低32位的高16位的结果*/
    vr11 = vr11 | vr17;
    vr15 = vr11 - vr13; //15
    vr10 = vr14 | v_u32_shl(vr15, 0x10); //10
    vr17 = vr5 + 1; //17
    vr17 = v_s32_sel(vmask0, vr17, vr5);
    /*vr11 减法高32位结果*/
    vr11 = vr3 - vr17;
    vmask0 = v_s32_cmp(EQ, vr11, 0);
    vr31 = v_u32_clz(vr11);
    vr30 = v_u32_clz(vr10) + 0x20;

    /*vr31 shift*/
    vr31 = v_s32_sel(vmask0, vr31, vr30) - 0x8;
    /*判断减法中shift与32的大小*/
    bool8_128 vmask6 = v_s32_cmp(LS, vr31, 0x20);

    /*此时30里面是当shift小于32时*/
    vr30 = 0x20 - vr31;
    vr29 = v_u32_shl(0xffffffff, vr30);
    /*vr29是当小于32时 低位需要左移的值*/
    vr29 = v_u32_shr(vr29 & vr10, vr30);
    vr13 = v_u32_shl(vr11, vr31);
    vr13 = vr13 | vr29;
    vr12 = v_u32_shl(vr10, vr31);
    vr30 = vr31 - 0x20;
    /*vr14 高位*/
    vr14 = v_u32_shl(vr10, vr30);
    vr12 = v_s32_sel(vmask6, 0, vr12);
    vr13 = v_s32_sel(vmask6, vr14, vr13);
    /*要考虑aSignificand < implicitBit << 3的低高位情况*/
    vmask0 = v_s32_cmp(LS, vr11, 0x800000);
    vr10 = v_s32_sel(vmask0, vr10, vr12);
    /*-------------------------------------------------------*/
    vr15 = vr0 - vr31;
    /*vr15 aExponent*/
    vr11 = v_s32_sel(vmask0, vr11, vr13);
    vr15 = v_s32_sel(vmask0, vr0, vr15);
    /*加法*/
    /*vr12 aSignificand 低位 16低位    vr13 aSignificand 低位 16高位*/
    vr12 = vr2 & 0xffff;
    vr13 = v_u32_shr(vr2, 0x10);
    /*vr16 bSignificand 低位 16低位    vr17 bSignificand 低位 16高位*/
    vr16 = vr4 & 0xffff;
    vr17 = v_u32_shr(vr4, 0x10);
    vr28 = vr16 + vr12;
    vr14 = v_u32_shr(vr28, 0x10);
    vr28 = vr28 & 0xffff;
    vr29 = vr13 + vr17 + vr14;
    vr14 = v_u32_shr(vr29, 0x10);
    vr29 = v_u32_shl(vr29 & 0xffff, 0x10);
    vr12 = vr29 | vr28;
    vr13 = vr3 + vr5 + vr14;
    vr28 = vr12 & 1;
    vr14 = v_u32_shl(vr13 & 1, 0x1f);
    vr16 = v_u32_shr(vr12, 1) | vr14 | vr28;
    vr17 = v_u32_shr(vr13, 1);
    vr14 = vr0 + 1;
    vr28 = vr13 & 0x1000000;
    vmask0 = v_s32_cmp(EQ, vr28, 0);
    vr12 = v_s32_sel(vmask0, vr16, vr12); 
    vr13 = v_s32_sel(vmask0, vr17, vr13);
    vr14 = v_s32_sel(vmask0, vr14, vr0);
    vmask1 = v_s32_cmp(EQ, vr7, 0);
    vr2 = v_s32_sel(vmask1, vr10, vr12);
    vr3 = v_s32_sel(vmask1, vr11, vr13);
    vr0 = v_s32_sel(vmask1, vr15, vr14);
    /* 需要判断一下sticky的大小 现在默认为1  shift为64的倍数*/
    vr10 = vr2 | 1;
    vmask0 = v_s32_cmp(LSEQ, vr0, 0);
    vr2 = v_s32_sel(vmask0, vr2, vr10); //80000001 error  vr10:1
    /*vr17 roundGuardSticky*/
    vr17 = vr2 & 0x7;
    vr31 = v_u32_shr(0xffffffff, 0x1d) & vr3;
    vr31 = v_u32_shl(vr31, 0x1d);
    
    vr10 = v_u32_shr(vr2, 0x3) | vr31;//error vr10:0 vr2:0
    vr11 = v_u32_shr(vr3, 0x3) & 0xfffff;
    /*result |= (rep_t)aExponent << significandBits;*/
    vr28 = v_u32_shl(vr0, 0x14);
    /*result |= resultSign;*/
    vr1 = vr11 | vr28 | vr6;
    vr12 = vr10 + 1;
    vr13 = vr10 & 1;
    vmask0 = v_s32_cmp(GT, vr17, 0x4);
    vmask1 = v_s32_cmp(EQ, vr17 , 0x4);
    vr13 = vr10 + vr13;
    vr13 = v_s32_sel(vmask1, vr10, vr13);
    vr0 = v_s32_sel(vmask0, vr13, vr12);


    vmask0 = v_s32_cmp(EQ, vr17_src, 0);
    vmask1 = v_s32_cmp(EQ, vr16_src, 0);
    vr28 = 0;
    vr7 = v_s32_sel(vmask0, 0, 1);
    vr6 = v_s32_sel(vmask1, 0, 1);
    vr7 = vr6 + vr7;
    vmask0 = v_s32_cmp(EQ, vr7, 0x2);
    vr30 = v_s32_sel(vmask0, vr0, vr10_src);
    // anything + zero = anything vm0 = !bAbs
    vr31 = v_s32_sel(vmask0, vr1, vr11_src);
    vr6 = vr10_src & vr12_src;
    vr7 = vr11_src & vr13_src;
    vr5 = v_s32_sel(vmask0, vr13_src, vr7);
    vr4 = v_s32_sel(vmask0, vr12_src, vr6);
    vmask0 = v_s32_cmp(EQ, vr14_src, 0);
    vmask1 = v_s32_cmp(EQ, vr15_src, 0);
    vr6 = v_s32_sel(vmask0, vr28, 1);
    vr7 = v_s32_sel(vmask1, vr28, 1) + vr6;
    vmask0 = v_s32_cmp(EQ, vr7, 0x2);
    vr31 = v_s32_sel(vmask0, vr31, vr5);
    // zero + anything = anything
    vr30 = v_s32_sel(vmask0, vr30, 0);
    vr7 = v_u32_xor(0x7fffffff, 0xfffff);
    vr5 = vr7;
    vr4 = 0;
    vmask0 = v_s32_cmp(EQ, vr16_src, 0);
    vmask6 = v_s32_cmp(EQ, vr14_src, 0);
    bool8_128 vmask7 = v_s32_cmp(EQ, vr15_src, vr7);
    vmask1 = v_s32_cmp(EQ, vr17_src, vr7);
    vr7 = v_s32_sel(vmask0, vr28, 1);
    vr6 = v_s32_sel(vmask1, vr28, 1);
    vr7 = vr6 + vr7;
    vmask0 = v_s32_cmp(EQ, vr7, 0x2);
    vr31 = v_s32_sel(vmask0, vr31, vr13_src);
    // anything remaining + +/-infinity = +/-infinity
    vr30 = v_s32_sel(vmask0, vr30, vr12_src);
    vr7 = v_s32_sel(vmask7, vr28, 1);
    vr6 = v_s32_sel(vmask6, vr28, 1);
    vr7 = vr6 + vr7;
    vmask7 = v_s32_cmp(EQ, vr7, 0x2);
    // +/-infinity + -/+infinity = qNaN
    vr3 = vr5 | 0x80000;
    vr6 = v_u32_xor(vr10_src, vr12_src);
    vr7 = v_u32_xor(vr11_src, vr13_src);
    vmask0 = v_s32_cmp(EQ, vr7, 0x80000000);
    vmask1 = v_s32_cmp(EQ, vr6, 0);
    vr7 = v_s32_sel(vmask0, vr28, 1);
    vr6 = v_s32_sel(vmask1, vr28, 1);
    vr7 = vr6 + vr7;
    vmask0 = v_s32_cmp(EQ, vr7, 0x2);
    vr7 = v_s32_sel(vmask0, vr11_src, vr3);
    vr6 = v_s32_sel(vmask0, vr10_src, vr4);
    vr30 = v_s32_sel(vmask7, vr30, vr6);
    // +/-infinity + anything remaining = +/- infinity
    vr31 = v_s32_sel(vmask7, vr31, vr7);
    vr7 = vr13_src | 0x80000;
    // anything + NaN = qNaN
    vmask0 = v_s32_cmp(EQ, vr16_src, 0);
    vr6 = v_u32_shr(vr17_src, 0x10);
    vr28 = vr17_src & 0xffff;
    vmask1 = v_s32_cmp(GT, vr6, 0x7ff0);
    vmask2 =  v_s32_cmp(EQ, vr28, 0);
    vr5 = v_s32_sel(vmask0, vr28, 1);
    vr4 = v_s32_sel(vmask2, vr28, 1);
    vr5 = vr4 + vr5;
    vmask2 = v_s32_cmp(EQ, vr5, 0x2);
    vr5 = v_s32_sel(vmask2, vr7, vr31);
    vr4 = v_s32_sel(vmask2, vr12_src, vr30);
    vmask2 = v_s32_cmp(EQ, vr6, 0x7ff0);
    vr5 = v_s32_sel(vmask2, vr31, vr5);
    vr4 = v_s32_sel(vmask2, vr30, vr4);
    vr31 = v_s32_sel(vmask1, vr5, vr7);
    // anything + NaN = qNaN
    vr30 = v_s32_sel(vmask1, vr4, vr12_src);
    vr7 = vr11_src | 0x80000;
    vmask0 = v_s32_cmp(EQ, vr14_src, 0);
    vr6 = v_u32_shr(vr15_src, 0x10);
    vr28 = vr15_src & 0xffff;
    vmask1 = v_s32_cmp(GT, vr6, 0x7ff0);
    vmask2 = v_s32_cmp(EQ, vr28, 0);
    vr5 = v_s32_sel(vmask0, vr28, 1);
    vr4 = v_s32_sel(vmask2, vr28, 1);
    vr5 = vr4 + vr5;
    vmask2 = v_s32_cmp(EQ, vr5, 0x2);
    vr5 = v_s32_sel(vmask2, vr7, vr31);
    vr4 = v_s32_sel(vmask2, vr10_src, vr30);
    vmask2 = v_s32_cmp(EQ, vr6, 0x7ff0);
    vr4 = v_s32_sel(vmask2, vr30, vr4);
    vr5 = v_s32_sel(vmask2, vr31, vr5);
    double8_128 res;
    res.hi = v_s32_sel(vmask1, vr5, vr7);
    res.lo = v_s32_sel(vmask1, vr4, vr10_src);
    
    return res;
}

inline int8_128 rep_clz(double8_128 a)
{
    int8_128 hi_clz = v_u32_clz(a.hi);
    int8_128 lo_clz = v_u32_clz(a.lo);
    return v_s32_add(hi_clz, v_s32_sel(v_s32_cmp(EQ, a.hi, v_u32_move_i(0)), v_u32_move_i(0), lo_clz));
}

inline Twodouble8_128 wideMultiply(int8_128 a_hi, int8_128 a_lo, int8_128 b_hi, int8_128 b_lo)
{
    int8_128 product11 = mul_i16(Word_hi(a_hi), Word_hi(b_hi));
    int8_128 product12 = mul_i16(Word_hi(a_hi), Word_lo(b_hi));
    int8_128 product13 = mul_i16(Word_hi(a_hi), Word_hi(b_lo));
    int8_128 product14 = mul_i16(Word_hi(a_hi), Word_lo(b_lo));
    int8_128 product21 = mul_i16(Word_lo(a_hi), Word_hi(b_hi));
    int8_128 product22 = mul_i16(Word_lo(a_hi), Word_lo(b_hi));
    int8_128 product23 = mul_i16(Word_lo(a_hi), Word_hi(b_lo));
    int8_128 product24 = mul_i16(Word_lo(a_hi), Word_lo(b_lo));
    int8_128 product31 = mul_i16(Word_hi(a_lo), Word_hi(b_hi));
    int8_128 product32 = mul_i16(Word_hi(a_lo), Word_lo(b_hi));
    int8_128 product33 = mul_i16(Word_hi(a_lo), Word_hi(b_lo));
    int8_128 product34 = mul_i16(Word_hi(a_lo), Word_lo(b_lo));
    int8_128 product41 = mul_i16(Word_lo(a_lo), Word_hi(b_hi));
    int8_128 product42 = mul_i16(Word_lo(a_lo), Word_lo(b_hi));
    int8_128 product43 = mul_i16(Word_lo(a_lo), Word_hi(b_lo));
    int8_128 product44 = mul_i16(Word_lo(a_lo), Word_lo(b_lo));

    double8_128 sum0 = {v_u32_move_i(0), product44};
    double8_128 sum1 = add_i32(product34, product43);
    double8_128 sum2 = add_i32(product24, product33);
                sum2 = add_i64(sum2, (double8_128){v_u32_move_i(0), product42});
    double8_128 sum3 = add_i32(product14, product23);
                sum3 = add_i64(sum3, (double8_128){v_u32_move_i(0), product32});
                sum3 = add_i64(sum3, (double8_128){v_u32_move_i(0), product41});
    double8_128 sum4 = add_i32(product13, product22);
                sum4 = add_i64(sum4, (double8_128){v_u32_move_i(0), product31});
    double8_128 sum5 = add_i32(product12, product21);
    double8_128 sum6 = {v_u32_move_i(0), product11};
  
    int8_128 temp = v_u32_shl(v_u32_and(sum1.lo, 0xffff), v_u32_move_i(16)); 
    double8_128 r0 = add_i32(sum0.lo, temp);
    double8_128 r1 = add_i64((double8_128){v_u32_move_i(0), sum0.hi}, (double8_128){v_u32_move_i(0), v_u32_or(v_u32_shl(sum1.hi, 16), v_u32_shr(sum1.lo, 16))});
                r1 = add_i64(r1, (double8_128){v_u32_move_i(0), sum2.lo});
                r1 = add_i64(r1, (double8_128){v_u32_move_i(0), v_u32_shl(sum3.lo, v_u32_move_i(16))});
 
    double8_128 lo = add_i64(r0, (double8_128){r1.lo, v_u32_move_i(0)});

    double8_128 hi = add_i64((double8_128){v_u32_move_i(0), r1.hi}, (double8_128){v_u32_move_i(0), v_u32_shr(sum1.hi, 16)});
                hi = add_i64(hi, (double8_128){v_u32_move_i(0), sum2.hi});
                hi = add_i64(hi, (double8_128){v_u32_shr(sum3.hi, v_u32_move_i(16)), v_u32_or(v_u32_shl(sum3.hi, v_u32_move_i(16)), v_u32_shr(sum3.lo, v_u32_move_i(16)))});
                hi = add_i64(hi, sum4);
                hi = add_i64(hi, (double8_128){v_u32_or(v_u32_shl(sum5.hi, v_u32_move_i(16)), v_u32_shr(sum5.lo, v_u32_move_i(16))), v_u32_shl(sum5.lo, v_u32_move_i(16))});
                hi = add_i64(hi, (double8_128){sum6.lo, v_u32_move_i(0)});

    return (Twodouble8_128){hi, lo};
}

inline double8_128 __mulXf3__(double8_128 a, double8_128 b) {
    // uint64_t aExponent = a >> significandBits & maxExponent;
    int8_128 aExponent = v_u32_shr(a.hi, v_u32_move_i(20)) & v_u32_move_i(0x7ff);
    int8_128 bExponent = v_u32_shr(b.hi, v_u32_move_i(20)) & v_u32_move_i(0x7ff);
    
    int8_128 productSign = v_u32_xor(a.hi, b.hi) & v_u32_move_i(0x80000000);

    // uint64_t aSignificand = a & significandMask;
    double8_128 aSignificand = {a.hi & v_u32_move_i(0xfffff), a.lo};
    double8_128 bSignificand = {b.hi & v_u32_move_i(0xfffff), b.lo};

    // aSignificand |= implicitBit;
    aSignificand.hi = v_u32_or(aSignificand.hi, v_u32_move_i(0x100000));
    bSignificand.hi = v_u32_or(bSignificand.hi, v_u32_move_i(0x100000));

    // wideMultiply(aSignificand, bSignificand << exponentBits, &productHi, &productLo);
    Twodouble8_128 Product = wideMultiply(aSignificand.hi, aSignificand.lo, 
                                          v_u32_or(v_u32_shl(bSignificand.hi, v_u32_move_i(11)), v_u32_shr(bSignificand.lo, v_u32_move_i(21))),
                                          v_u32_shl(bSignificand.lo, v_u32_move_i(11)));

    // int productExponent = aExponent + bExponent - exponentBias + scale(0);
    int8_128 productExponent = v_s32_sub(v_s32_add(aExponent, bExponent), v_u32_move_i(0x3ff));

    // if(productHi & implicitBit) productExponent++; else wideLeftShift(&productHi, &productLo, 1);;
    bool8_128 mask = v_s32_cmp(EQ, v_u32_and(Product.hi.hi, v_u32_move_i(0x100000)), v_u32_move_i(0));
    productExponent = v_s32_sel(mask, v_s32_add(productExponent, v_u32_move_i(1)), productExponent);

    double8_128 NewProductLo = {v_u32_or(v_u32_shl(Product.lo.hi, 1), v_u32_shr(Product.lo.lo, 31)), v_u32_shl(Product.lo.lo, 1)};
    double8_128 NewProductHi = {v_u32_or(v_u32_shl(Product.hi.hi, 1), v_u32_shr(Product.hi.lo, 31)), v_u32_shl(Product.hi.lo, 1)};
                NewProductHi.lo = v_u32_or(NewProductHi.lo, v_u32_shr(Product.lo.hi, 31));
    Product.hi.hi = v_s32_sel(mask, Product.hi.hi, NewProductHi.hi);
    Product.hi.lo = v_s32_sel(mask, Product.hi.lo, NewProductHi.lo);
    Product.lo.hi = v_s32_sel(mask, Product.lo.hi, NewProductLo.hi);
    Product.lo.lo = v_s32_sel(mask, Product.lo.lo, NewProductLo.lo);

    // productHi &= significandMask;
    Product.hi.hi = v_u32_and(Product.hi.hi, v_u32_move_i(0xfffff));

    // productHi |= (uint64_t(productExponent) << significandBits);
    Product.hi.hi = v_u32_or(Product.hi.hi, v_u32_shl(productExponent, 20));

    // productHi |= productSign;
    Product.hi.hi = v_u32_or(Product.hi.hi, productSign);

    // if(productLo > signBit) productHi++;
    bool8_128 Lo_mask = v_s32_cmp(GT, Product.lo.hi, v_u32_move_i(0x80000000));
    int8_128 add_temp = v_s32_sel(Lo_mask, v_u32_move_i(0), v_u32_move_i(1));

    // if (productLo == signBit) productHi += productHi & 1;
    Lo_mask = v_s32_cmp(EQ, Product.lo.hi, v_u32_move_i(0x80000000));
    add_temp = v_s32_sel(Lo_mask, add_temp, v_s32_add(add_temp, Product.hi.lo & 1));

    Product.hi = add_i64(Product.hi, (double8_128){v_u32_move_i(0), add_temp});
    return Product.hi;
}

inline double8_128 reduce_fast(double8_128 x, int8_128 *n)
{
    double8_128 hpi_inv = {v_u32_move_i(0x41645F30), v_u32_move_i(0x6DC9C883)};
    double8_128 hpi = {v_u32_move_i(0x3ff921fb), v_u32_move_i(0x54442d18)};

    double8_128 r = __mulXf3__(x, hpi_inv);

    int8_128 r_float = double2float(r);
    double8_128 r_core = float2double(r_float);
    r_core.hi = v_u32_xor(r_core.hi, v_u32_move_i(0x80000000));
    double8_128 r_extra = double_add(r, r_core);

    *n = v_cvt_ftoi(_$F(r_float), 0);
    *n = v_s32_add(*n, v_cvt_ftoi(_$F(double2float(r_extra)), 0));
    *n = v_s32_add(*n, v_u32_move_i(0x800000));
    *n = *n >> 24;

    double8_128 double_n = float2double(_$S(v_cvt_itof(*n)));
    double8_128 nxhpi = __mulXf3__(double_n, hpi);

    nxhpi.hi = v_u32_xor(nxhpi.hi, v_u32_move_i(0x80000000));

    double8_128 res = double_add(x, nxhpi);

    return res;
}

inline double8_128 reduce_large(int8_128 x, int8_128 *n)
{
    int __inv_pio4[24] = {0xa2,       0xa2f9,     0xa2f983,   0xa2f9836e,
                          0xf9836e4e, 0x836e4e44, 0x6e4e4415, 0x4e441529,
                          0x441529fc, 0x1529fc27, 0x29fc2757, 0xfc2757d1,
                          0x2757d1f5, 0x57d1f534, 0xd1f534dd, 0xf534ddc0,
                          0x34ddc0db, 0xddc0db62, 0xc0db6295, 0xdb629599,
                          0x6295993c, 0x95993c43, 0x993c4390, 0x3c439041};
    double8_128 pi63 = {v_u32_move_i(0x3c1921fb), v_u32_move_i(0x54442d18)};

    int8_128 idx = v_u32_shr(x, 26) & 0xf;
    int8_128 arr_0 = v_u32_move_i(0);
    int8_128 arr_4 = v_u32_move_i(0);
    int8_128 arr_8 = v_u32_move_i(0);
    for(int i = 0; i <= 15; i++) {
        bool8_128 mask = v_s32_cmp(EQ, idx, v_u32_move_i(i));
        arr_0 = v_s32_sel(mask, arr_0, v_u32_move_i(__inv_pio4[i]));
        arr_4 = v_s32_sel(mask, arr_4, v_u32_move_i(__inv_pio4[i + 4]));
        arr_8 = v_s32_sel(mask, arr_8, v_u32_move_i(__inv_pio4[i + 8]));
    }
    int8_128 shift = v_u32_shr(x, 23) & 7;
    x = (x & 0xffffff) | 0x800000;
    x = v_u32_shl(x, shift);

    double8_128 res0 = mul_i32(x, arr_0);
    double8_128 res1 = mul_i32(x, arr_4);
    double8_128 res2 = mul_i32(x, arr_8);

    res0.hi = res0.lo;
    res0.lo = res2.hi;

    res0 = add_i64(res0, res1);

    *n = v_u32_shr(v_s32_add(v_u32_shr(res0.hi, 16), v_u32_move_i(0x2000)) & 0xffff, 14);

    int8_128 temp_n = v_u32_shl(*n, 30);
    bool8_128 cmp = v_s32_cmp(GT, v_u32_shr(res0.hi, v_u32_move_i(1)), v_u32_shr(temp_n, v_u32_move_i(1)));
    int8_128 save_temp = res0.hi & v_u32_move_i(0x3);
    int8_128 borrow_res0 = v_u32_shr(res0.hi, 2) | v_u32_move_i(0x40000000);
    int8_128 complex_res = v_s32_sub(borrow_res0, v_u32_shr(temp_n, v_u32_move_i(2)));
    complex_res = v_u32_shl(complex_res, v_u32_move_i(2)) | save_temp;
    int8_128 simple_res = v_s32_sub(res0.hi, temp_n);
    res0.hi = v_s32_sel(cmp, complex_res, simple_res);

    return __mulXf3__(ll2double(res0), pi63);
}

inline int8_128 rem_pio2f(float8_128 x, int8_128 *y0, int8_128 *y1)
{
    int8_128 xi = _$S(x);
    int8_128 n;
    int8_128 reduce_fast_n;
    double8_128 dx = float2double(xi);
    double8_128 reduce_fast_dx = reduce_fast(dx, &reduce_fast_n);
    
    int8_128 reduce_large_n;
    double8_128 reduce_large_dx = reduce_large(xi, &reduce_large_n);
    
    bool8_128 mask = v_f32_cmp(LS, _$F(xi & v_u32_move_i(0x7fffffff)), v_u32_move_f(120.0));
    double8_128 double_y0;

    double_y0.hi = v_s32_sel(mask, reduce_large_dx.hi, reduce_fast_dx.hi);
    double_y0.lo = v_s32_sel(mask, reduce_large_dx.lo, reduce_fast_dx.lo);
    n = v_s32_sel(mask, reduce_large_n, reduce_fast_n);

    *y0 = double2float(double_y0);

    double8_128 temp = float2double(v_u32_xor(*y0, v_u32_move_i(0x80000000)));
    double8_128 double_y1 = double_add(double_y0, temp);

    *y1 = double2float(double_y1);

    return n;
}

inline float8_128 __kernel_sinf(float8_128 x, float8_128 y, int iy)
{
    float8_128 half = v_u32_move_b(0x3f000000);
    float8_128 S1 = v_u32_move_b(0xbe2aaaab);
    float8_128 S2 = v_u32_move_b(0x3c088889);
    float8_128 S3 = v_u32_move_b(0xb9500d01);
    float8_128 S4 = v_u32_move_b(0x3638ef1b);
    float8_128 S5 = v_u32_move_b(0xb2d72f34);
    float8_128 S6 = v_u32_move_b(0x2f2ec9d3);

    int8_128 ix = _$S(x);
    ix = ix & 0x7fffffff;
    
    float8_128 z = v_f32_mul_b(x, x);
    float8_128 v = v_f32_mul_b(z, x);
    // r = S2+z*(S3+z*(S4+z*(S5+z*S6)));
    float8_128 r = v_f32_add_b(S2, v_f32_mul_b(z, v_f32_add_b(S3, v_f32_mul_b(z, v_f32_add_b(S4, v_f32_mul_b(z, v_f32_add_b(S5, v_f32_mul_b(z, S6))))))));
    float8_128 res;
    if(iy == 0) res = v_f32_add_b(x, v_f32_mul_b(v, v_f32_add_b(S1, v_f32_mul_b(z, r))));
    else res = v_f32_sub_b(x, v_f32_sub_b(v_f32_sub_b(v_f32_mul_b(z, v_f32_sub_b(v_f32_mul_b(half, y), v_f32_mul_b(v, r))), y), v_f32_mul_b(v, S1)));
    bool8_128 mask = v_f32_cmp(LS, _$F(ix), v_u32_move_b(0x32000000));
    res = v_f32_sel(mask, res, x);
    return res;
}

inline float8_128 __kernel_cosf(float8_128 x, float8_128 y)
{
    float8_128 one = v_u32_move_b(0x3f800000);
    float8_128 C1 = v_u32_move_b(0x3d2aaaab);
    float8_128 C2 = v_u32_move_b(0xbab60b61);
    float8_128 C3 = v_u32_move_b(0x37d00d01);
    float8_128 C4 = v_u32_move_b(0xb493f27c);
    float8_128 C5 = v_u32_move_b(0x310f74f6);
    float8_128 C6 = v_u32_move_b(0xad47d74e);
    
    int8_128 ix = _$S(x);
    ix = ix & 0x7fffffff;

    float8_128 z = v_f32_mul_b(x, x);
    float8_128 r = v_f32_mul_b(z, v_f32_add_b(C1, v_f32_mul_b(z, v_f32_add_b(C2, v_f32_mul_b(z, v_f32_add_b(C3, v_f32_mul_b(z, v_f32_add_b(C4, v_f32_mul_b(z, v_f32_add_b(C5, v_f32_mul_b(z, C6)))))))))));
    
    float8_128 res;
    float8_128 qx = _$F(ix - v_u32_move_i(0x01000000));
    bool8_128 mask = v_s32_cmp(GT, ix, v_u32_move_i(0x3f480000));
    qx = v_f32_sel(mask, qx, v_u32_move_f(0.28125));
    float8_128 hz = v_f32_sub_b(v_f32_mul_b(v_u32_move_f(0.5), z), qx);
    float8_128 a = v_f32_sub_b(one, qx);
    res = v_f32_sub_b(a, v_f32_sub_b(hz, v_f32_sub_b(v_f32_mul_b(z, r), v_f32_mul_b(x, y))));

    mask = v_s32_cmp(LS, ix, v_u32_move_i(0x3e99999a));
    float8_128 res1 = v_f32_sub_b(one, v_f32_sub_b(v_f32_mul_b(v_u32_move_f(0.5), z), v_f32_sub_b(v_f32_mul_b(z, r), v_f32_mul_b(x, y))));
    res = v_f32_sel(mask, res, res1);

    mask = v_s32_cmp(LS, ix, v_u32_move_i(0x32000000));
    res = v_f32_sel(mask, res, one);

    return res;
}

inline float8_128 __dlc_cosf(float8_128 x)
{
    float8_128 res;
    int8_128 ix = _$S(x);
    ix = ix & 0x7fffffff;

    float8_128 z = v_u32_move_f(0);
    
    int8_128 y0, y1;
    int8_128 n = rem_pio2f(x, &y0, &y1);
    n = n & 3;

    float8_128 kernel_cos_res = __kernel_cosf(_$F(y0), _$F(y1));
    float8_128 kernel_sin_res = __kernel_sinf(_$F(y0), _$F(y1), 1);

    res = kernel_sin_res;
    bool8_128 mask0 = v_s32_cmp(EQ, n, v_u32_move_i(0));
    res = v_f32_sel(mask0, res, kernel_cos_res);
    bool8_128 mask1 = v_s32_cmp(EQ, n, v_u32_move_i(1));
    res = v_f32_sel(mask1, res, v_f32_mul_b(kernel_sin_res, v_u32_move_f(-1)));
    bool8_128 mask2 = v_s32_cmp(EQ, n, v_u32_move_i(2));
    res = v_f32_sel(mask2, res, v_f32_mul_b(kernel_cos_res, v_u32_move_f(-1)));

    // x >= 0x7f800000
    bool8_128 mask = v_s32_cmp(GT, ix, v_u32_move_i(0x7f800000));
    res = v_f32_sel(mask, res, v_f32_sub_b(x, x));

    // x <= 0x3fe921fb
    mask = v_s32_cmp(LS, ix, v_u32_move_i(0x3f490fda));
    res = v_f32_sel(mask, res, __kernel_cosf(x, z));

    bool8_128 infnan = v_f32_infnan(x);
    res = v_f32_sel(infnan, res, v_u32_move_b(0x7fc00000));

    return res;
}

inline float8_128 __dlc_cosf_libdevice(float8_128 a)
{
    float8_128 result0;
    asm (
        "{V0@(pr0)  vr10 = mov.u32 %[input0];}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr10;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr1 = and.u32 vr10, r44;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 17136;"
        "V1@(pr0)	vmsk4 = ls.f32 vr2, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr10;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 29;"
        "V1@(pr0)	vr11 = shl.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 49152;"
        "pseudo@0	@pseudo imm_2 = 3;"
        "V0@(pr0)	vr1 = and.u32 vr10, r44;"
        "V1@(pr0)	vr2 = shr.u32 vr10, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 2047;"
        "V0@(pr0)	vr2 = and.u32 vr2, r44;"
        "V1@(pr0)	vr1 = or.u32 vr2, vr1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 2048;"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 14336;"
        "V0@(pr0)	vr2 = and.u32 vr10, r44;"
        "V1@(pr0)	vr3 = or.u32 vr1, r45;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = eq.s32 vr2, r46;"
        "V1@(pr0)	vr1 = sel vmsk1 vr3, vr1;"
        "}"
        "{"
        "V1@(pr0)	vr10 = mov.u32 vr1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr11;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 24368;"
        "pseudo@0	@pseudo imm_1 = 16740;"
        "pseudo@0	@pseudo imm_2 = 51331;"
        "pseudo@0	@pseudo imm_3 = 28105;"
        "V0@(pr0)	vr12 = mov.u32 r44;"
        "V1@(pr0)	vr13 = mov.u32 r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr13;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr1 = and.u32 vr10, r44;"
        "V1@(pr0)	vr2 = and.u32 vr12, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "V0@(pr0)	vmsk1 = ls.f32 vr1, r44;"
        "V1@(pr0)	vmsk2 = ls.f32 vr2, r44;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, r46;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, r46;"
        "}"
        "{"
        "V0@(pr0)	vr12 = sel vmsk2 vr12, r46;"
        "V1@(pr0)	vr13 = sel vmsk2 vr13, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr6 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr7 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 15;"
        "V0@(pr0)	vr10 = and.u32 vr10, r44;"
        "V1@(pr0)	vr12 = and.u32 vr12, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "V0@(pr0)	vr10 = or.u32 vr10, r44;"
        "V1@(pr0)	vr12 = or.u32 vr12, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 11;"
        "V1@(pr0)	vr12 = shl.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 21;"
        "V1@(pr0)	vr1 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 11;"
        "V0@(pr0)	vr12 = or.u32 vr12, vr1;"
        "V1@(pr0)	vr13 = shl.u32 vr13, r32;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr13, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = and.u32 vr12, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr13, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr12, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = and.u32 vr12, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr12, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 r46;"
        "V1@(pr0)	vr10 = mov.u32 vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr15 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr14 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr10;"
        "V1@(pr0)	vr17 = mov.u32 vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = or.u32 vr1, vr2;"
        "V1@(pr0)	vr14 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr15 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr14 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr15 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr14 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr12 = mov.u32 r46;"
        "V1@(pr0)	vr13 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "V1@(pr0)	vr15 = shr.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr15 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr14 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 vr15;"
        "V1@(pr0)	vr29 = mov.u32 vr14;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr15;"
        "V1@(pr0)	vr15 = mov.u32 vr0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr16;"
        "V1@(pr0)	vr11 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr12;"
        "V1@(pr0)	vr17 = mov.u32 vr13;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr29;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr15 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr14 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 vr15;"
        "V1@(pr0)	vr29 = mov.u32 vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr16;"
        "V1@(pr0)	vr13 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr10;"
        "V1@(pr0)	vr17 = mov.u32 vr11;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr28;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr16;"
        "V1@(pr0)	vr11 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr29 = shl.u32 vr29, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr15 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr14 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr12;"
        "V1@(pr0)	vr17 = mov.u32 vr13;"
        "}"
        "{"
        "V0@(pr0)	vr30 = mov.u32 vr14;"
        "V1@(pr0)	vr14 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr31 = mov.u32 vr17;"
        "V1@(pr0)	vr17 = mov.u32 vr16;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr11;"
        "V1@(pr0)	vr15 = mov.u32 vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr16;"
        "V1@(pr0)	vr11 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr13;"
        "V1@(pr0)	vr13 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr28;"
        "V1@(pr0)	vr17 = mov.u32 vr29;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr30;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr13;"
        "V1@(pr0)	vr15 = mov.u32 vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr13 = mov.u32 vr17;"
        "V1@(pr0)	vr12 = mov.u32 vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "V0@(pr0)	vr3 = xor.u32 vr6, vr7;"
        "V1@(pr0)	vr3 = and.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr3 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 2047;"
        "pseudo@0	@pseudo imm_3 = 0;"
        "V0@(pr0)	vr7 = and.u32 vr7, r45;"
        "V1@(pr0)	vr6 = and.u32 vr6, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 1023;"
        "pseudo@0	@pseudo imm_3 = 0;"
        "V0@(pr0)	vr6 = add.s32 vr6, vr7;"
        "V1@(pr0)	vr6 = sub.s32 vr6, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 16;"
        "V0@(pr0)	vr1 = and.u32 vr10, r45;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr6, r48;"
        "V1@(pr0)	vmsk1 = eq.s32 vr1, r46;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr2, vr6;"
        "}"
        "{"
        "V1@(pr0)	vr2 = shl.u32 vr10, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V1@(pr0)	vr5 = shr.u32 vr11, r32;"
        "}"
        "{"
        "V0@(pr0)	vr2 = or.u32 vr2, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V1@(pr0)	vr5 = shl.u32 vr13, r32;"
        "}"
        "{"
        "V1@(pr0)	vr4 = shl.u32 vr11, r48;"
        "}"
        "{"
        "V0@(pr0)	vr5 = or.u32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr2;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr5;"
        "}"
        "{"
        "V1@(pr0)	vr2 = shl.u32 vr12, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V1@(pr0)	vr5 = shr.u32 vr13, r32;"
        "}"
        "{"
        "V0@(pr0)	vr2 = or.u32 vr2, vr5;"
        "}"
        "{"
        "V1@(pr0)	vr5 = shl.u32 vr13, r48;"
        "}"
        "{"
        "V0@(pr0)	vr12 = sel vmsk1 vr12, vr2;"
        "V1@(pr0)	vr13 = sel vmsk1 vr13, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 15;"
        "V0@(pr0)	vr1 = and.u32 vr10, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr2 = shl.u32 vr6, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = gt.s32 vr6, r46;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr10, vr3;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32768;"
        "V0@(pr0)	vr1 = add.s32 vr11, r48;"
        "V1@(pr0)	vmsk1 = gt.s32 vr12, r44;"
        "}"
        "{"
        "V0@(pr0)	vr11 = sel vmsk1 vr11, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr1 = and.u32 vr11, r48;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr11;"
        "V1@(pr0)	vr15 = mov.u32 vr10;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr1;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32768;"
        "V1@(pr0)	vmsk1 = eq.s32 vr12, r45;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr16;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr17;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr28 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr29 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr30 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr31 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr1 = and.u32 vr28, r44;"
        "V1@(pr0)	vr2 = and.u32 vr30, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = lseq.s32 vr1, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 8;"
        "V1@(pr0)	vr1 = or.u32 vr28, r45;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr1, vr10;"
        "V1@(pr0)	vr11 = sel vmsk1 vr29, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = lseq.s32 vr2, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 8;"
        "V1@(pr0)	vr1 = or.u32 vr30, r45;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr1, vr10;"
        "V1@(pr0)	vr11 = sel vmsk1 vr31, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32768;"
        "V0@(pr0)	vr3 = xor.u32 vr28, vr30;"
        "V1@(pr0)	vr3 = and.u32 vr3, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = eq.f32 vr1, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32760;"
        "V0@(pr0)	vr5 = mov.u32 r44;"
        "}"
        "{"
        "V0@(pr0)	vr4 = xor.u32 vr1, vr3;"
        "}"
        "{"
        "V1@(pr0)	vmsk2 = eq.s32 vr2, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr5;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = eq.f32 vr2, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32760;"
        "V0@(pr0)	vr5 = mov.u32 r44;"
        "}"
        "{"
        "V0@(pr0)	vr4 = xor.u32 vr2, vr3;"
        "}"
        "{"
        "V1@(pr0)	vmsk2 = eq.s32 vr1, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr5;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr11;"
        "}"
        "{"
        "V1@(pr0)	vmsk1 = eq.f32 vr1, r46;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr3;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, r46;"
        "}"
        "{"
        "V1@(pr0)	vmsk1 = eq.f32 vr2, r46;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr3;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 49152;"
        "pseudo@0	@pseudo imm_2 = 3;"
        "V0@(pr0)	vr13 = and.u32 vr10, r44;"
        "V1@(pr0)	vr1 = shl.u32 vr10, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 16383;"
        "V0@(pr0)	vr1 = and.u32 vr1, r44;"
        "V1@(pr0)	vr13 = or.u32 vr1, vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 29;"
        "V1@(pr0)	vr1 = shr.u32 vr11, r32;"
        "}"
        "{"
        "V0@(pr0)	vr13 = or.u32 vr1, vr13;"
        "}"
        "{"
        "V0@(pr0)	vr13 = cvtftoint.s32 vr13, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 128;"
        "V0@(pr0)	vr13 = add.s32 vr13, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 24;"
        "V1@(pr0)	vr13 = shra.s32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr13;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr1 = cvtinttof.f32 vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 29;"
        "V1@(pr0)	vr11 = shl.u32 vr1, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 49152;"
        "pseudo@0	@pseudo imm_2 = 3;"
        "V0@(pr0)	vr10 = and.u32 vr1, r44;"
        "V1@(pr0)	vr2 = shr.u32 vr1, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 2047;"
        "V0@(pr0)	vr2 = and.u32 vr2, r44;"
        "V1@(pr0)	vr10 = or.u32 vr2, vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 2048;"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 14336;"
        "V0@(pr0)	vr2 = and.u32 vr1, r44;"
        "V1@(pr0)	vr3 = or.u32 vr10, r45;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = eq.s32 vr2, r46;"
        "V1@(pr0)	vr10 = sel vmsk1 vr3, vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8699;"
        "pseudo@0	@pseudo imm_1 = 16377;"
        "pseudo@0	@pseudo imm_2 = 11544;"
        "pseudo@0	@pseudo imm_3 = 21572;"
        "V0@(pr0)	vr12 = mov.u32 r44;"
        "V1@(pr0)	vr13 = mov.u32 r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr13;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr1 = and.u32 vr10, r44;"
        "V1@(pr0)	vr2 = and.u32 vr12, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "V0@(pr0)	vmsk1 = ls.f32 vr1, r44;"
        "V1@(pr0)	vmsk2 = ls.f32 vr2, r44;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, r46;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, r46;"
        "}"
        "{"
        "V0@(pr0)	vr12 = sel vmsk2 vr12, r46;"
        "V1@(pr0)	vr13 = sel vmsk2 vr13, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr6 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr7 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 15;"
        "V0@(pr0)	vr10 = and.u32 vr10, r44;"
        "V1@(pr0)	vr12 = and.u32 vr12, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "V0@(pr0)	vr10 = or.u32 vr10, r44;"
        "V1@(pr0)	vr12 = or.u32 vr12, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 11;"
        "V1@(pr0)	vr12 = shl.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 21;"
        "V1@(pr0)	vr1 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 11;"
        "V0@(pr0)	vr12 = or.u32 vr12, vr1;"
        "V1@(pr0)	vr13 = shl.u32 vr13, r32;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr13, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = and.u32 vr12, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr13, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr12, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = and.u32 vr12, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr12, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 r46;"
        "V1@(pr0)	vr10 = mov.u32 vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr15 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr14 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr10;"
        "V1@(pr0)	vr17 = mov.u32 vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = or.u32 vr1, vr2;"
        "V1@(pr0)	vr14 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr15 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr14 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr15 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr14 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr12 = mov.u32 r46;"
        "V1@(pr0)	vr13 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "V1@(pr0)	vr15 = shr.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr15 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr14 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 vr15;"
        "V1@(pr0)	vr29 = mov.u32 vr14;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr15;"
        "V1@(pr0)	vr15 = mov.u32 vr0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr16;"
        "V1@(pr0)	vr11 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr12;"
        "V1@(pr0)	vr17 = mov.u32 vr13;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr29;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr15 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr14 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 vr15;"
        "V1@(pr0)	vr29 = mov.u32 vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr16;"
        "V1@(pr0)	vr13 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr10;"
        "V1@(pr0)	vr17 = mov.u32 vr11;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr28;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr16;"
        "V1@(pr0)	vr11 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr29 = shl.u32 vr29, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr15 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr14 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr12;"
        "V1@(pr0)	vr17 = mov.u32 vr13;"
        "}"
        "{"
        "V0@(pr0)	vr30 = mov.u32 vr14;"
        "V1@(pr0)	vr14 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr31 = mov.u32 vr17;"
        "V1@(pr0)	vr17 = mov.u32 vr16;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr11;"
        "V1@(pr0)	vr15 = mov.u32 vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr16;"
        "V1@(pr0)	vr11 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr13;"
        "V1@(pr0)	vr13 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr28;"
        "V1@(pr0)	vr17 = mov.u32 vr29;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr30;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr13;"
        "V1@(pr0)	vr15 = mov.u32 vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr13 = mov.u32 vr17;"
        "V1@(pr0)	vr12 = mov.u32 vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "V0@(pr0)	vr3 = xor.u32 vr6, vr7;"
        "V1@(pr0)	vr3 = and.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr3 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 2047;"
        "pseudo@0	@pseudo imm_3 = 0;"
        "V0@(pr0)	vr7 = and.u32 vr7, r45;"
        "V1@(pr0)	vr6 = and.u32 vr6, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 1023;"
        "pseudo@0	@pseudo imm_3 = 0;"
        "V0@(pr0)	vr6 = add.s32 vr6, vr7;"
        "V1@(pr0)	vr6 = sub.s32 vr6, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 16;"
        "V0@(pr0)	vr1 = and.u32 vr10, r45;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr6, r48;"
        "V1@(pr0)	vmsk1 = eq.s32 vr1, r46;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr2, vr6;"
        "}"
        "{"
        "V1@(pr0)	vr2 = shl.u32 vr10, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V1@(pr0)	vr5 = shr.u32 vr11, r32;"
        "}"
        "{"
        "V0@(pr0)	vr2 = or.u32 vr2, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V1@(pr0)	vr5 = shl.u32 vr13, r32;"
        "}"
        "{"
        "V1@(pr0)	vr4 = shl.u32 vr11, r48;"
        "}"
        "{"
        "V0@(pr0)	vr5 = or.u32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr2;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr5;"
        "}"
        "{"
        "V1@(pr0)	vr2 = shl.u32 vr12, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V1@(pr0)	vr5 = shr.u32 vr13, r32;"
        "}"
        "{"
        "V0@(pr0)	vr2 = or.u32 vr2, vr5;"
        "}"
        "{"
        "V1@(pr0)	vr5 = shl.u32 vr13, r48;"
        "}"
        "{"
        "V0@(pr0)	vr12 = sel vmsk1 vr12, vr2;"
        "V1@(pr0)	vr13 = sel vmsk1 vr13, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 15;"
        "V0@(pr0)	vr1 = and.u32 vr10, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr2 = shl.u32 vr6, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = gt.s32 vr6, r46;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr10, vr3;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32768;"
        "V0@(pr0)	vr1 = add.s32 vr11, r48;"
        "V1@(pr0)	vmsk1 = gt.s32 vr12, r44;"
        "}"
        "{"
        "V0@(pr0)	vr11 = sel vmsk1 vr11, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr1 = and.u32 vr11, r48;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr11;"
        "V1@(pr0)	vr15 = mov.u32 vr10;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr1;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32768;"
        "V1@(pr0)	vmsk1 = eq.s32 vr12, r45;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr16;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr17;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr28 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr29 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr30 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr31 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr1 = and.u32 vr28, r44;"
        "V1@(pr0)	vr2 = and.u32 vr30, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = lseq.s32 vr1, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 8;"
        "V1@(pr0)	vr1 = or.u32 vr28, r45;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr1, vr10;"
        "V1@(pr0)	vr11 = sel vmsk1 vr29, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = lseq.s32 vr2, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 8;"
        "V1@(pr0)	vr1 = or.u32 vr30, r45;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr1, vr10;"
        "V1@(pr0)	vr11 = sel vmsk1 vr31, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32768;"
        "V0@(pr0)	vr3 = xor.u32 vr28, vr30;"
        "V1@(pr0)	vr3 = and.u32 vr3, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = eq.f32 vr1, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32760;"
        "V0@(pr0)	vr5 = mov.u32 r44;"
        "}"
        "{"
        "V0@(pr0)	vr4 = xor.u32 vr1, vr3;"
        "}"
        "{"
        "V1@(pr0)	vmsk2 = eq.s32 vr2, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr5;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = eq.f32 vr2, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32760;"
        "V0@(pr0)	vr5 = mov.u32 r44;"
        "}"
        "{"
        "V0@(pr0)	vr4 = xor.u32 vr2, vr3;"
        "}"
        "{"
        "V1@(pr0)	vmsk2 = eq.s32 vr1, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr5;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr11;"
        "}"
        "{"
        "V1@(pr0)	vmsk1 = eq.f32 vr1, r46;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr3;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, r46;"
        "}"
        "{"
        "V1@(pr0)	vmsk1 = eq.f32 vr2, r46;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr3;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32768;"
        "V0@(pr0)	vr12 = mov.u32 vr11;"
        "V1@(pr0)	vr13 = xor.u32 vr10, r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr1 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr11 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr10 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr1;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 32767;"
        "V0@(pr0)	vr14 = and.u32 vr10, r44;"
        "V1@(pr0)	vr15 = and.u32 vr11, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 32767;"
        "V0@(pr0)	vr16 = and.u32 vr12, r44;"
        "V1@(pr0)	vr17 = and.u32 vr13, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8192;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 5120;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr15;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 6144;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 7168;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = gt.s32 vr17, vr15;"
        "V1@(pr0)	vmsk1 = eq.s32 vr17, vr15;"
        "}"
        "{"
        "V0@(pr0)	vmsk2 = gt.s32 vr16, vr14;"
        "}"
        "{"
        "V0@(pr0)	vr1 = sel vmsk2 vr13, vr11;"
        "V1@(pr0)	vr0 = sel vmsk2 vr12, vr10;"
        "}"
        "{"
        "V0@(pr0)	vr1 = sel vmsk1 vr13, vr1;"
        "V1@(pr0)	vr0 = sel vmsk1 vr12, vr0;"
        "}"
        "{"
        "V0@(pr0)	vr1 = sel vmsk0 vr1, vr11;"
        "V1@(pr0)	vr0 = sel vmsk0 vr0, vr10;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = eq.s32 vr1, vr11;"
        "V1@(pr0)	vmsk1 = eq.s32 vr0, vr10;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr7 = sel vmsk0 vr5, r48;"
        "V1@(pr0)	vr6 = sel vmsk1 vr5, r48;"
        "}"
        "{"
        "V0@(pr0)	vr7 = and.u32 vr6, vr7;"
        "V1@(pr0)	vmsk0 = eq.s32 vr7, r48;"
        "}"
        "{"
        "V0@(pr0)	vr15 = sel vmsk0 vr11, vr13;"
        "V1@(pr0)	vr14 = sel vmsk0 vr10, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr1;"
        "V1@(pr0)	vr16 = mov.u32 vr0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2047;"
        "pseudo@0	@pseudo imm_1 = 20;"
        "V0@(pr0)	vr7 = mov.u32 r32;"
        "V1@(pr0)	vr6 = mov.u32 r33;"
        "}"
        "{"
        "V1@(pr0)	vr0 = shr.u32 vr15, vr6;"
        "}"
        "{"
        "V0@(pr0)	vr0 = and.u32 vr0, vr7;"
        "}"
        "{"
        "V1@(pr0)	vr1 = shr.u32 vr17, vr6;"
        "}"
        "{"
        "V0@(pr0)	vr1 = and.u32 vr1, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 15;"
        "V0@(pr0)	vr2 = and.u32 vr14, r44;"
        "V1@(pr0)	vr3 = and.u32 vr15, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 15;"
        "V0@(pr0)	vr4 = and.u32 vr16, r44;"
        "V1@(pr0)	vr5 = and.u32 vr17, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32768;"
        "V0@(pr0)	vr6 = and.u32 vr15, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32768;"
        "V0@(pr0)	vr7 = xor.u32 vr15, vr17;"
        "V1@(pr0)	vr7 = and.u32 vr7, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "pseudo@0	@pseudo imm_2 = 3;"
        "V0@(pr0)	vr3 = or.u32 vr3, r44;"
        "V1@(pr0)	vr3 = shl.u32 vr3, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 57344;"
        "pseudo@0	@pseudo imm_2 = 29;"
        "V0@(pr0)	vr28 = and.u32 vr2, r44;"
        "V1@(pr0)	vr28 = shr.u32 vr28, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3;"
        "V0@(pr0)	vr3 = or.u32 vr3, vr28;"
        "V1@(pr0)	vr2 = shl.u32 vr2, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "pseudo@0	@pseudo imm_2 = 3;"
        "V0@(pr0)	vr5 = or.u32 vr5, r44;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 57344;"
        "pseudo@0	@pseudo imm_2 = 29;"
        "V0@(pr0)	vr28 = and.u32 vr4, r44;"
        "V1@(pr0)	vr28 = shr.u32 vr28, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3;"
        "V0@(pr0)	vr5 = or.u32 vr5, vr28;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 64;"
        "V0@(pr0)	vr28 = sub.s32 vr0, vr1;"
        "V1@(pr0)	vr29 = mov.u32 r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr29 = sub.s32 vr29, vr28;"
        "V1@(pr0)	vmsk0 = gt.s32 vr29, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr31 = mov.u32 r32;"
        "V1@(pr0)	vr31 = sub.s32 vr31, vr29;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr30 = mov.u32 r44;"
        "}"
        "{"
        "V1@(pr0)	vr30 = shl.u32 vr30, vr31;"
        "}"
        "{"
        "V0@(pr0)	vr30 = and.u32 vr4, vr30;"
        "V1@(pr0)	vr30 = shr.u32 vr30, vr31;"
        "}"
        "{"
        "V1@(pr0)	vr11 = shl.u32 vr5, vr29;"
        "}"
        "{"
        "V0@(pr0)	vr11 = or.u32 vr11, vr30;"
        "V1@(pr0)	vr10 = shl.u32 vr4, vr29;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V1@(pr0)	vr30 = sub.s32 vr29, r32;"
        "}"
        "{"
        "V1@(pr0)	vr12 = shl.u32 vr4, vr30;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk0 vr10, r46;"
        "V1@(pr0)	vr11 = sel vmsk0 vr11, vr12;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = eq.s32 vr10, r46;"
        "V1@(pr0)	vmsk1 = eq.s32 vr11, r46;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 r46;"
        "V1@(pr0)	vr17 = sel vmsk0 vr11, r48;"
        "}"
        "{"
        "V0@(pr0)	vr16 = sel vmsk1 vr11, r48;"
        "V1@(pr0)	vr17 = add.s32 vr16, vr17;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vmsk0 = eq.s32 vr17, r32;"
        "V1@(pr0)	vr17 = sel vmsk0 vr11, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V1@(pr0)	vmsk0 = gt.s32 vr28, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 65535;"
        "V0@(pr0)	vr31 = mov.u32 r45;"
        "V1@(pr0)	vr30 = mov.u32 r32;"
        "}"
        "{"
        "V0@(pr0)	vr30 = sub.s32 vr30, vr28;"
        "V1@(pr0)	vr31 = shr.u32 vr31, vr30;"
        "}"
        "{"
        "V0@(pr0)	vr31 = and.u32 vr31, vr5;"
        "V1@(pr0)	vr31 = shl.u32 vr31, vr30;"
        "}"
        "{"
        "V1@(pr0)	vr10 = shr.u32 vr4, vr28;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr10, vr31;"
        "V1@(pr0)	vr11 = shr.u32 vr5, vr28;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr31 = sub.s32 vr28, r32;"
        "V1@(pr0)	vr12 = shr.u32 vr5, vr31;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk0 vr10, vr12;"
        "V1@(pr0)	vr11 = sel vmsk0 vr11, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 64;"
        "V0@(pr0)	vr10 = or.u32 vr10, vr17;"
        "V1@(pr0)	vmsk0 = ls.s32 vr28, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 r46;"
        "V1@(pr0)	vr11 = sel vmsk0 vr17, vr11;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 r48;"
        "V1@(pr0)	vr10 = sel vmsk0 vr17, vr10;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = eq.s32 vr28, r36;"
        "V1@(pr0)	vr4 = sel vmsk0 vr10, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "V0@(pr0)	vr5 = sel vmsk0 vr11, vr5;"
        "V1@(pr0)	vr10 = and.u32 vr2, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr12 = and.u32 vr4, r33;"
        "V1@(pr0)	vr11 = shr.u32 vr2, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr13 = shr.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = gteq.s32 vr10, vr12;"
        "V1@(pr0)	vr28 = mov.u32 r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr17 = sel vmsk0 vr28, r46;"
        "V1@(pr0)	vr17 = shl.u32 vr17, r32;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr10, vr17;"
        "V1@(pr0)	vr14 = sub.s32 vr10, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr15 = add.s32 vr13, r48;"
        "V1@(pr0)	vr13 = sel vmsk0 vr15, vr13;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = gteq.s32 vr11, vr13;"
        "V1@(pr0)	vr17 = sel vmsk0 vr28, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr17 = shl.u32 vr17, r32;"
        "}"
        "{"
        "V0@(pr0)	vr11 = or.u32 vr11, vr17;"
        "V1@(pr0)	vr15 = sub.s32 vr11, vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr14, vr15;"
        "}"
        "{"
        "V0@(pr0)	vr17 = add.s32 vr5, r48;"
        "V1@(pr0)	vr17 = sel vmsk0 vr17, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr11 = sub.s32 vr3, vr17;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = eq.s32 vr11, r46;"
        "V1@(pr0)	vr31 = count.u32 vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr30 = count.u32 vr10;"
        "V1@(pr0)	vr30 = add.s32 vr30, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr31 = sel vmsk0 vr31, vr30;"
        "V1@(pr0)	vr31 = sub.s32 vr31, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vmsk6 = ls.s32 vr31, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr30 = mov.u32 r32;"
        "V1@(pr0)	vr30 = sub.s32 vr30, vr31;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr29 = mov.u32 r44;"
        "V1@(pr0)	vr29 = shl.u32 vr29, vr30;"
        "}"
        "{"
        "V0@(pr0)	vr29 = and.u32 vr29, vr10;"
        "V1@(pr0)	vr29 = shr.u32 vr29, vr30;"
        "}"
        "{"
        "V1@(pr0)	vr13 = shl.u32 vr11, vr31;"
        "}"
        "{"
        "V0@(pr0)	vr13 = or.u32 vr13, vr29;"
        "V1@(pr0)	vr12 = shl.u32 vr10, vr31;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr30 = mov.u32 r32;"
        "V1@(pr0)	vr30 = sub.s32 vr31, vr30;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr14 = shl.u32 vr10, vr30;"
        "}"
        "{"
        "V0@(pr0)	vr12 = sel vmsk6 vr15, vr12;"
        "V1@(pr0)	vr13 = sel vmsk6 vr14, vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 128;"
        "V0@(pr0)	vmsk0 = ls.s32 vr11, r36;"
        "V1@(pr0)	vr10 = sel vmsk0 vr10, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr15 = sub.s32 vr0, vr31;"
        "}"
        "{"
        "V0@(pr0)	vr11 = sel vmsk0 vr11, vr13;"
        "V1@(pr0)	vr15 = sel vmsk0 vr0, vr15;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr12 = and.u32 vr2, r33;"
        "V1@(pr0)	vr13 = shr.u32 vr2, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr16 = and.u32 vr4, r33;"
        "V1@(pr0)	vr17 = shr.u32 vr4, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr28 = add.s32 vr12, vr16;"
        "V1@(pr0)	vr14 = shr.u32 vr28, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "V0@(pr0)	vr28 = and.u32 vr28, r32;"
        "}"
        "{"
        "V0@(pr0)	vr29 = add.s32 vr13, vr17;"
        "V1@(pr0)	vr29 = add.s32 vr29, vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr29, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "V0@(pr0)	vr29 = and.u32 vr29, r32;"
        "V1@(pr0)	vr29 = shl.u32 vr29, r33;"
        "}"
        "{"
        "V0@(pr0)	vr12 = or.u32 vr29, vr28;"
        "V1@(pr0)	vr13 = add.s32 vr3, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr13 = add.s32 vr13, vr14;"
        "V1@(pr0)	vr28 = and.u32 vr12, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V0@(pr0)	vr14 = and.u32 vr13, r48;"
        "V1@(pr0)	vr14 = shl.u32 vr14, r32;"
        "}"
        "{"
        "V1@(pr0)	vr16 = shr.u32 vr12, r48;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr16, vr14;"
        "V1@(pr0)	vr17 = shr.u32 vr13, r48;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr16, vr28;"
        "V1@(pr0)	vr14 = add.s32 vr0, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 256;"
        "V0@(pr0)	vr28 = and.u32 vr13, r36;"
        "V1@(pr0)	vmsk0 = eq.s32 vr28, r46;"
        "}"
        "{"
        "V0@(pr0)	vr12 = sel vmsk0 vr16, vr12;"
        "V1@(pr0)	vr13 = sel vmsk0 vr17, vr13;"
        "}"
        "{"
        "V0@(pr0)	vr14 = sel vmsk0 vr14, vr0;"
        "V1@(pr0)	vmsk1 = eq.s32 vr7, r46;"
        "}"
        "{"
        "V0@(pr0)	vr2 = sel vmsk1 vr10, vr12;"
        "V1@(pr0)	vr3 = sel vmsk1 vr11, vr13;"
        "}"
        "{"
        "V0@(pr0)	vr0 = sel vmsk1 vr15, vr14;"
        "V1@(pr0)	vr31 = mov.u32 r48;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr2, r48;"
        "V1@(pr0)	vmsk0 = lseq.s32 vr0, r46;"
        "}"
        "{"
        "V0@(pr0)	vr2 = sel vmsk0 vr2, vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 7;"
        "pseudo@0	@pseudo imm_1 = 3;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 65535;"
        "V0@(pr0)	vr17 = and.u32 vr2, r32;"
        "V1@(pr0)	vr31 = mov.u32 r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 29;"
        "V1@(pr0)	vr31 = shr.u32 vr31, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 29;"
        "V0@(pr0)	vr31 = and.u32 vr3, vr31;"
        "V1@(pr0)	vr31 = shl.u32 vr31, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3;"
        "V1@(pr0)	vr10 = shr.u32 vr2, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3;"
        "V0@(pr0)	vr10 = or.u32 vr10, vr31;"
        "V1@(pr0)	vr11 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 15;"
        "V0@(pr0)	vr10 = and.u32 vr10, r44;"
        "V1@(pr0)	vr11 = and.u32 vr11, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr28 = shl.u32 vr0, r32;"
        "}"
        "{"
        "V0@(pr0)	vr11 = or.u32 vr11, vr28;"
        "V1@(pr0)	vr11 = or.u32 vr11, vr6;"
        "}"
        "{"
        "V0@(pr0)	vr12 = add.s32 vr10, r48;"
        "V1@(pr0)	vr13 = and.u32 vr10, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "V0@(pr0)	vmsk0 = gt.s32 vr17, r32;"
        "V1@(pr0)	vmsk1 = eq.s32 vr17, r32;"
        "}"
        "{"
        "V0@(pr0)	vr13 = add.s32 vr10, vr13;"
        "V1@(pr0)	vr13 = sel vmsk1 vr10, vr13;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk0 vr13, vr12;"
        "V1@(pr0)	vr0 = mov.u32 vr10;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mov.u32 vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr10 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr11 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr12 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr13 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr14 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 5120;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr15 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 6144;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr16 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 7168;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr17 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8192;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = eq.s32 vr17, r46;"
        "V1@(pr0)	vmsk1 = eq.s32 vr16, r46;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 r46;"
        "V1@(pr0)	vr7 = sel vmsk0 vr28, r48;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr28, r48;"
        "V1@(pr0)	vr7 = add.s32 vr6, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vmsk0 = eq.s32 vr7, r32;"
        "V1@(pr0)	vr30 = sel vmsk0 vr0, vr10;"
        "}"
        "{"
        "V0@(pr0)	vr31 = sel vmsk0 vr1, vr11;"
        "V1@(pr0)	vr6 = and.u32 vr10, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr7 = and.u32 vr11, vr13;"
        "V1@(pr0)	vr5 = sel vmsk0 vr13, vr7;"
        "}"
        "{"
        "V0@(pr0)	vr4 = sel vmsk0 vr12, vr6;"
        "V1@(pr0)	vmsk0 = eq.s32 vr14, r46;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = eq.s32 vr15, r46;"
        "V1@(pr0)	vr6 = sel vmsk0 vr28, r48;"
        "}"
        "{"
        "V0@(pr0)	vr7 = sel vmsk1 vr28, r48;"
        "V1@(pr0)	vr7 = add.s32 vr6, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vmsk0 = eq.s32 vr7, r32;"
        "V1@(pr0)	vr31 = sel vmsk0 vr31, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr30 = sel vmsk0 vr30, r34;"
        "V1@(pr0)	vr7 = mov.u32 r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 15;"
        "V0@(pr0)	vr6 = mov.u32 r44;"
        "V1@(pr0)	vr7 = xor.u32 vr7, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr6 = xor.u32 vr6, r44;"
        "V1@(pr0)	vmsk0 = eq.s32 vr16, vr6;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mov.u32 vr7;"
        "V1@(pr0)	vr4 = mov.u32 vr6;"
        "}"
        "{"
        "V0@(pr0)	vmsk6 = eq.s32 vr14, vr6;"
        "V1@(pr0)	vmsk7 = eq.s32 vr15, vr7;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = eq.s32 vr17, vr7;"
        "V1@(pr0)	vr7 = sel vmsk0 vr28, r48;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr28, r48;"
        "V1@(pr0)	vr7 = add.s32 vr6, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vmsk0 = eq.s32 vr7, r32;"
        "V1@(pr0)	vr31 = sel vmsk0 vr31, vr13;"
        "}"
        "{"
        "V0@(pr0)	vr30 = sel vmsk0 vr30, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr7 = sel vmsk7 vr28, r48;"
        "V1@(pr0)	vr6 = sel vmsk6 vr28, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vr7 = add.s32 vr6, vr7;"
        "V1@(pr0)	vmsk7 = eq.s32 vr7, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr3 = or.u32 vr5, r36;"
        "V1@(pr0)	vr6 = xor.u32 vr10, vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32768;"
        "V0@(pr0)	vr7 = xor.u32 vr11, vr13;"
        "V1@(pr0)	vmsk0 = eq.s32 vr7, r36;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = eq.s32 vr6, r46;"
        "V1@(pr0)	vr7 = sel vmsk0 vr28, r48;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr28, r48;"
        "V1@(pr0)	vr7 = add.s32 vr6, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vmsk0 = eq.s32 vr7, r32;"
        "V1@(pr0)	vr7 = sel vmsk0 vr11, vr3;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk0 vr10, vr4;"
        "V1@(pr0)	vr30 = sel vmsk7 vr30, vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr31 = sel vmsk7 vr31, vr7;"
        "V1@(pr0)	vr7 = or.u32 vr13, r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk0 = eq.s32 vr16, r46;"
        "V1@(pr0)	vr6 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "V0@(pr0)	vr28 = and.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32752;"
        "V0@(pr0)	vmsk1 = gt.s32 vr6, r32;"
        "V1@(pr0)	vmsk2 = eq.s32 vr28, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk0 vr28, r48;"
        "V1@(pr0)	vr4 = sel vmsk2 vr28, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vr5 = add.s32 vr4, vr5;"
        "V1@(pr0)	vmsk2 = eq.s32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr7, vr31;"
        "V1@(pr0)	vr4 = sel vmsk2 vr12, vr30;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32752;"
        "V0@(pr0)	vmsk2 = eq.s32 vr6, r32;"
        "V1@(pr0)	vr4 = sel vmsk2 vr30, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr31, vr5;"
        "V1@(pr0)	vr31 = sel vmsk1 vr5, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr30 = sel vmsk1 vr4, vr12;"
        "V1@(pr0)	vr7 = or.u32 vr11, r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk0 = eq.s32 vr14, r46;"
        "V1@(pr0)	vr6 = shr.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "V0@(pr0)	vr28 = and.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32752;"
        "V0@(pr0)	vmsk1 = gt.s32 vr6, r32;"
        "V1@(pr0)	vmsk2 = eq.s32 vr28, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk0 vr28, r48;"
        "V1@(pr0)	vr4 = sel vmsk2 vr28, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vr5 = add.s32 vr4, vr5;"
        "V1@(pr0)	vmsk2 = eq.s32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr7, vr31;"
        "V1@(pr0)	vr4 = sel vmsk2 vr10, vr30;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32752;"
        "V0@(pr0)	vmsk2 = eq.s32 vr6, r32;"
        "V1@(pr0)	vr4 = sel vmsk2 vr30, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr31, vr5;"
        "V1@(pr0)	vr11 = sel vmsk1 vr5, vr7;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr4, vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr12 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr1 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V1@(pr0)	vr2 = shr.u32 vr1, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 5120;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr12;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 26;"
        "V1@(pr0)	vr1 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 15;"
        "V1@(pr0)	vr1 = and.u32 vr1, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 162;"
        "pseudo@0	@pseudo imm_3 = 0;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 28238;"
        "pseudo@0	@pseudo imm_1 = 63875;"
        "pseudo@0	@pseudo imm_2 = 10748;"
        "pseudo@0	@pseudo imm_3 = 17429;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "pseudo@0	@pseudo imm_2 = 41721;"
        "pseudo@0	@pseudo imm_3 = 0;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20036;"
        "pseudo@0	@pseudo imm_1 = 33646;"
        "pseudo@0	@pseudo imm_2 = 64551;"
        "pseudo@0	@pseudo imm_3 = 5417;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "pseudo@0	@pseudo imm_2 = 63875;"
        "pseudo@0	@pseudo imm_3 = 162;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 17429;"
        "pseudo@0	@pseudo imm_1 = 28238;"
        "pseudo@0	@pseudo imm_2 = 10071;"
        "pseudo@0	@pseudo imm_3 = 10748;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3;"
        "pseudo@0	@pseudo imm_2 = 33646;"
        "pseudo@0	@pseudo imm_3 = 41721;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 5417;"
        "pseudo@0	@pseudo imm_1 = 20036;"
        "pseudo@0	@pseudo imm_2 = 22481;"
        "pseudo@0	@pseudo imm_3 = 64551;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "pseudo@0	@pseudo imm_2 = 28238;"
        "pseudo@0	@pseudo imm_3 = 63875;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 10748;"
        "pseudo@0	@pseudo imm_1 = 17429;"
        "pseudo@0	@pseudo imm_2 = 53749;"
        "pseudo@0	@pseudo imm_3 = 10071;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 5;"
        "pseudo@0	@pseudo imm_2 = 20036;"
        "pseudo@0	@pseudo imm_3 = 33646;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 64551;"
        "pseudo@0	@pseudo imm_1 = 5417;"
        "pseudo@0	@pseudo imm_2 = 62772;"
        "pseudo@0	@pseudo imm_3 = 22481;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 6;"
        "pseudo@0	@pseudo imm_2 = 17429;"
        "pseudo@0	@pseudo imm_3 = 28238;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 10071;"
        "pseudo@0	@pseudo imm_1 = 10748;"
        "pseudo@0	@pseudo imm_2 = 13533;"
        "pseudo@0	@pseudo imm_3 = 53749;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 7;"
        "pseudo@0	@pseudo imm_2 = 5417;"
        "pseudo@0	@pseudo imm_3 = 20036;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 22481;"
        "pseudo@0	@pseudo imm_1 = 64551;"
        "pseudo@0	@pseudo imm_2 = 56768;"
        "pseudo@0	@pseudo imm_3 = 62772;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "pseudo@0	@pseudo imm_2 = 10748;"
        "pseudo@0	@pseudo imm_3 = 17429;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 53749;"
        "pseudo@0	@pseudo imm_1 = 10071;"
        "pseudo@0	@pseudo imm_2 = 49371;"
        "pseudo@0	@pseudo imm_3 = 13533;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 9;"
        "pseudo@0	@pseudo imm_2 = 64551;"
        "pseudo@0	@pseudo imm_3 = 5417;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 62772;"
        "pseudo@0	@pseudo imm_1 = 22481;"
        "pseudo@0	@pseudo imm_2 = 56162;"
        "pseudo@0	@pseudo imm_3 = 56768;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 10;"
        "pseudo@0	@pseudo imm_2 = 10071;"
        "pseudo@0	@pseudo imm_3 = 10748;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 13533;"
        "pseudo@0	@pseudo imm_1 = 53749;"
        "pseudo@0	@pseudo imm_2 = 25237;"
        "pseudo@0	@pseudo imm_3 = 49371;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 11;"
        "pseudo@0	@pseudo imm_2 = 22481;"
        "pseudo@0	@pseudo imm_3 = 64551;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 56768;"
        "pseudo@0	@pseudo imm_1 = 62772;"
        "pseudo@0	@pseudo imm_2 = 38297;"
        "pseudo@0	@pseudo imm_3 = 56162;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 12;"
        "pseudo@0	@pseudo imm_2 = 53749;"
        "pseudo@0	@pseudo imm_3 = 10071;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 49371;"
        "pseudo@0	@pseudo imm_1 = 13533;"
        "pseudo@0	@pseudo imm_2 = 39228;"
        "pseudo@0	@pseudo imm_3 = 25237;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 13;"
        "pseudo@0	@pseudo imm_2 = 62772;"
        "pseudo@0	@pseudo imm_3 = 22481;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 56162;"
        "pseudo@0	@pseudo imm_1 = 56768;"
        "pseudo@0	@pseudo imm_2 = 15427;"
        "pseudo@0	@pseudo imm_3 = 38297;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 14;"
        "pseudo@0	@pseudo imm_2 = 13533;"
        "pseudo@0	@pseudo imm_3 = 53749;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 25237;"
        "pseudo@0	@pseudo imm_1 = 49371;"
        "pseudo@0	@pseudo imm_2 = 17296;"
        "pseudo@0	@pseudo imm_3 = 39228;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 15;"
        "pseudo@0	@pseudo imm_2 = 56768;"
        "pseudo@0	@pseudo imm_3 = 62772;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 38297;"
        "pseudo@0	@pseudo imm_1 = 56162;"
        "pseudo@0	@pseudo imm_2 = 36929;"
        "pseudo@0	@pseudo imm_3 = 15427;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 23;"
        "V1@(pr0)	vr1 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 7;"
        "V1@(pr0)	vr1 = and.u32 vr1, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 255;"
        "pseudo@0	@pseudo imm_2 = 128;"
        "V0@(pr0)	vr10 = and.u32 vr10, r44;"
        "V1@(pr0)	vr10 = or.u32 vr10, r38;"
        "}"
        "{"
        "V1@(pr0)	vr10 = shl.u32 vr10, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr10;"
        "V1@(pr0)	vr13 = mov.u32 vr28;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr14;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr12, r54;"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr12, r32;"
        "}"
        "{"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr13, r32;"
        "}"
        "{"
        "V1@(pr0)	vr15 = and.u32 vr12, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr16;"
        "V1@(pr0)	vr13 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 vr13;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr10;"
        "V1@(pr0)	vr13 = mov.u32 vr30;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr14;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr12, r54;"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr12, r32;"
        "}"
        "{"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr13, r32;"
        "}"
        "{"
        "V1@(pr0)	vr15 = and.u32 vr12, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr16;"
        "V1@(pr0)	vr13 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr30 = mov.u32 vr12;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr10;"
        "V1@(pr0)	vr13 = mov.u32 vr29;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr14;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr12, r54;"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr12, r32;"
        "}"
        "{"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr13, r32;"
        "}"
        "{"
        "V1@(pr0)	vr15 = and.u32 vr12, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr16;"
        "V1@(pr0)	vr13 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr12;"
        "V1@(pr0)	vr17 = mov.u32 vr13;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 vr28;"
        "V1@(pr0)	vr14 = mov.u32 vr30;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8192;"
        "V0@(pr0)	vr1 = add.s32 vr2, r32;"
        "V1@(pr0)	vr1 = and.u32 vr1, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 14;"
        "V1@(pr0)	vr1 = shr.u32 vr1, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 14;"
        "V1@(pr0)	vr3 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V1@(pr0)	vr2 = sub.s32 vr2, vr3;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr16 = and.u32 vr16, r54;"
        "V1@(pr0)	vr2 = shl.u32 vr2, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr16, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr1;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32768;"
        "V0@(pr0)	vr12 = mov.u32 r44;"
        "V1@(pr0)	vmsk7 = gteq.f32 vr16, r46;"
        "}"
        "{"
        "V1@(pr0)	vr12 = sel vmsk7 vr12, r46;"
        "}"
        "{"
        "V0@(pr0)	vmsk3 = eq.s32 vr17, r46;"
        "V1@(pr0)	vr6 = sub.s32 vr17, r48;"
        "}"
        "{"
        "V1@(pr0)	vr7 = sub.s32 vr16, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr6 = sel vmsk3 vr6, r44;"
        "V1@(pr0)	vr7 = sel vmsk3 vr16, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr6 = xor.u32 vr6, r44;"
        "V1@(pr0)	vr7 = xor.u32 vr7, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr17 = sel vmsk7 vr6, vr17;"
        "V1@(pr0)	vr16 = sel vmsk7 vr7, vr16;"
        "}"
        "{"
        "V0@(pr0)	vr2 = count.u32 vr16;"
        "V1@(pr0)	vr1 = count.u32 vr17;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vmsk6 = ls.s32 vr2, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V0@(pr0)	vr6 = mov.u32 r32;"
        "V1@(pr0)	vr1 = sub.s32 vr6, vr1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 63;"
        "V0@(pr0)	vr6 = mov.u32 r32;"
        "V1@(pr0)	vr2 = sub.s32 vr6, vr2;"
        "}"
        "{"
        "V1@(pr0)	vr13 = sel vmsk6 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr0 = mov.u32 vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1023;"
        "V0@(pr0)	vr13 = add.s32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr13 = shl.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 20;"
        "V0@(pr0)	vmsk5 = lseq.s32 vr0, r32;"
        "V1@(pr0)	vmsk4 = lseq.s32 vr0, r33;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr6 = sub.s32 vr0, r32;"
        "V1@(pr0)	vr7 = mov.u32 r48;"
        "}"
        "{"
        "V1@(pr0)	vr6 = shl.u32 vr7, vr6;"
        "}"
        "{"
        "V1@(pr0)	vr6 = sub.s32 vr16, vr6;"
        "}"
        "{"
        "V1@(pr0)	vr7 = shl.u32 vr7, vr0;"
        "}"
        "{"
        "V1@(pr0)	vr7 = sub.s32 vr17, vr7;"
        "}"
        "{"
        "V1@(pr0)	vr5 = sel vmsk4 vr6, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "V0@(pr0)	vr6 = sub.s32 vr0, r32;"
        "V1@(pr0)	vr3 = shr.u32 vr5, vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 84;"
        "V0@(pr0)	vr6 = mov.u32 r32;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sub.s32 vr6, vr0;"
        "V1@(pr0)	vr4 = shl.u32 vr5, vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "V0@(pr0)	vr6 = sub.s32 vr0, r32;"
        "V1@(pr0)	vr6 = shr.u32 vr17, vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V0@(pr0)	vr4 = add.s32 vr4, vr6;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 vr3;"
        "V1@(pr0)	vr14 = mov.u32 vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "V0@(pr0)	vr6 = mov.u32 r32;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sub.s32 vr6, vr0;"
        "V1@(pr0)	vr3 = shl.u32 vr5, vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V0@(pr0)	vr6 = sub.s32 vr0, r32;"
        "V1@(pr0)	vr6 = shr.u32 vr17, vr6;"
        "}"
        "{"
        "V1@(pr0)	vr3 = add.s32 vr3, vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "V0@(pr0)	vr7 = mov.u32 r32;"
        "}"
        "{"
        "V0@(pr0)	vr7 = sub.s32 vr7, vr0;"
        "V1@(pr0)	vr4 = shl.u32 vr17, vr7;"
        "}"
        "{"
        "V0@(pr0)	vr15 = sel vmsk5 vr15, vr3;"
        "V1@(pr0)	vr14 = sel vmsk5 vr14, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V0@(pr0)	vr6 = mov.u32 r32;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sub.s32 vr6, vr0;"
        "V1@(pr0)	vr3 = shl.u32 vr5, vr6;"
        "}"
        "{"
        "V0@(pr0)	vr15 = sel vmsk4 vr15, vr3;"
        "V1@(pr0)	vr14 = sel vmsk4 vr14, r46;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr12, vr13;"
        "V1@(pr0)	vr10 = or.u32 vr10, vr15;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8699;"
        "pseudo@0	@pseudo imm_1 = 15385;"
        "pseudo@0	@pseudo imm_2 = 11544;"
        "pseudo@0	@pseudo imm_3 = 21572;"
        "V0@(pr0)	vr12 = mov.u32 r44;"
        "V1@(pr0)	vr13 = mov.u32 r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr13;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr1 = and.u32 vr10, r44;"
        "V1@(pr0)	vr2 = and.u32 vr12, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "V0@(pr0)	vmsk1 = ls.f32 vr1, r44;"
        "V1@(pr0)	vmsk2 = ls.f32 vr2, r44;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, r46;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, r46;"
        "}"
        "{"
        "V0@(pr0)	vr12 = sel vmsk2 vr12, r46;"
        "V1@(pr0)	vr13 = sel vmsk2 vr13, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr6 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr7 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 15;"
        "V0@(pr0)	vr10 = and.u32 vr10, r44;"
        "V1@(pr0)	vr12 = and.u32 vr12, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "V0@(pr0)	vr10 = or.u32 vr10, r44;"
        "V1@(pr0)	vr12 = or.u32 vr12, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 11;"
        "V1@(pr0)	vr12 = shl.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 21;"
        "V1@(pr0)	vr1 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 11;"
        "V0@(pr0)	vr12 = or.u32 vr12, vr1;"
        "V1@(pr0)	vr13 = shl.u32 vr13, r32;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr13, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = and.u32 vr12, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr13, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr12, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = and.u32 vr12, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr12, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 r46;"
        "V1@(pr0)	vr10 = mov.u32 vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr15 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr14 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr10;"
        "V1@(pr0)	vr17 = mov.u32 vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = or.u32 vr1, vr2;"
        "V1@(pr0)	vr14 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr15 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr14 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr15 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr14 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr12 = mov.u32 r46;"
        "V1@(pr0)	vr13 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "V1@(pr0)	vr15 = shr.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr15 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr14 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 vr15;"
        "V1@(pr0)	vr29 = mov.u32 vr14;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr15;"
        "V1@(pr0)	vr15 = mov.u32 vr0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr16;"
        "V1@(pr0)	vr11 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr12;"
        "V1@(pr0)	vr17 = mov.u32 vr13;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr29;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr15 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr14 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 vr15;"
        "V1@(pr0)	vr29 = mov.u32 vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr16;"
        "V1@(pr0)	vr13 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr10;"
        "V1@(pr0)	vr17 = mov.u32 vr11;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr28;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr16;"
        "V1@(pr0)	vr11 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr29 = shl.u32 vr29, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr15 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr14 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr12;"
        "V1@(pr0)	vr17 = mov.u32 vr13;"
        "}"
        "{"
        "V0@(pr0)	vr30 = mov.u32 vr14;"
        "V1@(pr0)	vr14 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr31 = mov.u32 vr17;"
        "V1@(pr0)	vr17 = mov.u32 vr16;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr11;"
        "V1@(pr0)	vr15 = mov.u32 vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr16;"
        "V1@(pr0)	vr11 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr13;"
        "V1@(pr0)	vr13 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr28;"
        "V1@(pr0)	vr17 = mov.u32 vr29;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr30;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr13;"
        "V1@(pr0)	vr15 = mov.u32 vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr13 = mov.u32 vr17;"
        "V1@(pr0)	vr12 = mov.u32 vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "V0@(pr0)	vr3 = xor.u32 vr6, vr7;"
        "V1@(pr0)	vr3 = and.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr3 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 2047;"
        "pseudo@0	@pseudo imm_3 = 0;"
        "V0@(pr0)	vr7 = and.u32 vr7, r45;"
        "V1@(pr0)	vr6 = and.u32 vr6, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 1023;"
        "pseudo@0	@pseudo imm_3 = 0;"
        "V0@(pr0)	vr6 = add.s32 vr6, vr7;"
        "V1@(pr0)	vr6 = sub.s32 vr6, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 16;"
        "V0@(pr0)	vr1 = and.u32 vr10, r45;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr6, r48;"
        "V1@(pr0)	vmsk1 = eq.s32 vr1, r46;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr2, vr6;"
        "}"
        "{"
        "V1@(pr0)	vr2 = shl.u32 vr10, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V1@(pr0)	vr5 = shr.u32 vr11, r32;"
        "}"
        "{"
        "V0@(pr0)	vr2 = or.u32 vr2, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V1@(pr0)	vr5 = shl.u32 vr13, r32;"
        "}"
        "{"
        "V1@(pr0)	vr4 = shl.u32 vr11, r48;"
        "}"
        "{"
        "V0@(pr0)	vr5 = or.u32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr2;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr5;"
        "}"
        "{"
        "V1@(pr0)	vr2 = shl.u32 vr12, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V1@(pr0)	vr5 = shr.u32 vr13, r32;"
        "}"
        "{"
        "V0@(pr0)	vr2 = or.u32 vr2, vr5;"
        "}"
        "{"
        "V1@(pr0)	vr5 = shl.u32 vr13, r48;"
        "}"
        "{"
        "V0@(pr0)	vr12 = sel vmsk1 vr12, vr2;"
        "V1@(pr0)	vr13 = sel vmsk1 vr13, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 15;"
        "V0@(pr0)	vr1 = and.u32 vr10, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr2 = shl.u32 vr6, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = gt.s32 vr6, r46;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr10, vr3;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32768;"
        "V0@(pr0)	vr1 = add.s32 vr11, r48;"
        "V1@(pr0)	vmsk1 = gt.s32 vr12, r44;"
        "}"
        "{"
        "V0@(pr0)	vr11 = sel vmsk1 vr11, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr1 = and.u32 vr11, r48;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr11;"
        "V1@(pr0)	vr15 = mov.u32 vr10;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr1;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32768;"
        "V1@(pr0)	vmsk1 = eq.s32 vr12, r45;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr16;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr17;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr28 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr29 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr30 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr31 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr1 = and.u32 vr28, r44;"
        "V1@(pr0)	vr2 = and.u32 vr30, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = lseq.s32 vr1, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 8;"
        "V1@(pr0)	vr1 = or.u32 vr28, r45;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr1, vr10;"
        "V1@(pr0)	vr11 = sel vmsk1 vr29, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = lseq.s32 vr2, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 8;"
        "V1@(pr0)	vr1 = or.u32 vr30, r45;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr1, vr10;"
        "V1@(pr0)	vr11 = sel vmsk1 vr31, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32768;"
        "V0@(pr0)	vr3 = xor.u32 vr28, vr30;"
        "V1@(pr0)	vr3 = and.u32 vr3, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = eq.f32 vr1, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32760;"
        "V0@(pr0)	vr5 = mov.u32 r44;"
        "}"
        "{"
        "V0@(pr0)	vr4 = xor.u32 vr1, vr3;"
        "}"
        "{"
        "V1@(pr0)	vmsk2 = eq.s32 vr2, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr5;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = eq.f32 vr2, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32760;"
        "V0@(pr0)	vr5 = mov.u32 r44;"
        "}"
        "{"
        "V0@(pr0)	vr4 = xor.u32 vr2, vr3;"
        "}"
        "{"
        "V1@(pr0)	vmsk2 = eq.s32 vr1, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr5;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr11;"
        "}"
        "{"
        "V1@(pr0)	vmsk1 = eq.f32 vr1, r46;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr3;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, r46;"
        "}"
        "{"
        "V1@(pr0)	vmsk1 = eq.f32 vr2, r46;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr3;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr12 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr5 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr4 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr1 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr2 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr3 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 5120;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32768;"
        "V0@(pr0)	vmsk1 = eq.s32 vr4, r48;"
        "V1@(pr0)	vr6 = xor.u32 vr10, r36;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 17136;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 32767;"
        "V0@(pr0)	vr6 = and.u32 vr5, r45;"
        "V1@(pr0)	vmsk4 = ls.f32 vr6, r44;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk4 vr10, vr2;"
        "V1@(pr0)	vr11 = sel vmsk4 vr11, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr13 = sel vmsk4 vr12, vr3;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 49152;"
        "pseudo@0	@pseudo imm_2 = 3;"
        "V0@(pr0)	vr4 = and.u32 vr10, r44;"
        "V1@(pr0)	vr1 = shl.u32 vr10, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 16383;"
        "V0@(pr0)	vr1 = and.u32 vr1, r44;"
        "V1@(pr0)	vr4 = or.u32 vr1, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 29;"
        "V1@(pr0)	vr1 = shr.u32 vr11, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = or.u32 vr1, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr13;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 29;"
        "V1@(pr0)	vr12 = shl.u32 vr4, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 49152;"
        "pseudo@0	@pseudo imm_2 = 3;"
        "V0@(pr0)	vr6 = and.u32 vr4, r44;"
        "V1@(pr0)	vr2 = shr.u32 vr4, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 2047;"
        "V0@(pr0)	vr2 = and.u32 vr2, r44;"
        "V1@(pr0)	vr6 = or.u32 vr2, vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 2048;"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 14336;"
        "V0@(pr0)	vr2 = and.u32 vr4, r44;"
        "V1@(pr0)	vr3 = or.u32 vr6, r45;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = eq.s32 vr2, r46;"
        "V1@(pr0)	vr6 = sel vmsk1 vr3, vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32768;"
        "V1@(pr0)	vr13 = xor.u32 vr6, r36;"
        "}"
        "{"
        "V0@(pr0)	vr2 = mov.u32 vr10;"
        "V1@(pr0)	vr10 = mov.u32 vr11;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 32767;"
        "V0@(pr0)	vr14 = and.u32 vr10, r44;"
        "V1@(pr0)	vr15 = and.u32 vr11, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 32767;"
        "V0@(pr0)	vr16 = and.u32 vr12, r44;"
        "V1@(pr0)	vr17 = and.u32 vr13, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8192;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 5120;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr15;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 6144;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 7168;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VS@(pr0)	[vmem:1+2,4,0] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = gt.s32 vr17, vr15;"
        "V1@(pr0)	vmsk1 = eq.s32 vr17, vr15;"
        "}"
        "{"
        "V0@(pr0)	vmsk2 = gt.s32 vr16, vr14;"
        "}"
        "{"
        "V0@(pr0)	vr1 = sel vmsk2 vr13, vr11;"
        "V1@(pr0)	vr0 = sel vmsk2 vr12, vr10;"
        "}"
        "{"
        "V0@(pr0)	vr1 = sel vmsk1 vr13, vr1;"
        "V1@(pr0)	vr0 = sel vmsk1 vr12, vr0;"
        "}"
        "{"
        "V0@(pr0)	vr1 = sel vmsk0 vr1, vr11;"
        "V1@(pr0)	vr0 = sel vmsk0 vr0, vr10;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = eq.s32 vr1, vr11;"
        "V1@(pr0)	vmsk1 = eq.s32 vr0, vr10;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr7 = sel vmsk0 vr5, r48;"
        "V1@(pr0)	vr6 = sel vmsk1 vr5, r48;"
        "}"
        "{"
        "V0@(pr0)	vr7 = and.u32 vr6, vr7;"
        "V1@(pr0)	vmsk0 = eq.s32 vr7, r48;"
        "}"
        "{"
        "V0@(pr0)	vr15 = sel vmsk0 vr11, vr13;"
        "V1@(pr0)	vr14 = sel vmsk0 vr10, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr1;"
        "V1@(pr0)	vr16 = mov.u32 vr0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2047;"
        "pseudo@0	@pseudo imm_1 = 20;"
        "V0@(pr0)	vr7 = mov.u32 r32;"
        "V1@(pr0)	vr6 = mov.u32 r33;"
        "}"
        "{"
        "V1@(pr0)	vr0 = shr.u32 vr15, vr6;"
        "}"
        "{"
        "V0@(pr0)	vr0 = and.u32 vr0, vr7;"
        "}"
        "{"
        "V1@(pr0)	vr1 = shr.u32 vr17, vr6;"
        "}"
        "{"
        "V0@(pr0)	vr1 = and.u32 vr1, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 15;"
        "V0@(pr0)	vr2 = and.u32 vr14, r44;"
        "V1@(pr0)	vr3 = and.u32 vr15, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 15;"
        "V0@(pr0)	vr4 = and.u32 vr16, r44;"
        "V1@(pr0)	vr5 = and.u32 vr17, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32768;"
        "V0@(pr0)	vr6 = and.u32 vr15, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32768;"
        "V0@(pr0)	vr7 = xor.u32 vr15, vr17;"
        "V1@(pr0)	vr7 = and.u32 vr7, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "pseudo@0	@pseudo imm_2 = 3;"
        "V0@(pr0)	vr3 = or.u32 vr3, r44;"
        "V1@(pr0)	vr3 = shl.u32 vr3, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 57344;"
        "pseudo@0	@pseudo imm_2 = 29;"
        "V0@(pr0)	vr28 = and.u32 vr2, r44;"
        "V1@(pr0)	vr28 = shr.u32 vr28, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3;"
        "V0@(pr0)	vr3 = or.u32 vr3, vr28;"
        "V1@(pr0)	vr2 = shl.u32 vr2, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "pseudo@0	@pseudo imm_2 = 3;"
        "V0@(pr0)	vr5 = or.u32 vr5, r44;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 57344;"
        "pseudo@0	@pseudo imm_2 = 29;"
        "V0@(pr0)	vr28 = and.u32 vr4, r44;"
        "V1@(pr0)	vr28 = shr.u32 vr28, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3;"
        "V0@(pr0)	vr5 = or.u32 vr5, vr28;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 64;"
        "V0@(pr0)	vr28 = sub.s32 vr0, vr1;"
        "V1@(pr0)	vr29 = mov.u32 r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr29 = sub.s32 vr29, vr28;"
        "V1@(pr0)	vmsk0 = gt.s32 vr29, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr31 = mov.u32 r32;"
        "V1@(pr0)	vr31 = sub.s32 vr31, vr29;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr30 = mov.u32 r44;"
        "}"
        "{"
        "V1@(pr0)	vr30 = shl.u32 vr30, vr31;"
        "}"
        "{"
        "V0@(pr0)	vr30 = and.u32 vr4, vr30;"
        "V1@(pr0)	vr30 = shr.u32 vr30, vr31;"
        "}"
        "{"
        "V1@(pr0)	vr11 = shl.u32 vr5, vr29;"
        "}"
        "{"
        "V0@(pr0)	vr11 = or.u32 vr11, vr30;"
        "V1@(pr0)	vr10 = shl.u32 vr4, vr29;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V1@(pr0)	vr30 = sub.s32 vr29, r32;"
        "}"
        "{"
        "V1@(pr0)	vr12 = shl.u32 vr4, vr30;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk0 vr10, r46;"
        "V1@(pr0)	vr11 = sel vmsk0 vr11, vr12;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = eq.s32 vr10, r46;"
        "V1@(pr0)	vmsk1 = eq.s32 vr11, r46;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 r46;"
        "V1@(pr0)	vr17 = sel vmsk0 vr11, r48;"
        "}"
        "{"
        "V0@(pr0)	vr16 = sel vmsk1 vr11, r48;"
        "V1@(pr0)	vr17 = add.s32 vr16, vr17;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vmsk0 = eq.s32 vr17, r32;"
        "V1@(pr0)	vr17 = sel vmsk0 vr11, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V1@(pr0)	vmsk0 = gt.s32 vr28, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 65535;"
        "V0@(pr0)	vr31 = mov.u32 r45;"
        "V1@(pr0)	vr30 = mov.u32 r32;"
        "}"
        "{"
        "V0@(pr0)	vr30 = sub.s32 vr30, vr28;"
        "V1@(pr0)	vr31 = shr.u32 vr31, vr30;"
        "}"
        "{"
        "V0@(pr0)	vr31 = and.u32 vr31, vr5;"
        "V1@(pr0)	vr31 = shl.u32 vr31, vr30;"
        "}"
        "{"
        "V1@(pr0)	vr10 = shr.u32 vr4, vr28;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr10, vr31;"
        "V1@(pr0)	vr11 = shr.u32 vr5, vr28;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr31 = sub.s32 vr28, r32;"
        "V1@(pr0)	vr12 = shr.u32 vr5, vr31;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk0 vr10, vr12;"
        "V1@(pr0)	vr11 = sel vmsk0 vr11, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 64;"
        "V0@(pr0)	vr10 = or.u32 vr10, vr17;"
        "V1@(pr0)	vmsk0 = ls.s32 vr28, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 r46;"
        "V1@(pr0)	vr11 = sel vmsk0 vr17, vr11;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 r48;"
        "V1@(pr0)	vr10 = sel vmsk0 vr17, vr10;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = eq.s32 vr28, r36;"
        "V1@(pr0)	vr4 = sel vmsk0 vr10, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "V0@(pr0)	vr5 = sel vmsk0 vr11, vr5;"
        "V1@(pr0)	vr10 = and.u32 vr2, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr12 = and.u32 vr4, r33;"
        "V1@(pr0)	vr11 = shr.u32 vr2, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr13 = shr.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = gteq.s32 vr10, vr12;"
        "V1@(pr0)	vr28 = mov.u32 r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr17 = sel vmsk0 vr28, r46;"
        "V1@(pr0)	vr17 = shl.u32 vr17, r32;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr10, vr17;"
        "V1@(pr0)	vr14 = sub.s32 vr10, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr15 = add.s32 vr13, r48;"
        "V1@(pr0)	vr13 = sel vmsk0 vr15, vr13;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = gteq.s32 vr11, vr13;"
        "V1@(pr0)	vr17 = sel vmsk0 vr28, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr17 = shl.u32 vr17, r32;"
        "}"
        "{"
        "V0@(pr0)	vr11 = or.u32 vr11, vr17;"
        "V1@(pr0)	vr15 = sub.s32 vr11, vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr14, vr15;"
        "}"
        "{"
        "V0@(pr0)	vr17 = add.s32 vr5, r48;"
        "V1@(pr0)	vr17 = sel vmsk0 vr17, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr11 = sub.s32 vr3, vr17;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = eq.s32 vr11, r46;"
        "V1@(pr0)	vr31 = count.u32 vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr30 = count.u32 vr10;"
        "V1@(pr0)	vr30 = add.s32 vr30, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr31 = sel vmsk0 vr31, vr30;"
        "V1@(pr0)	vr31 = sub.s32 vr31, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vmsk6 = ls.s32 vr31, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr30 = mov.u32 r32;"
        "V1@(pr0)	vr30 = sub.s32 vr30, vr31;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr29 = mov.u32 r44;"
        "V1@(pr0)	vr29 = shl.u32 vr29, vr30;"
        "}"
        "{"
        "V0@(pr0)	vr29 = and.u32 vr29, vr10;"
        "V1@(pr0)	vr29 = shr.u32 vr29, vr30;"
        "}"
        "{"
        "V1@(pr0)	vr13 = shl.u32 vr11, vr31;"
        "}"
        "{"
        "V0@(pr0)	vr13 = or.u32 vr13, vr29;"
        "V1@(pr0)	vr12 = shl.u32 vr10, vr31;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr30 = mov.u32 r32;"
        "V1@(pr0)	vr30 = sub.s32 vr31, vr30;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr14 = shl.u32 vr10, vr30;"
        "}"
        "{"
        "V0@(pr0)	vr12 = sel vmsk6 vr15, vr12;"
        "V1@(pr0)	vr13 = sel vmsk6 vr14, vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 128;"
        "V0@(pr0)	vmsk0 = ls.s32 vr11, r36;"
        "V1@(pr0)	vr10 = sel vmsk0 vr10, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr15 = sub.s32 vr0, vr31;"
        "}"
        "{"
        "V0@(pr0)	vr11 = sel vmsk0 vr11, vr13;"
        "V1@(pr0)	vr15 = sel vmsk0 vr0, vr15;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr12 = and.u32 vr2, r33;"
        "V1@(pr0)	vr13 = shr.u32 vr2, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr16 = and.u32 vr4, r33;"
        "V1@(pr0)	vr17 = shr.u32 vr4, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr28 = add.s32 vr12, vr16;"
        "V1@(pr0)	vr14 = shr.u32 vr28, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "V0@(pr0)	vr28 = and.u32 vr28, r32;"
        "}"
        "{"
        "V0@(pr0)	vr29 = add.s32 vr13, vr17;"
        "V1@(pr0)	vr29 = add.s32 vr29, vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr29, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "V0@(pr0)	vr29 = and.u32 vr29, r32;"
        "V1@(pr0)	vr29 = shl.u32 vr29, r33;"
        "}"
        "{"
        "V0@(pr0)	vr12 = or.u32 vr29, vr28;"
        "V1@(pr0)	vr13 = add.s32 vr3, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr13 = add.s32 vr13, vr14;"
        "V1@(pr0)	vr28 = and.u32 vr12, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V0@(pr0)	vr14 = and.u32 vr13, r48;"
        "V1@(pr0)	vr14 = shl.u32 vr14, r32;"
        "}"
        "{"
        "V1@(pr0)	vr16 = shr.u32 vr12, r48;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr16, vr14;"
        "V1@(pr0)	vr17 = shr.u32 vr13, r48;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr16, vr28;"
        "V1@(pr0)	vr14 = add.s32 vr0, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 256;"
        "V0@(pr0)	vr28 = and.u32 vr13, r36;"
        "V1@(pr0)	vmsk0 = eq.s32 vr28, r46;"
        "}"
        "{"
        "V0@(pr0)	vr12 = sel vmsk0 vr16, vr12;"
        "V1@(pr0)	vr13 = sel vmsk0 vr17, vr13;"
        "}"
        "{"
        "V0@(pr0)	vr14 = sel vmsk0 vr14, vr0;"
        "V1@(pr0)	vmsk1 = eq.s32 vr7, r46;"
        "}"
        "{"
        "V0@(pr0)	vr2 = sel vmsk1 vr10, vr12;"
        "V1@(pr0)	vr3 = sel vmsk1 vr11, vr13;"
        "}"
        "{"
        "V0@(pr0)	vr0 = sel vmsk1 vr15, vr14;"
        "V1@(pr0)	vr31 = mov.u32 r48;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr2, r48;"
        "V1@(pr0)	vmsk0 = lseq.s32 vr0, r46;"
        "}"
        "{"
        "V0@(pr0)	vr2 = sel vmsk0 vr2, vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 7;"
        "pseudo@0	@pseudo imm_1 = 3;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 65535;"
        "V0@(pr0)	vr17 = and.u32 vr2, r32;"
        "V1@(pr0)	vr31 = mov.u32 r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 29;"
        "V1@(pr0)	vr31 = shr.u32 vr31, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 29;"
        "V0@(pr0)	vr31 = and.u32 vr3, vr31;"
        "V1@(pr0)	vr31 = shl.u32 vr31, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3;"
        "V1@(pr0)	vr10 = shr.u32 vr2, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3;"
        "V0@(pr0)	vr10 = or.u32 vr10, vr31;"
        "V1@(pr0)	vr11 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 15;"
        "V0@(pr0)	vr10 = and.u32 vr10, r44;"
        "V1@(pr0)	vr11 = and.u32 vr11, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr28 = shl.u32 vr0, r32;"
        "}"
        "{"
        "V0@(pr0)	vr11 = or.u32 vr11, vr28;"
        "V1@(pr0)	vr11 = or.u32 vr11, vr6;"
        "}"
        "{"
        "V0@(pr0)	vr12 = add.s32 vr10, r48;"
        "V1@(pr0)	vr13 = and.u32 vr10, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "V0@(pr0)	vmsk0 = gt.s32 vr17, r32;"
        "V1@(pr0)	vmsk1 = eq.s32 vr17, r32;"
        "}"
        "{"
        "V0@(pr0)	vr13 = add.s32 vr10, vr13;"
        "V1@(pr0)	vr13 = sel vmsk1 vr10, vr13;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk0 vr13, vr12;"
        "V1@(pr0)	vr0 = mov.u32 vr10;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mov.u32 vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr10 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr11 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr12 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr13 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr14 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 5120;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr15 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 6144;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr16 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 7168;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr17 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8192;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = eq.s32 vr17, r46;"
        "V1@(pr0)	vmsk1 = eq.s32 vr16, r46;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 r46;"
        "V1@(pr0)	vr7 = sel vmsk0 vr28, r48;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr28, r48;"
        "V1@(pr0)	vr7 = add.s32 vr6, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vmsk0 = eq.s32 vr7, r32;"
        "V1@(pr0)	vr30 = sel vmsk0 vr0, vr10;"
        "}"
        "{"
        "V0@(pr0)	vr31 = sel vmsk0 vr1, vr11;"
        "V1@(pr0)	vr6 = and.u32 vr10, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr7 = and.u32 vr11, vr13;"
        "V1@(pr0)	vr5 = sel vmsk0 vr13, vr7;"
        "}"
        "{"
        "V0@(pr0)	vr4 = sel vmsk0 vr12, vr6;"
        "V1@(pr0)	vmsk0 = eq.s32 vr14, r46;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = eq.s32 vr15, r46;"
        "V1@(pr0)	vr6 = sel vmsk0 vr28, r48;"
        "}"
        "{"
        "V0@(pr0)	vr7 = sel vmsk1 vr28, r48;"
        "V1@(pr0)	vr7 = add.s32 vr6, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vmsk0 = eq.s32 vr7, r32;"
        "V1@(pr0)	vr31 = sel vmsk0 vr31, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr30 = sel vmsk0 vr30, r34;"
        "V1@(pr0)	vr7 = mov.u32 r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 15;"
        "V0@(pr0)	vr6 = mov.u32 r44;"
        "V1@(pr0)	vr7 = xor.u32 vr7, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr6 = xor.u32 vr6, r44;"
        "V1@(pr0)	vmsk0 = eq.s32 vr16, vr6;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mov.u32 vr7;"
        "V1@(pr0)	vr4 = mov.u32 vr6;"
        "}"
        "{"
        "V0@(pr0)	vmsk6 = eq.s32 vr14, vr6;"
        "V1@(pr0)	vmsk7 = eq.s32 vr15, vr7;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = eq.s32 vr17, vr7;"
        "V1@(pr0)	vr7 = sel vmsk0 vr28, r48;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr28, r48;"
        "V1@(pr0)	vr7 = add.s32 vr6, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vmsk0 = eq.s32 vr7, r32;"
        "V1@(pr0)	vr31 = sel vmsk0 vr31, vr13;"
        "}"
        "{"
        "V0@(pr0)	vr30 = sel vmsk0 vr30, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr7 = sel vmsk7 vr28, r48;"
        "V1@(pr0)	vr6 = sel vmsk6 vr28, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vr7 = add.s32 vr6, vr7;"
        "V1@(pr0)	vmsk7 = eq.s32 vr7, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr3 = or.u32 vr5, r36;"
        "V1@(pr0)	vr6 = xor.u32 vr10, vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32768;"
        "V0@(pr0)	vr7 = xor.u32 vr11, vr13;"
        "V1@(pr0)	vmsk0 = eq.s32 vr7, r36;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = eq.s32 vr6, r46;"
        "V1@(pr0)	vr7 = sel vmsk0 vr28, r48;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr28, r48;"
        "V1@(pr0)	vr7 = add.s32 vr6, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vmsk0 = eq.s32 vr7, r32;"
        "V1@(pr0)	vr7 = sel vmsk0 vr11, vr3;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk0 vr10, vr4;"
        "V1@(pr0)	vr30 = sel vmsk7 vr30, vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr31 = sel vmsk7 vr31, vr7;"
        "V1@(pr0)	vr7 = or.u32 vr13, r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk0 = eq.s32 vr16, r46;"
        "V1@(pr0)	vr6 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "V0@(pr0)	vr28 = and.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32752;"
        "V0@(pr0)	vmsk1 = gt.s32 vr6, r32;"
        "V1@(pr0)	vmsk2 = eq.s32 vr28, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk0 vr28, r48;"
        "V1@(pr0)	vr4 = sel vmsk2 vr28, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vr5 = add.s32 vr4, vr5;"
        "V1@(pr0)	vmsk2 = eq.s32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr7, vr31;"
        "V1@(pr0)	vr4 = sel vmsk2 vr12, vr30;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32752;"
        "V0@(pr0)	vmsk2 = eq.s32 vr6, r32;"
        "V1@(pr0)	vr4 = sel vmsk2 vr30, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr31, vr5;"
        "V1@(pr0)	vr31 = sel vmsk1 vr5, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr30 = sel vmsk1 vr4, vr12;"
        "V1@(pr0)	vr7 = or.u32 vr11, r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk0 = eq.s32 vr14, r46;"
        "V1@(pr0)	vr6 = shr.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "V0@(pr0)	vr28 = and.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32752;"
        "V0@(pr0)	vmsk1 = gt.s32 vr6, r32;"
        "V1@(pr0)	vmsk2 = eq.s32 vr28, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk0 vr28, r48;"
        "V1@(pr0)	vr4 = sel vmsk2 vr28, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vr5 = add.s32 vr4, vr5;"
        "V1@(pr0)	vmsk2 = eq.s32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr7, vr31;"
        "V1@(pr0)	vr4 = sel vmsk2 vr10, vr30;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32752;"
        "V0@(pr0)	vmsk2 = eq.s32 vr6, r32;"
        "V1@(pr0)	vr4 = sel vmsk2 vr30, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr31, vr5;"
        "V1@(pr0)	vr11 = sel vmsk1 vr5, vr7;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr4, vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 49152;"
        "pseudo@0	@pseudo imm_2 = 3;"
        "V0@(pr0)	vr5 = and.u32 vr11, r44;"
        "V1@(pr0)	vr1 = shl.u32 vr11, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 16383;"
        "V0@(pr0)	vr1 = and.u32 vr1, r44;"
        "V1@(pr0)	vr5 = or.u32 vr1, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 29;"
        "V1@(pr0)	vr1 = shr.u32 vr10, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = or.u32 vr1, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr4 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr13 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr4;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr5;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "S0@(pr0)	r3 = add.s32 r9, r32;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "S0@(pr0)	r3 = add.s32 r3, r9;"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "VL@(pr0)	vr10 = ld [vmem:1+2,4,0];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3;"
        "V0@(pr0)	vr15 = and.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr0 = and.u32 vr11, r44;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr11, vr11;"
        "V1@(pr0)	vr7 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr4 = mul.f32 vr3, vr11;"
        "V1@(pr0)	vmsk1 = eq.s32 vr13, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 51667;"
        "pseudo@0	@pseudo imm_1 = 12078;"
        "pseudo@0	@pseudo imm_2 = 12084;"
        "pseudo@0	@pseudo imm_3 = 45783;"
        "V0@(pr0)	vr1 = mul.f32 vr3, r44;"
        "V1@(pr0)	vr1 = add.f32 vr1, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 61211;"
        "pseudo@0	@pseudo imm_1 = 13880;"
        "V0@(pr0)	vr1 = mul.f32 vr3, vr1;"
        "V1@(pr0)	vr1 = add.f32 vr1, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3329;"
        "pseudo@0	@pseudo imm_1 = 47440;"
        "V0@(pr0)	vr1 = mul.f32 vr3, vr1;"
        "V1@(pr0)	vr1 = add.f32 vr1, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 34953;"
        "pseudo@0	@pseudo imm_1 = 15368;"
        "V0@(pr0)	vr1 = mul.f32 vr3, vr1;"
        "V1@(pr0)	vr5 = add.f32 vr1, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 43691;"
        "pseudo@0	@pseudo imm_1 = 48682;"
        "V0@(pr0)	vr1 = mul.f32 vr3, vr5;"
        "V1@(pr0)	vr1 = add.f32 vr1, r44;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.f32 vr11, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr2 = mul.f32 vr12, r50;"
        "V1@(pr0)	vr6 = sel vmsk1 vr6, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr1 = sub.f32 vr2, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr3, vr1;"
        "V1@(pr0)	vr1 = sub.f32 vr1, vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 43691;"
        "pseudo@0	@pseudo imm_1 = 48682;"
        "V0@(pr0)	vr2 = mul.f32 vr4, r44;"
        "V1@(pr0)	vr1 = sub.f32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr2 = cvtftoint.s32 vr11, r44;"
        "V1@(pr0)	vr1 = sub.f32 vr11, vr1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 12800;"
        "V0@(pr0)	vr6 = sel vmsk1 vr1, vr6;"
        "V1@(pr0)	vmsk1 = ls.s32 vr0, r36;"
        "}"
        "{"
        "V0@(pr0)	vr1 = sel vmsk1 vr6, vr11;"
        "V1@(pr0)	vmsk1 = eq.s32 vr2, r46;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr6, vr1;"
        "V1@(pr0)	vr11 = mov.u32 vr6;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mul.f32 vr11, r57;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V1@(pr0)	vmsk1 = eq.s32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr14 = sel vmsk1 vr11, vr17;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr4 = mul.f32 vr11, vr11;"
        "V1@(pr0)	vr7 = and.u32 vr11, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 39322;"
        "pseudo@0	@pseudo imm_1 = 16025;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vmsk1 = ls.s32 vr7, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 55118;"
        "pseudo@0	@pseudo imm_1 = 44359;"
        "pseudo@0	@pseudo imm_2 = 29942;"
        "pseudo@0	@pseudo imm_3 = 12559;"
        "V0@(pr0)	vr5 = mul.f32 vr4, r44;"
        "V1@(pr0)	vr5 = add.f32 vr5, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 62076;"
        "pseudo@0	@pseudo imm_1 = 46227;"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr5 = add.f32 vr5, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3329;"
        "pseudo@0	@pseudo imm_1 = 14288;"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr5 = add.f32 vr5, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2913;"
        "pseudo@0	@pseudo imm_1 = 47798;"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr5 = add.f32 vr5, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 43691;"
        "pseudo@0	@pseudo imm_1 = 15658;"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr5 = add.f32 vr5, r44;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr11, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr2 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr1 = sub.f32 vr2, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr2 = mul.f32 vr4, r50;"
        "V1@(pr0)	vr1 = sub.f32 vr2, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr2 = mov.u32 r49;"
        "V1@(pr0)	vr1 = sub.f32 vr2, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr0, vr1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16200;"
        "pseudo@0	@pseudo imm_1 = 256;"
        "V0@(pr0)	vmsk2 = gt.s32 vr7, r36;"
        "V1@(pr0)	vr1 = sub.s32 vr7, r37;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16016;"
        "V0@(pr0)	vr2 = mul.f32 vr4, r50;"
        "V1@(pr0)	vr1 = sel vmsk2 vr1, r44;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mov.u32 r49;"
        "V1@(pr0)	vr2 = sub.f32 vr2, vr1;"
        "}"
        "{"
        "V1@(pr0)	vr1 = sub.f32 vr3, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr11, vr12;"
        "V1@(pr0)	vr3 = add.f32 vr3, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr2 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr3 = sub.f32 vr3, vr2;"
        "}"
        "{"
        "V1@(pr0)	vr1 = sub.f32 vr1, vr3;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 12800;"
        "V0@(pr0)	vr6 = sel vmsk1 vr1, vr6;"
        "V1@(pr0)	vmsk1 = ls.s32 vr7, r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr2 = cvtftoint.s32 vr11, r44;"
        "}"
        "{"
        "V0@(pr0)	vr1 = sel vmsk1 vr6, r49;"
        "V1@(pr0)	vmsk1 = eq.s32 vr2, r46;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr6, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 vr6;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mul.f32 vr11, r57;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "V1@(pr0)	vmsk1 = eq.s32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr14 = sel vmsk1 vr14, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V1@(pr0)	vmsk1 = eq.s32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr14 = sel vmsk1 vr14, vr17;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 vr10;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 r46;"
        "V1@(pr0)	vr13 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr4 = mul.f32 vr11, vr11;"
        "V1@(pr0)	vr7 = and.u32 vr11, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 39322;"
        "pseudo@0	@pseudo imm_1 = 16025;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vmsk1 = ls.s32 vr7, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 55118;"
        "pseudo@0	@pseudo imm_1 = 44359;"
        "pseudo@0	@pseudo imm_2 = 29942;"
        "pseudo@0	@pseudo imm_3 = 12559;"
        "V0@(pr0)	vr5 = mul.f32 vr4, r44;"
        "V1@(pr0)	vr5 = add.f32 vr5, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 62076;"
        "pseudo@0	@pseudo imm_1 = 46227;"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr5 = add.f32 vr5, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3329;"
        "pseudo@0	@pseudo imm_1 = 14288;"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr5 = add.f32 vr5, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2913;"
        "pseudo@0	@pseudo imm_1 = 47798;"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr5 = add.f32 vr5, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 43691;"
        "pseudo@0	@pseudo imm_1 = 15658;"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr5 = add.f32 vr5, r44;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr11, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr2 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr1 = sub.f32 vr2, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr2 = mul.f32 vr4, r50;"
        "V1@(pr0)	vr1 = sub.f32 vr2, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr2 = mov.u32 r49;"
        "V1@(pr0)	vr1 = sub.f32 vr2, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr0, vr1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16200;"
        "pseudo@0	@pseudo imm_1 = 256;"
        "V0@(pr0)	vmsk2 = gt.s32 vr7, r36;"
        "V1@(pr0)	vr1 = sub.s32 vr7, r37;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16016;"
        "V0@(pr0)	vr2 = mul.f32 vr4, r50;"
        "V1@(pr0)	vr1 = sel vmsk2 vr1, r44;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mov.u32 r49;"
        "V1@(pr0)	vr2 = sub.f32 vr2, vr1;"
        "}"
        "{"
        "V1@(pr0)	vr1 = sub.f32 vr3, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr11, vr12;"
        "V1@(pr0)	vr3 = add.f32 vr3, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr2 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr3 = sub.f32 vr3, vr2;"
        "}"
        "{"
        "V1@(pr0)	vr1 = sub.f32 vr1, vr3;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 12800;"
        "V0@(pr0)	vr6 = sel vmsk1 vr1, vr6;"
        "V1@(pr0)	vmsk1 = ls.s32 vr7, r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr2 = cvtftoint.s32 vr11, r44;"
        "}"
        "{"
        "V0@(pr0)	vr1 = sel vmsk1 vr6, r49;"
        "V1@(pr0)	vmsk1 = eq.s32 vr2, r46;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr6, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr1 = and.u32 vr10, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4056;"
        "pseudo@0	@pseudo imm_1 = 16201;"
        "V0@(pr0)	vmsk1 = ls.f32 vr1, r44;"
        "}"
        "{"
        "V1@(pr0)	vr14 = sel vmsk1 vr14, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr1 = and.u32 vr10, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32640;"
        "V0@(pr0)	vmsk1 = ls.f32 vr1, r44;"
        "V1@(pr0)	vr15 = sub.f32 vr10, vr10;"
        "}"
        "{"
        "V1@(pr0)	vr14 = sel vmsk1 vr15, vr14;"
        "}"
        "{"
        "V0@(pr0)	%[res0] = mov.u32 vr14;"
        "}"
        : [res0] "=x" (result0)
        : [input0] "x" (a)
        :"vr4", "vr30", "vr10", "vr6", "vr12", "vr7", "vr15", "vr31", "vr1", "vr5", "vr0", "vr28", "vr13", "vr29", "vr2", "vr17", "vr16", "vr11", "vr3", "vr14", "r1", "r0", "r3", "r9", "vmsk3", "vmsk0", "vmsk5", "vmsk4", "vmsk7", "vmsk1", "vmsk2", "vmsk6"
        );
    return result0;
}

#endif // _COSF_H_
