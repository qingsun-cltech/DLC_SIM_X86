#ifndef _LDEXPF_H_
#define _LDEXPF_H_

inline float8_128 __dlc_ldexpf(float8_128 a, int8_128 b)
{
    float8_128 result0;
    asm (
        "{V0@(pr0)  vr10 = mov.u32 %[input0];}"
        "{V0@(pr0)  vr11 = mov.u32 %[input1];}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 32767;"
        "V0@(pr0)	vr0 = and.u32 vr10, r45;"
        "V1@(pr0)	vr1 = exte.s32 vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 127;"
        "V0@(pr0)	vr1 = add.s32 vr1, r32;"
        "V1@(pr0)	vr15 = mov.u32 vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 128;"
        "V0@(pr0)	vmsk1 = ls.s32 vr0, r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr3 = count.u32 vr0;"
        "V1@(pr0)	vr3 = sub.s32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 25;"
        "V0@(pr0)	vr4 = mov.u32 r32;"
        "V1@(pr0)	vr2 = shl.u32 vr0, vr3;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32895;"
        "V0@(pr0)	vr4 = sub.s32 vr4, vr3;"
        "V1@(pr0)	vr2 = and.u32 vr2, r44;"
        "}"
        "{"
        "V0@(pr0)	vr14 = add.s32 vr4, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 23;"
        "V0@(pr0)	vr5 = and.u32 vr10, r47;"
        "V1@(pr0)	vr4 = shl.u32 vr14, r34;"
        "}"
        "{"
        "V0@(pr0)	vr2 = or.u32 vr4, vr2;"
        "V1@(pr0)	vr2 = or.u32 vr2, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 25;"
        "V0@(pr0)	vr4 = sub.s32 vr14, r32;"
        "V1@(pr0)	vr1 = sel vmsk1 vr1, vr4;"
        "}"
        "{"
        "V1@(pr0)	vr4 = sel vmsk1 vr10, vr2;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = infnan.f32 vr10;"
        "V1@(pr0)	vmsk1 = eq.s32 vr0, r46;"
        "}"
        "{"
        "V0@(pr0)	vr4 = sel vmsk0 vr4, vr10;"
        "V1@(pr0)	vr10 = sel vmsk1 vr4, vr10;"
        "}"
        "{"
        "V0@(pr0)	vr6 = add.s32 vr1, vr11;"
        "V1@(pr0)	vr7 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 127;"
        "pseudo@0	@pseudo imm_2 = 128;"
        "V0@(pr0)	vr4 = and.u32 vr10, r44;"
        "V1@(pr0)	vr4 = or.u32 vr4, r38;"
        "}"
        "{"
        "V0@(pr0)	vr7 = sub.s32 vr7, vr6;"
        "V1@(pr0)	vr7 = add.s32 vr7, r48;"
        "}"
        "{"
        "V1@(pr0)	vr4 = shr.u32 vr4, vr7;"
        "}"
        "{"
        "V0@(pr0)	vr5 = and.u32 vr10, r47;"
        "V1@(pr0)	vr13 = or.u32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65514;"
        "V0@(pr0)	vmsk0 = ls.s32 vr6, r40;"
        "V1@(pr0)	vr5 = and.u32 vr10, r47;"
        "}"
        "{"
        "V1@(pr0)	vr13 = sel vmsk0 vr13, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32895;"
        "V0@(pr0)	vmsk0 = gt.s32 vr6, r46;"
        "V1@(pr0)	vr4 = and.u32 vr10, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 23;"
        "V1@(pr0)	vr7 = shl.u32 vr6, r34;"
        "}"
        "{"
        "V0@(pr0)	vr4 = or.u32 vr4, vr7;"
        "V1@(pr0)	vr13 = sel vmsk0 vr13, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 254;"
        "V0@(pr0)	vmsk0 = gt.s32 vr6, r32;"
        "V1@(pr0)	vr5 = and.u32 vr10, r47;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32640;"
        "V0@(pr0)	vr4 = or.u32 vr5, r36;"
        "V1@(pr0)	vr13 = sel vmsk0 vr13, vr4;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = infnan.f32 vr10;"
        "V1@(pr0)	vmsk1 = eq.s32 vr10, r46;"
        "}"
        "{"
        "V0@(pr0)	vr13 = sel vmsk0 vr13, vr10;"
        "V1@(pr0)	vr13 = sel vmsk1 vr13, vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32768;"
        "V0@(pr0)	vmsk1 = eq.s32 vr10, r36;"
        "V1@(pr0)	vr13 = sel vmsk1 vr13, r36;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = eq.s32 vr11, r46;"
        "V1@(pr0)	vr10 = sel vmsk0 vr13, vr15;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = infnan.f32 vr28;"
        "V1@(pr0)	vmsk1 = eq.s32 vr28, r46;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk0 vr10, vr28;"
        "V1@(pr0)	vr10 = sel vmsk1 vr10, vr28;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32768;"
        "V0@(pr0)	vmsk1 = eq.s32 vr28, r36;"
        "V1@(pr0)	vr10 = sel vmsk1 vr10, r36;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = eq.s32 vr11, r46;"
        "V1@(pr0)	%[res0] = sel vmsk0 vr10, vr28;"
        "}"
        : [res0] "=x" (result0)
        : [input0] "x" (a),  [input1] "x" (b)
        :"vr2", "vr1", "vr4", "vr5", "vr0", "vr11", "vr3", "vr15", "vr14", "vr7", "vr28", "vr13", "vr10", "vr6", "vmsk1", "vmsk0"
        );
    return result0;
}

#endif // _LDEXPF_H_
