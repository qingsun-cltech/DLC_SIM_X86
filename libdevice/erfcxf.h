#ifndef _ERFCXF_H_
#define _ERFCXF_H_

inline float8_128 __dlc_erfcxf(float8_128 a)
{
    float8_128 result0;
    asm (
        "{V0@(pr0)  vr10 = mov.u32 %[input0];}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr3 = and.u32 vr10, r44;"
        "V1@(pr0)	vr6 = sub.f32 vr3, r51;"
        "}"
        "{"
        "V1@(pr0)	vr7 = add.f32 vr3, r51;"
        "}"
        "{"
        "V0@(pr0)	(urf) = rcp.f32 vr7;"
        "MTR@(pr0)	vr13 = pop urf;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mul.f32 vr6, vr13;"
        "V1@(pr0)	vr1 = add.f32 vr12, r49;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mul.f32 vr1, r59;"
        "V1@(pr0)	vr15 = add.f32 vr15, vr3;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr3, r57;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr12, vr1;"
        "V1@(pr0)	vr5 = add.f32 vr5, vr15;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr13, vr5;"
        "V1@(pr0)	vr12 = add.f32 vr1, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr13, vr5;"
        "V1@(pr0)	vr12 = add.f32 vr1, vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32768;"
        "pseudo@0	@pseudo imm_1 = 14456;"
        "V0@(pr0)	vr7 = mov.u32 r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3683;"
        "pseudo@0	@pseudo imm_1 = 14633;"
        "V0@(pr0)	vr1 = mul.f32 vr7, vr12;"
        "V1@(pr0)	vr7 = add.f32 vr1, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 43007;"
        "pseudo@0	@pseudo imm_1 = 47541;"
        "V0@(pr0)	vr1 = mul.f32 vr7, vr12;"
        "V1@(pr0)	vr7 = add.f32 vr1, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 5438;"
        "pseudo@0	@pseudo imm_1 = 47799;"
        "V0@(pr0)	vr1 = mul.f32 vr7, vr12;"
        "V1@(pr0)	vr7 = add.f32 vr1, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3775;"
        "pseudo@0	@pseudo imm_1 = 15006;"
        "V0@(pr0)	vr1 = mul.f32 vr7, vr12;"
        "V1@(pr0)	vr7 = add.f32 vr1, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 24859;"
        "pseudo@0	@pseudo imm_1 = 15374;"
        "V0@(pr0)	vr1 = mul.f32 vr7, vr12;"
        "V1@(pr0)	vr7 = add.f32 vr1, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 19616;"
        "pseudo@0	@pseudo imm_1 = 48131;"
        "V0@(pr0)	vr1 = mul.f32 vr7, vr12;"
        "V1@(pr0)	vr7 = add.f32 vr1, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3510;"
        "pseudo@0	@pseudo imm_1 = 48478;"
        "V0@(pr0)	vr1 = mul.f32 vr7, vr12;"
        "V1@(pr0)	vr7 = add.f32 vr1, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 64598;"
        "pseudo@0	@pseudo imm_1 = 15911;"
        "V0@(pr0)	vr1 = mul.f32 vr7, vr12;"
        "V1@(pr0)	vr7 = add.f32 vr1, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1037;"
        "pseudo@0	@pseudo imm_1 = 48682;"
        "V0@(pr0)	vr1 = mul.f32 vr7, vr12;"
        "V1@(pr0)	vr7 = add.f32 vr1, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 64231;"
        "pseudo@0	@pseudo imm_1 = 48573;"
        "V0@(pr0)	vr1 = mul.f32 vr7, vr12;"
        "V1@(pr0)	vr7 = add.f32 vr1, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 53277;"
        "pseudo@0	@pseudo imm_1 = 16013;"
        "V0@(pr0)	vr1 = mul.f32 vr7, vr12;"
        "V1@(pr0)	vr7 = add.f32 vr1, r44;"
        "}"
        "{"
        "V1@(pr0)	vr4 = add.f32 vr3, r50;"
        "}"
        "{"
        "V0@(pr0)	(urf) = rcp.f32 vr4;"
        "MTR@(pr0)	vr13 = pop urf;"
        "}"
        "{"
        "V0@(pr0)	vr13 = mul.f32 vr13, r50;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr7, vr13;"
        "V1@(pr0)	vr12 = add.f32 vr1, vr13;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr3, r57;"
        "V1@(pr0)	vr15 = add.f32 vr12, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr15, vr1;"
        "V1@(pr0)	vr1 = add.f32 vr1, r49;"
        "}"
        "{"
        "V1@(pr0)	vr2 = sub.f32 vr7, vr12;"
        "}"
        "{"
        "V1@(pr0)	vr5 = add.f32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr5, vr13;"
        "V1@(pr0)	vr13 = add.f32 vr1, vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32640;"
        "pseudo@0	@pseudo imm_1 = 32704;"
        "V0@(pr0)	vmsk1 = gt.s32 vr3, r36;"
        "V1@(pr0)	vr13 = sel vmsk1 vr13, r37;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32640;"
        "V0@(pr0)	vmsk1 = eq.s32 vr3, r36;"
        "V1@(pr0)	vr13 = sel vmsk1 vr13, r46;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mul.f32 vr10, vr10;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr14, r57;"
        "}"
        "{"
        "V0@(pr0)	vr4 = mul.f32 vr10, vr10;"
        "V1@(pr0)	vr4 = add.f32 vr4, vr1;"
        "}"
        "{"
        "V0@(pr0)	(urf) = exp.f32 vr14;"
        "V1@(pr0)	vmsk0 = ls.f32 vr14, r46;"
        "MTR@(pr0)	vr5 = pop urf;"
        "}"
        "{"
        "V0@(pr0)	(urf) = rcp.f32 vr5;"
        "MTR@(pr0)	vr1 = pop urf;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk0 vr5, vr1;"
        "V1@(pr0)	vr1 = sub.f32 vr5, vr13;"
        "}"
        "{"
        "V1@(pr0)	vr2 = add.f32 vr4, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr2 = mul.f32 vr5, vr2;"
        "V1@(pr0)	vr1 = add.f32 vr2, vr1;"
        "}"
        "{"
        "V1@(pr0)	vr1 = add.f32 vr1, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32639;"
        "V0@(pr0)	vmsk1 = gt.s32 vr5, r44;"
        "V1@(pr0)	vr1 = sel vmsk1 vr1, vr5;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = ls.s32 vr10, r46;"
        "V1@(pr0)	%[res0] = sel vmsk1 vr13, vr1;"
        "}"
        : [res0] "=x" (result0)
        : [input0] "x" (a)
        :"vr2", "vr1", "vr5", "vr4", "vr3", "vr7", "vr15", "vr14", "vr13", "vr10", "vr6", "vr12", "vmsk1", "vmsk0"
        );
    return result0;
}

#endif // _ERFCXF_H_
