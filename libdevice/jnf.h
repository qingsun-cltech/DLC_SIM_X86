#ifndef _JNF_H_
#define _JNF_H_

inline float8_128 __dlc_jnf(int8_128 a, float8_128 b)
{
    float8_128 result0;
    asm (
        "{V0@(pr0)  vr10 = mov.u32 %[input0];}"
        "{V0@(pr0)  vr11 = mov.u32 %[input1];}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r1 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r1 = sub.s32 r1, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VS@(pr0)	 [vmem:r0,$1,$255] = st vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VS@(pr0)	 [vmem:r0,$1,$255] = st vr10;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r1;"
        "V0@(pr0)	vr10 = mov.u32 vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr16 = and.u32 vr10, r44;"
        "V1@(pr0)	vr11 = mov.u32 r49;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mul.f32 vr10, vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16000;"
        "V0@(pr0)	vr14 = mul.f32 vr28, r36;"
        "V1@(pr0)	vr11 = sub.f32 vr11, vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 12800;"
        "V0@(pr0)	vmsk0 = ls.f32 vr16, r36;"
        "V1@(pr0)	vr11 = sel vmsk0 vr11, r49;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 44860;"
        "pseudo@0	@pseudo imm_1 = 45470;"
        "pseudo@0	@pseudo imm_2 = 36488;"
        "pseudo@0	@pseudo imm_3 = 13813;"
        "V0@(pr0)	vr29 = mul.f32 vr28, r44;"
        "V1@(pr0)	vr29 = add.f32 vr29, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 13614;"
        "pseudo@0	@pseudo imm_1 = 47431;"
        "V0@(pr0)	vr29 = mul.f32 vr29, vr28;"
        "V1@(pr0)	vr29 = add.f32 vr29, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 15488;"
        "V0@(pr0)	vr29 = mul.f32 vr29, vr28;"
        "V1@(pr0)	vr29 = add.f32 vr29, r44;"
        "}"
        "{"
        "V0@(pr0)	vr29 = mul.f32 vr29, vr28;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 17896;"
        "pseudo@0	@pseudo imm_1 = 12448;"
        "pseudo@0	@pseudo imm_2 = 55974;"
        "pseudo@0	@pseudo imm_3 = 13577;"
        "V0@(pr0)	vr30 = mul.f32 vr28, r44;"
        "V1@(pr0)	vr30 = add.f32 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 13975;"
        "pseudo@0	@pseudo imm_1 = 14581;"
        "V0@(pr0)	vr30 = mul.f32 vr28, vr30;"
        "V1@(pr0)	vr30 = add.f32 vr30, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 59204;"
        "pseudo@0	@pseudo imm_1 = 15487;"
        "V0@(pr0)	vr30 = mul.f32 vr28, vr30;"
        "V1@(pr0)	vr30 = add.f32 vr30, r44;"
        "}"
        "{"
        "V0@(pr0)	vr30 = mul.f32 vr28, vr30;"
        "V1@(pr0)	vr30 = add.f32 vr30, r49;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32640;"
        "V0@(pr0)	vr0 = and.u32 vr30, r44;"
        "V1@(pr0)	vmsk1 = eq.s32 vr0, r45;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr0, r50;"
        "V1@(pr0)	vr2 = mov.u32 vr0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 23007;"
        "pseudo@0	@pseudo imm_1 = 24375;"
        "V0@(pr0)	vr3 = mov.u32 r44;"
        "V1@(pr0)	vr2 = shr.u32 vr2, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16320;"
        "V0@(pr0)	vr2 = sub.s32 vr3, vr2;"
        "V1@(pr0)	vr4 = mov.u32 r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32768;"
        "V0@(pr0)	vr3 = mul.f32 vr2, vr2;"
        "V1@(pr0)	vr6 = and.u32 vr30, r36;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr1, vr3;"
        "V1@(pr0)	vr5 = sub.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr5, vr2;"
        "V1@(pr0)	vr2 = mov.u32 vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr2, vr2;"
        "V1@(pr0)	vmsk0 = eq.f32 vr30, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr1, vr3;"
        "V1@(pr0)	vr5 = sub.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr5, vr2;"
        "V1@(pr0)	vr2 = mov.u32 vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32640;"
        "V0@(pr0)	vr3 = mul.f32 vr2, vr2;"
        "V1@(pr0)	vr7 = or.u32 vr6, r44;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr1, vr3;"
        "V1@(pr0)	vr5 = sub.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr5, vr2;"
        "V1@(pr0)	vr2 = or.u32 vr6, r46;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr5, vr5;"
        "V1@(pr0)	vr1 = or.u32 vr6, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr30 = sel vmsk0 vr1, vr7;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16000;"
        "V0@(pr0)	vr30 = mul.f32 vr30, vr29;"
        "V1@(pr0)	vr29 = sub.f32 vr30, r36;"
        "}"
        "{"
        "V0@(pr0)	vr29 = mul.f32 vr29, vr28;"
        "V1@(pr0)	vr12 = add.f32 vr29, r49;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16000;"
        "V0@(pr0)	vr13 = mul.f32 vr28, r36;"
        "V1@(pr0)	vr14 = mov.u32 r49;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mul.f32 vr28, vr30;"
        "V1@(pr0)	vr13 = sub.f32 vr14, vr13;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = ls.f32 vr16, r49;"
        "V1@(pr0)	vr13 = add.f32 vr13, vr15;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 14592;"
        "V0@(pr0)	vr12 = sel vmsk0 vr13, vr12;"
        "V1@(pr0)	vmsk0 = ls.f32 vr16, r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr11 = sel vmsk0 vr12, vr11;"
        "V1@(pr0)	vr10 = and.u32 vr10, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r1 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r1 = sub.s32 r1, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VS@(pr0)	 [vmem:r0,$1,$255] = st vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VS@(pr0)	 [vmem:r0,$1,$255] = st vr10;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr10;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr1 = and.u32 vr10, r44;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 17136;"
        "V1@(pr0)	vmsk4 = ls.f32 vr2, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr10;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 29;"
        "V1@(pr0)	vr11 = shl.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 49152;"
        "pseudo@0	@pseudo imm_2 = 3;"
        "V0@(pr0)	vr1 = and.u32 vr10, r44;"
        "V1@(pr0)	vr2 = shr.u32 vr10, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 2047;"
        "V0@(pr0)	vr2 = and.u32 vr2, r44;"
        "V1@(pr0)	vr1 = or.u32 vr2, vr1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 2048;"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 14336;"
        "V0@(pr0)	vr2 = and.u32 vr10, r44;"
        "V1@(pr0)	vr3 = or.u32 vr1, r45;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = eq.s32 vr2, r46;"
        "V1@(pr0)	vr1 = sel vmsk1 vr3, vr1;"
        "}"
        "{"
        "V1@(pr0)	vr10 = mov.u32 vr1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr11;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 24368;"
        "pseudo@0	@pseudo imm_1 = 16740;"
        "pseudo@0	@pseudo imm_2 = 51331;"
        "pseudo@0	@pseudo imm_3 = 28105;"
        "V0@(pr0)	vr12 = mov.u32 r44;"
        "V1@(pr0)	vr13 = mov.u32 r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr13;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr1 = and.u32 vr10, r44;"
        "V1@(pr0)	vr2 = and.u32 vr12, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "V0@(pr0)	vmsk1 = ls.f32 vr1, r44;"
        "V1@(pr0)	vmsk2 = ls.f32 vr2, r44;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, r46;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, r46;"
        "}"
        "{"
        "V0@(pr0)	vr12 = sel vmsk2 vr12, r46;"
        "V1@(pr0)	vr13 = sel vmsk2 vr13, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr6 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr7 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 15;"
        "V0@(pr0)	vr10 = and.u32 vr10, r44;"
        "V1@(pr0)	vr12 = and.u32 vr12, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "V0@(pr0)	vr10 = or.u32 vr10, r44;"
        "V1@(pr0)	vr12 = or.u32 vr12, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 11;"
        "V1@(pr0)	vr12 = shl.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 21;"
        "V1@(pr0)	vr1 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 11;"
        "V0@(pr0)	vr12 = or.u32 vr12, vr1;"
        "V1@(pr0)	vr13 = shl.u32 vr13, r32;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr13, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = and.u32 vr12, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr13, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr12, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = and.u32 vr12, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr12, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 r46;"
        "V1@(pr0)	vr10 = mov.u32 vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr10;"
        "V1@(pr0)	vr17 = mov.u32 vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = or.u32 vr1, vr2;"
        "V1@(pr0)	vr14 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr12 = mov.u32 r46;"
        "V1@(pr0)	vr13 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "V1@(pr0)	vr15 = shr.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 vr15;"
        "V1@(pr0)	vr29 = mov.u32 vr14;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr15;"
        "V1@(pr0)	vr15 = mov.u32 vr0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr16;"
        "V1@(pr0)	vr11 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr12;"
        "V1@(pr0)	vr17 = mov.u32 vr13;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr29;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 vr15;"
        "V1@(pr0)	vr29 = mov.u32 vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr16;"
        "V1@(pr0)	vr13 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr10;"
        "V1@(pr0)	vr17 = mov.u32 vr11;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr28;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr16;"
        "V1@(pr0)	vr11 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr29 = shl.u32 vr29, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr12;"
        "V1@(pr0)	vr17 = mov.u32 vr13;"
        "}"
        "{"
        "V0@(pr0)	vr30 = mov.u32 vr14;"
        "V1@(pr0)	vr14 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr31 = mov.u32 vr17;"
        "V1@(pr0)	vr17 = mov.u32 vr16;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr11;"
        "V1@(pr0)	vr15 = mov.u32 vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr16;"
        "V1@(pr0)	vr11 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr13;"
        "V1@(pr0)	vr13 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr28;"
        "V1@(pr0)	vr17 = mov.u32 vr29;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr30;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr13;"
        "V1@(pr0)	vr15 = mov.u32 vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr13 = mov.u32 vr17;"
        "V1@(pr0)	vr12 = mov.u32 vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "V0@(pr0)	vr3 = xor.u32 vr6, vr7;"
        "V1@(pr0)	vr3 = and.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr3 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 2047;"
        "pseudo@0	@pseudo imm_3 = 0;"
        "V0@(pr0)	vr7 = and.u32 vr7, r45;"
        "V1@(pr0)	vr6 = and.u32 vr6, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 1023;"
        "pseudo@0	@pseudo imm_3 = 0;"
        "V0@(pr0)	vr6 = add.s32 vr6, vr7;"
        "V1@(pr0)	vr6 = sub.s32 vr6, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 16;"
        "V0@(pr0)	vr1 = and.u32 vr10, r45;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr6, r48;"
        "V1@(pr0)	vmsk1 = eq.s32 vr1, r46;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr2, vr6;"
        "}"
        "{"
        "V1@(pr0)	vr2 = shl.u32 vr10, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V1@(pr0)	vr5 = shr.u32 vr11, r32;"
        "}"
        "{"
        "V0@(pr0)	vr2 = or.u32 vr2, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V1@(pr0)	vr5 = shl.u32 vr13, r32;"
        "}"
        "{"
        "V1@(pr0)	vr4 = shl.u32 vr11, r48;"
        "}"
        "{"
        "V0@(pr0)	vr5 = or.u32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr2;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr5;"
        "}"
        "{"
        "V1@(pr0)	vr2 = shl.u32 vr12, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V1@(pr0)	vr5 = shr.u32 vr13, r32;"
        "}"
        "{"
        "V0@(pr0)	vr2 = or.u32 vr2, vr5;"
        "}"
        "{"
        "V1@(pr0)	vr5 = shl.u32 vr13, r48;"
        "}"
        "{"
        "V0@(pr0)	vr12 = sel vmsk1 vr12, vr2;"
        "V1@(pr0)	vr13 = sel vmsk1 vr13, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 15;"
        "V0@(pr0)	vr1 = and.u32 vr10, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr2 = shl.u32 vr6, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = gt.s32 vr6, r46;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr10, vr3;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32768;"
        "V0@(pr0)	vr1 = add.s32 vr11, r48;"
        "V1@(pr0)	vmsk1 = gt.s32 vr12, r44;"
        "}"
        "{"
        "V0@(pr0)	vr11 = sel vmsk1 vr11, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr1 = and.u32 vr11, r48;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr11;"
        "V1@(pr0)	vr15 = mov.u32 vr10;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr1;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32768;"
        "V1@(pr0)	vmsk1 = eq.s32 vr12, r45;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr16;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr17;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr28 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr29 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr30 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr31 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr1 = and.u32 vr28, r44;"
        "V1@(pr0)	vr2 = and.u32 vr30, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = lseq.s32 vr1, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 8;"
        "V1@(pr0)	vr1 = or.u32 vr28, r45;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr1, vr10;"
        "V1@(pr0)	vr11 = sel vmsk1 vr29, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = lseq.s32 vr2, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 8;"
        "V1@(pr0)	vr1 = or.u32 vr30, r45;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr1, vr10;"
        "V1@(pr0)	vr11 = sel vmsk1 vr31, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32768;"
        "V0@(pr0)	vr3 = xor.u32 vr28, vr30;"
        "V1@(pr0)	vr3 = and.u32 vr3, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = eq.f32 vr1, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32760;"
        "V0@(pr0)	vr5 = mov.u32 r44;"
        "}"
        "{"
        "V0@(pr0)	vr4 = xor.u32 vr1, vr3;"
        "}"
        "{"
        "V1@(pr0)	vmsk2 = eq.s32 vr2, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr5;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = eq.f32 vr2, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32760;"
        "V0@(pr0)	vr5 = mov.u32 r44;"
        "}"
        "{"
        "V0@(pr0)	vr4 = xor.u32 vr2, vr3;"
        "}"
        "{"
        "V1@(pr0)	vmsk2 = eq.s32 vr1, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr5;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr11;"
        "}"
        "{"
        "V1@(pr0)	vmsk1 = eq.f32 vr1, r46;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr3;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, r46;"
        "}"
        "{"
        "V1@(pr0)	vmsk1 = eq.f32 vr2, r46;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr3;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 49152;"
        "pseudo@0	@pseudo imm_2 = 3;"
        "V0@(pr0)	vr13 = and.u32 vr10, r44;"
        "V1@(pr0)	vr1 = shl.u32 vr10, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 16383;"
        "V0@(pr0)	vr1 = and.u32 vr1, r44;"
        "V1@(pr0)	vr13 = or.u32 vr1, vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 29;"
        "V1@(pr0)	vr1 = shr.u32 vr11, r32;"
        "}"
        "{"
        "V0@(pr0)	vr13 = or.u32 vr1, vr13;"
        "}"
        "{"
        "V0@(pr0)	vr13 = cvtftoint.s32 vr13, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 128;"
        "V0@(pr0)	vr13 = add.s32 vr13, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 24;"
        "V1@(pr0)	vr13 = shra.s32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr13;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr1 = cvtinttof.f32 vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 29;"
        "V1@(pr0)	vr11 = shl.u32 vr1, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 49152;"
        "pseudo@0	@pseudo imm_2 = 3;"
        "V0@(pr0)	vr10 = and.u32 vr1, r44;"
        "V1@(pr0)	vr2 = shr.u32 vr1, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 2047;"
        "V0@(pr0)	vr2 = and.u32 vr2, r44;"
        "V1@(pr0)	vr10 = or.u32 vr2, vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 2048;"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 14336;"
        "V0@(pr0)	vr2 = and.u32 vr1, r44;"
        "V1@(pr0)	vr3 = or.u32 vr10, r45;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = eq.s32 vr2, r46;"
        "V1@(pr0)	vr10 = sel vmsk1 vr3, vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8699;"
        "pseudo@0	@pseudo imm_1 = 16377;"
        "pseudo@0	@pseudo imm_2 = 11544;"
        "pseudo@0	@pseudo imm_3 = 21572;"
        "V0@(pr0)	vr12 = mov.u32 r44;"
        "V1@(pr0)	vr13 = mov.u32 r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr13;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr1 = and.u32 vr10, r44;"
        "V1@(pr0)	vr2 = and.u32 vr12, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "V0@(pr0)	vmsk1 = ls.f32 vr1, r44;"
        "V1@(pr0)	vmsk2 = ls.f32 vr2, r44;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, r46;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, r46;"
        "}"
        "{"
        "V0@(pr0)	vr12 = sel vmsk2 vr12, r46;"
        "V1@(pr0)	vr13 = sel vmsk2 vr13, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr6 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr7 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 15;"
        "V0@(pr0)	vr10 = and.u32 vr10, r44;"
        "V1@(pr0)	vr12 = and.u32 vr12, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "V0@(pr0)	vr10 = or.u32 vr10, r44;"
        "V1@(pr0)	vr12 = or.u32 vr12, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 11;"
        "V1@(pr0)	vr12 = shl.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 21;"
        "V1@(pr0)	vr1 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 11;"
        "V0@(pr0)	vr12 = or.u32 vr12, vr1;"
        "V1@(pr0)	vr13 = shl.u32 vr13, r32;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr13, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = and.u32 vr12, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr13, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr12, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = and.u32 vr12, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr12, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 r46;"
        "V1@(pr0)	vr10 = mov.u32 vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr10;"
        "V1@(pr0)	vr17 = mov.u32 vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = or.u32 vr1, vr2;"
        "V1@(pr0)	vr14 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr12 = mov.u32 r46;"
        "V1@(pr0)	vr13 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "V1@(pr0)	vr15 = shr.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 vr15;"
        "V1@(pr0)	vr29 = mov.u32 vr14;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr15;"
        "V1@(pr0)	vr15 = mov.u32 vr0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr16;"
        "V1@(pr0)	vr11 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr12;"
        "V1@(pr0)	vr17 = mov.u32 vr13;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr29;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 vr15;"
        "V1@(pr0)	vr29 = mov.u32 vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr16;"
        "V1@(pr0)	vr13 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr10;"
        "V1@(pr0)	vr17 = mov.u32 vr11;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr28;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr16;"
        "V1@(pr0)	vr11 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr29 = shl.u32 vr29, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr12;"
        "V1@(pr0)	vr17 = mov.u32 vr13;"
        "}"
        "{"
        "V0@(pr0)	vr30 = mov.u32 vr14;"
        "V1@(pr0)	vr14 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr31 = mov.u32 vr17;"
        "V1@(pr0)	vr17 = mov.u32 vr16;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr11;"
        "V1@(pr0)	vr15 = mov.u32 vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr16;"
        "V1@(pr0)	vr11 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr13;"
        "V1@(pr0)	vr13 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr28;"
        "V1@(pr0)	vr17 = mov.u32 vr29;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr30;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr13;"
        "V1@(pr0)	vr15 = mov.u32 vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr13 = mov.u32 vr17;"
        "V1@(pr0)	vr12 = mov.u32 vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "V0@(pr0)	vr3 = xor.u32 vr6, vr7;"
        "V1@(pr0)	vr3 = and.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr3 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 2047;"
        "pseudo@0	@pseudo imm_3 = 0;"
        "V0@(pr0)	vr7 = and.u32 vr7, r45;"
        "V1@(pr0)	vr6 = and.u32 vr6, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 1023;"
        "pseudo@0	@pseudo imm_3 = 0;"
        "V0@(pr0)	vr6 = add.s32 vr6, vr7;"
        "V1@(pr0)	vr6 = sub.s32 vr6, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 16;"
        "V0@(pr0)	vr1 = and.u32 vr10, r45;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr6, r48;"
        "V1@(pr0)	vmsk1 = eq.s32 vr1, r46;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr2, vr6;"
        "}"
        "{"
        "V1@(pr0)	vr2 = shl.u32 vr10, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V1@(pr0)	vr5 = shr.u32 vr11, r32;"
        "}"
        "{"
        "V0@(pr0)	vr2 = or.u32 vr2, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V1@(pr0)	vr5 = shl.u32 vr13, r32;"
        "}"
        "{"
        "V1@(pr0)	vr4 = shl.u32 vr11, r48;"
        "}"
        "{"
        "V0@(pr0)	vr5 = or.u32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr2;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr5;"
        "}"
        "{"
        "V1@(pr0)	vr2 = shl.u32 vr12, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V1@(pr0)	vr5 = shr.u32 vr13, r32;"
        "}"
        "{"
        "V0@(pr0)	vr2 = or.u32 vr2, vr5;"
        "}"
        "{"
        "V1@(pr0)	vr5 = shl.u32 vr13, r48;"
        "}"
        "{"
        "V0@(pr0)	vr12 = sel vmsk1 vr12, vr2;"
        "V1@(pr0)	vr13 = sel vmsk1 vr13, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 15;"
        "V0@(pr0)	vr1 = and.u32 vr10, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr2 = shl.u32 vr6, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = gt.s32 vr6, r46;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr10, vr3;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32768;"
        "V0@(pr0)	vr1 = add.s32 vr11, r48;"
        "V1@(pr0)	vmsk1 = gt.s32 vr12, r44;"
        "}"
        "{"
        "V0@(pr0)	vr11 = sel vmsk1 vr11, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr1 = and.u32 vr11, r48;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr11;"
        "V1@(pr0)	vr15 = mov.u32 vr10;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr1;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32768;"
        "V1@(pr0)	vmsk1 = eq.s32 vr12, r45;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr16;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr17;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr28 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr29 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr30 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr31 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr1 = and.u32 vr28, r44;"
        "V1@(pr0)	vr2 = and.u32 vr30, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = lseq.s32 vr1, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 8;"
        "V1@(pr0)	vr1 = or.u32 vr28, r45;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr1, vr10;"
        "V1@(pr0)	vr11 = sel vmsk1 vr29, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = lseq.s32 vr2, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 8;"
        "V1@(pr0)	vr1 = or.u32 vr30, r45;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr1, vr10;"
        "V1@(pr0)	vr11 = sel vmsk1 vr31, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32768;"
        "V0@(pr0)	vr3 = xor.u32 vr28, vr30;"
        "V1@(pr0)	vr3 = and.u32 vr3, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = eq.f32 vr1, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32760;"
        "V0@(pr0)	vr5 = mov.u32 r44;"
        "}"
        "{"
        "V0@(pr0)	vr4 = xor.u32 vr1, vr3;"
        "}"
        "{"
        "V1@(pr0)	vmsk2 = eq.s32 vr2, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr5;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = eq.f32 vr2, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32760;"
        "V0@(pr0)	vr5 = mov.u32 r44;"
        "}"
        "{"
        "V0@(pr0)	vr4 = xor.u32 vr2, vr3;"
        "}"
        "{"
        "V1@(pr0)	vmsk2 = eq.s32 vr1, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr5;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr11;"
        "}"
        "{"
        "V1@(pr0)	vmsk1 = eq.f32 vr1, r46;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr3;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, r46;"
        "}"
        "{"
        "V1@(pr0)	vmsk1 = eq.f32 vr2, r46;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr3;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32768;"
        "V0@(pr0)	vr12 = mov.u32 vr11;"
        "V1@(pr0)	vr13 = xor.u32 vr10, r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr1 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr11 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr10 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr1;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 32767;"
        "V0@(pr0)	vr14 = and.u32 vr10, r44;"
        "V1@(pr0)	vr15 = and.u32 vr11, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 32767;"
        "V0@(pr0)	vr16 = and.u32 vr12, r44;"
        "V1@(pr0)	vr17 = and.u32 vr13, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8192;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 5120;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr15;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 6144;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 7168;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = gt.s32 vr17, vr15;"
        "V1@(pr0)	vmsk1 = eq.s32 vr17, vr15;"
        "}"
        "{"
        "V0@(pr0)	vmsk2 = gt.s32 vr16, vr14;"
        "}"
        "{"
        "V0@(pr0)	vr1 = sel vmsk2 vr13, vr11;"
        "V1@(pr0)	vr0 = sel vmsk2 vr12, vr10;"
        "}"
        "{"
        "V0@(pr0)	vr1 = sel vmsk1 vr13, vr1;"
        "V1@(pr0)	vr0 = sel vmsk1 vr12, vr0;"
        "}"
        "{"
        "V0@(pr0)	vr1 = sel vmsk0 vr1, vr11;"
        "V1@(pr0)	vr0 = sel vmsk0 vr0, vr10;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = eq.s32 vr1, vr11;"
        "V1@(pr0)	vmsk1 = eq.s32 vr0, vr10;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr7 = sel vmsk0 vr5, r48;"
        "V1@(pr0)	vr6 = sel vmsk1 vr5, r48;"
        "}"
        "{"
        "V0@(pr0)	vr7 = and.u32 vr6, vr7;"
        "V1@(pr0)	vmsk0 = eq.s32 vr7, r48;"
        "}"
        "{"
        "V0@(pr0)	vr15 = sel vmsk0 vr11, vr13;"
        "V1@(pr0)	vr14 = sel vmsk0 vr10, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr1;"
        "V1@(pr0)	vr16 = mov.u32 vr0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2047;"
        "pseudo@0	@pseudo imm_1 = 20;"
        "V0@(pr0)	vr7 = mov.u32 r32;"
        "V1@(pr0)	vr6 = mov.u32 r33;"
        "}"
        "{"
        "V1@(pr0)	vr0 = shr.u32 vr15, vr6;"
        "}"
        "{"
        "V0@(pr0)	vr0 = and.u32 vr0, vr7;"
        "}"
        "{"
        "V1@(pr0)	vr1 = shr.u32 vr17, vr6;"
        "}"
        "{"
        "V0@(pr0)	vr1 = and.u32 vr1, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 15;"
        "V0@(pr0)	vr2 = and.u32 vr14, r44;"
        "V1@(pr0)	vr3 = and.u32 vr15, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 15;"
        "V0@(pr0)	vr4 = and.u32 vr16, r44;"
        "V1@(pr0)	vr5 = and.u32 vr17, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32768;"
        "V0@(pr0)	vr6 = and.u32 vr15, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32768;"
        "V0@(pr0)	vr7 = xor.u32 vr15, vr17;"
        "V1@(pr0)	vr7 = and.u32 vr7, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "pseudo@0	@pseudo imm_2 = 3;"
        "V0@(pr0)	vr3 = or.u32 vr3, r44;"
        "V1@(pr0)	vr3 = shl.u32 vr3, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 57344;"
        "pseudo@0	@pseudo imm_2 = 29;"
        "V0@(pr0)	vr28 = and.u32 vr2, r44;"
        "V1@(pr0)	vr28 = shr.u32 vr28, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3;"
        "V0@(pr0)	vr3 = or.u32 vr3, vr28;"
        "V1@(pr0)	vr2 = shl.u32 vr2, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "pseudo@0	@pseudo imm_2 = 3;"
        "V0@(pr0)	vr5 = or.u32 vr5, r44;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 57344;"
        "pseudo@0	@pseudo imm_2 = 29;"
        "V0@(pr0)	vr28 = and.u32 vr4, r44;"
        "V1@(pr0)	vr28 = shr.u32 vr28, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3;"
        "V0@(pr0)	vr5 = or.u32 vr5, vr28;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 64;"
        "V0@(pr0)	vr28 = sub.s32 vr0, vr1;"
        "V1@(pr0)	vr29 = mov.u32 r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr29 = sub.s32 vr29, vr28;"
        "V1@(pr0)	vmsk0 = gt.s32 vr29, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr31 = mov.u32 r32;"
        "V1@(pr0)	vr31 = sub.s32 vr31, vr29;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr30 = mov.u32 r44;"
        "}"
        "{"
        "V1@(pr0)	vr30 = shl.u32 vr30, vr31;"
        "}"
        "{"
        "V0@(pr0)	vr30 = and.u32 vr4, vr30;"
        "V1@(pr0)	vr30 = shr.u32 vr30, vr31;"
        "}"
        "{"
        "V1@(pr0)	vr11 = shl.u32 vr5, vr29;"
        "}"
        "{"
        "V0@(pr0)	vr11 = or.u32 vr11, vr30;"
        "V1@(pr0)	vr10 = shl.u32 vr4, vr29;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V1@(pr0)	vr30 = sub.s32 vr29, r32;"
        "}"
        "{"
        "V1@(pr0)	vr12 = shl.u32 vr4, vr30;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk0 vr10, r46;"
        "V1@(pr0)	vr11 = sel vmsk0 vr11, vr12;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = eq.s32 vr10, r46;"
        "V1@(pr0)	vmsk1 = eq.s32 vr11, r46;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 r46;"
        "V1@(pr0)	vr17 = sel vmsk0 vr11, r48;"
        "}"
        "{"
        "V0@(pr0)	vr16 = sel vmsk1 vr11, r48;"
        "V1@(pr0)	vr17 = add.s32 vr16, vr17;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vmsk0 = eq.s32 vr17, r32;"
        "V1@(pr0)	vr17 = sel vmsk0 vr11, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V1@(pr0)	vmsk0 = gt.s32 vr28, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 65535;"
        "V0@(pr0)	vr31 = mov.u32 r45;"
        "V1@(pr0)	vr30 = mov.u32 r32;"
        "}"
        "{"
        "V0@(pr0)	vr30 = sub.s32 vr30, vr28;"
        "V1@(pr0)	vr31 = shr.u32 vr31, vr30;"
        "}"
        "{"
        "V0@(pr0)	vr31 = and.u32 vr31, vr5;"
        "V1@(pr0)	vr31 = shl.u32 vr31, vr30;"
        "}"
        "{"
        "V1@(pr0)	vr10 = shr.u32 vr4, vr28;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr10, vr31;"
        "V1@(pr0)	vr11 = shr.u32 vr5, vr28;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr31 = sub.s32 vr28, r32;"
        "V1@(pr0)	vr12 = shr.u32 vr5, vr31;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk0 vr10, vr12;"
        "V1@(pr0)	vr11 = sel vmsk0 vr11, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 64;"
        "V0@(pr0)	vr10 = or.u32 vr10, vr17;"
        "V1@(pr0)	vmsk0 = ls.s32 vr28, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 r46;"
        "V1@(pr0)	vr11 = sel vmsk0 vr17, vr11;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 r48;"
        "V1@(pr0)	vr10 = sel vmsk0 vr17, vr10;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = eq.s32 vr28, r36;"
        "V1@(pr0)	vr4 = sel vmsk0 vr10, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "V0@(pr0)	vr5 = sel vmsk0 vr11, vr5;"
        "V1@(pr0)	vr10 = and.u32 vr2, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr12 = and.u32 vr4, r33;"
        "V1@(pr0)	vr11 = shr.u32 vr2, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr13 = shr.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = gteq.s32 vr10, vr12;"
        "V1@(pr0)	vr28 = mov.u32 r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr17 = sel vmsk0 vr28, r46;"
        "V1@(pr0)	vr17 = shl.u32 vr17, r32;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr10, vr17;"
        "V1@(pr0)	vr14 = sub.s32 vr10, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr15 = add.s32 vr13, r48;"
        "V1@(pr0)	vr13 = sel vmsk0 vr15, vr13;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = gteq.s32 vr11, vr13;"
        "V1@(pr0)	vr17 = sel vmsk0 vr28, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr17 = shl.u32 vr17, r32;"
        "}"
        "{"
        "V0@(pr0)	vr11 = or.u32 vr11, vr17;"
        "V1@(pr0)	vr15 = sub.s32 vr11, vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr14, vr15;"
        "}"
        "{"
        "V0@(pr0)	vr17 = add.s32 vr5, r48;"
        "V1@(pr0)	vr17 = sel vmsk0 vr17, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr11 = sub.s32 vr3, vr17;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = eq.s32 vr11, r46;"
        "V1@(pr0)	vr31 = count.u32 vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr30 = count.u32 vr10;"
        "V1@(pr0)	vr30 = add.s32 vr30, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr31 = sel vmsk0 vr31, vr30;"
        "V1@(pr0)	vr31 = sub.s32 vr31, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vmsk6 = ls.s32 vr31, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr30 = mov.u32 r32;"
        "V1@(pr0)	vr30 = sub.s32 vr30, vr31;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr29 = mov.u32 r44;"
        "V1@(pr0)	vr29 = shl.u32 vr29, vr30;"
        "}"
        "{"
        "V0@(pr0)	vr29 = and.u32 vr29, vr10;"
        "V1@(pr0)	vr29 = shr.u32 vr29, vr30;"
        "}"
        "{"
        "V1@(pr0)	vr13 = shl.u32 vr11, vr31;"
        "}"
        "{"
        "V0@(pr0)	vr13 = or.u32 vr13, vr29;"
        "V1@(pr0)	vr12 = shl.u32 vr10, vr31;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr30 = mov.u32 r32;"
        "V1@(pr0)	vr30 = sub.s32 vr31, vr30;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr14 = shl.u32 vr10, vr30;"
        "}"
        "{"
        "V0@(pr0)	vr12 = sel vmsk6 vr15, vr12;"
        "V1@(pr0)	vr13 = sel vmsk6 vr14, vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 128;"
        "V0@(pr0)	vmsk0 = ls.s32 vr11, r36;"
        "V1@(pr0)	vr10 = sel vmsk0 vr10, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr15 = sub.s32 vr0, vr31;"
        "}"
        "{"
        "V0@(pr0)	vr11 = sel vmsk0 vr11, vr13;"
        "V1@(pr0)	vr15 = sel vmsk0 vr0, vr15;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr12 = and.u32 vr2, r33;"
        "V1@(pr0)	vr13 = shr.u32 vr2, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr16 = and.u32 vr4, r33;"
        "V1@(pr0)	vr17 = shr.u32 vr4, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr28 = add.s32 vr12, vr16;"
        "V1@(pr0)	vr14 = shr.u32 vr28, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "V0@(pr0)	vr28 = and.u32 vr28, r32;"
        "}"
        "{"
        "V0@(pr0)	vr29 = add.s32 vr13, vr17;"
        "V1@(pr0)	vr29 = add.s32 vr29, vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr29, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "V0@(pr0)	vr29 = and.u32 vr29, r32;"
        "V1@(pr0)	vr29 = shl.u32 vr29, r33;"
        "}"
        "{"
        "V0@(pr0)	vr12 = or.u32 vr29, vr28;"
        "V1@(pr0)	vr13 = add.s32 vr3, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr13 = add.s32 vr13, vr14;"
        "V1@(pr0)	vr28 = and.u32 vr12, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V0@(pr0)	vr14 = and.u32 vr13, r48;"
        "V1@(pr0)	vr14 = shl.u32 vr14, r32;"
        "}"
        "{"
        "V1@(pr0)	vr16 = shr.u32 vr12, r48;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr16, vr14;"
        "V1@(pr0)	vr17 = shr.u32 vr13, r48;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr16, vr28;"
        "V1@(pr0)	vr14 = add.s32 vr0, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 256;"
        "V0@(pr0)	vr28 = and.u32 vr13, r36;"
        "V1@(pr0)	vmsk0 = eq.s32 vr28, r46;"
        "}"
        "{"
        "V0@(pr0)	vr12 = sel vmsk0 vr16, vr12;"
        "V1@(pr0)	vr13 = sel vmsk0 vr17, vr13;"
        "}"
        "{"
        "V0@(pr0)	vr14 = sel vmsk0 vr14, vr0;"
        "V1@(pr0)	vmsk1 = eq.s32 vr7, r46;"
        "}"
        "{"
        "V0@(pr0)	vr2 = sel vmsk1 vr10, vr12;"
        "V1@(pr0)	vr3 = sel vmsk1 vr11, vr13;"
        "}"
        "{"
        "V0@(pr0)	vr0 = sel vmsk1 vr15, vr14;"
        "V1@(pr0)	vr31 = mov.u32 r48;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr2, r48;"
        "V1@(pr0)	vmsk0 = lseq.s32 vr0, r46;"
        "}"
        "{"
        "V0@(pr0)	vr2 = sel vmsk0 vr2, vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 7;"
        "pseudo@0	@pseudo imm_1 = 3;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 65535;"
        "V0@(pr0)	vr17 = and.u32 vr2, r32;"
        "V1@(pr0)	vr31 = mov.u32 r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 29;"
        "V1@(pr0)	vr31 = shr.u32 vr31, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 29;"
        "V0@(pr0)	vr31 = and.u32 vr3, vr31;"
        "V1@(pr0)	vr31 = shl.u32 vr31, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3;"
        "V1@(pr0)	vr10 = shr.u32 vr2, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3;"
        "V0@(pr0)	vr10 = or.u32 vr10, vr31;"
        "V1@(pr0)	vr11 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 15;"
        "V0@(pr0)	vr10 = and.u32 vr10, r44;"
        "V1@(pr0)	vr11 = and.u32 vr11, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr28 = shl.u32 vr0, r32;"
        "}"
        "{"
        "V0@(pr0)	vr11 = or.u32 vr11, vr28;"
        "V1@(pr0)	vr11 = or.u32 vr11, vr6;"
        "}"
        "{"
        "V0@(pr0)	vr12 = add.s32 vr10, r48;"
        "V1@(pr0)	vr13 = and.u32 vr10, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "V0@(pr0)	vmsk0 = gt.s32 vr17, r32;"
        "V1@(pr0)	vmsk1 = eq.s32 vr17, r32;"
        "}"
        "{"
        "V0@(pr0)	vr13 = add.s32 vr10, vr13;"
        "V1@(pr0)	vr13 = sel vmsk1 vr10, vr13;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk0 vr13, vr12;"
        "V1@(pr0)	vr0 = mov.u32 vr10;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mov.u32 vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr10 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr11 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr12 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr13 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 5120;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 6144;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr16 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 7168;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr17 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8192;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = eq.s32 vr17, r46;"
        "V1@(pr0)	vmsk1 = eq.s32 vr16, r46;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 r46;"
        "V1@(pr0)	vr7 = sel vmsk0 vr28, r48;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr28, r48;"
        "V1@(pr0)	vr7 = add.s32 vr6, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vmsk0 = eq.s32 vr7, r32;"
        "V1@(pr0)	vr30 = sel vmsk0 vr0, vr10;"
        "}"
        "{"
        "V0@(pr0)	vr31 = sel vmsk0 vr1, vr11;"
        "V1@(pr0)	vr6 = and.u32 vr10, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr7 = and.u32 vr11, vr13;"
        "V1@(pr0)	vr5 = sel vmsk0 vr13, vr7;"
        "}"
        "{"
        "V0@(pr0)	vr4 = sel vmsk0 vr12, vr6;"
        "V1@(pr0)	vmsk0 = eq.s32 vr14, r46;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = eq.s32 vr15, r46;"
        "V1@(pr0)	vr6 = sel vmsk0 vr28, r48;"
        "}"
        "{"
        "V0@(pr0)	vr7 = sel vmsk1 vr28, r48;"
        "V1@(pr0)	vr7 = add.s32 vr6, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vmsk0 = eq.s32 vr7, r32;"
        "V1@(pr0)	vr31 = sel vmsk0 vr31, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr30 = sel vmsk0 vr30, r34;"
        "V1@(pr0)	vr7 = mov.u32 r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 15;"
        "V0@(pr0)	vr6 = mov.u32 r44;"
        "V1@(pr0)	vr7 = xor.u32 vr7, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr6 = xor.u32 vr6, r44;"
        "V1@(pr0)	vmsk0 = eq.s32 vr16, vr6;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mov.u32 vr7;"
        "V1@(pr0)	vr4 = mov.u32 vr6;"
        "}"
        "{"
        "V0@(pr0)	vmsk6 = eq.s32 vr14, vr6;"
        "V1@(pr0)	vmsk7 = eq.s32 vr15, vr7;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = eq.s32 vr17, vr7;"
        "V1@(pr0)	vr7 = sel vmsk0 vr28, r48;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr28, r48;"
        "V1@(pr0)	vr7 = add.s32 vr6, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vmsk0 = eq.s32 vr7, r32;"
        "V1@(pr0)	vr31 = sel vmsk0 vr31, vr13;"
        "}"
        "{"
        "V0@(pr0)	vr30 = sel vmsk0 vr30, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr7 = sel vmsk7 vr28, r48;"
        "V1@(pr0)	vr6 = sel vmsk6 vr28, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vr7 = add.s32 vr6, vr7;"
        "V1@(pr0)	vmsk7 = eq.s32 vr7, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr3 = or.u32 vr5, r36;"
        "V1@(pr0)	vr6 = xor.u32 vr10, vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32768;"
        "V0@(pr0)	vr7 = xor.u32 vr11, vr13;"
        "V1@(pr0)	vmsk0 = eq.s32 vr7, r36;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = eq.s32 vr6, r46;"
        "V1@(pr0)	vr7 = sel vmsk0 vr28, r48;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr28, r48;"
        "V1@(pr0)	vr7 = add.s32 vr6, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vmsk0 = eq.s32 vr7, r32;"
        "V1@(pr0)	vr7 = sel vmsk0 vr11, vr3;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk0 vr10, vr4;"
        "V1@(pr0)	vr30 = sel vmsk7 vr30, vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr31 = sel vmsk7 vr31, vr7;"
        "V1@(pr0)	vr7 = or.u32 vr13, r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk0 = eq.s32 vr16, r46;"
        "V1@(pr0)	vr6 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "V0@(pr0)	vr28 = and.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32752;"
        "V0@(pr0)	vmsk1 = gt.s32 vr6, r32;"
        "V1@(pr0)	vmsk2 = eq.s32 vr28, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk0 vr28, r48;"
        "V1@(pr0)	vr4 = sel vmsk2 vr28, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vr5 = add.s32 vr4, vr5;"
        "V1@(pr0)	vmsk2 = eq.s32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr7, vr31;"
        "V1@(pr0)	vr4 = sel vmsk2 vr12, vr30;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32752;"
        "V0@(pr0)	vmsk2 = eq.s32 vr6, r32;"
        "V1@(pr0)	vr4 = sel vmsk2 vr30, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr31, vr5;"
        "V1@(pr0)	vr31 = sel vmsk1 vr5, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr30 = sel vmsk1 vr4, vr12;"
        "V1@(pr0)	vr7 = or.u32 vr11, r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk0 = eq.s32 vr14, r46;"
        "V1@(pr0)	vr6 = shr.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "V0@(pr0)	vr28 = and.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32752;"
        "V0@(pr0)	vmsk1 = gt.s32 vr6, r32;"
        "V1@(pr0)	vmsk2 = eq.s32 vr28, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk0 vr28, r48;"
        "V1@(pr0)	vr4 = sel vmsk2 vr28, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vr5 = add.s32 vr4, vr5;"
        "V1@(pr0)	vmsk2 = eq.s32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr7, vr31;"
        "V1@(pr0)	vr4 = sel vmsk2 vr10, vr30;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32752;"
        "V0@(pr0)	vmsk2 = eq.s32 vr6, r32;"
        "V1@(pr0)	vr4 = sel vmsk2 vr30, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr31, vr5;"
        "V1@(pr0)	vr11 = sel vmsk1 vr5, vr7;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr4, vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr12 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr1 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V1@(pr0)	vr2 = shr.u32 vr1, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 5120;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr12;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 26;"
        "V1@(pr0)	vr1 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 15;"
        "V1@(pr0)	vr1 = and.u32 vr1, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 162;"
        "pseudo@0	@pseudo imm_3 = 0;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 28238;"
        "pseudo@0	@pseudo imm_1 = 63875;"
        "pseudo@0	@pseudo imm_2 = 10748;"
        "pseudo@0	@pseudo imm_3 = 17429;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "pseudo@0	@pseudo imm_2 = 41721;"
        "pseudo@0	@pseudo imm_3 = 0;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20036;"
        "pseudo@0	@pseudo imm_1 = 33646;"
        "pseudo@0	@pseudo imm_2 = 64551;"
        "pseudo@0	@pseudo imm_3 = 5417;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "pseudo@0	@pseudo imm_2 = 63875;"
        "pseudo@0	@pseudo imm_3 = 162;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 17429;"
        "pseudo@0	@pseudo imm_1 = 28238;"
        "pseudo@0	@pseudo imm_2 = 10071;"
        "pseudo@0	@pseudo imm_3 = 10748;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3;"
        "pseudo@0	@pseudo imm_2 = 33646;"
        "pseudo@0	@pseudo imm_3 = 41721;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 5417;"
        "pseudo@0	@pseudo imm_1 = 20036;"
        "pseudo@0	@pseudo imm_2 = 22481;"
        "pseudo@0	@pseudo imm_3 = 64551;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "pseudo@0	@pseudo imm_2 = 28238;"
        "pseudo@0	@pseudo imm_3 = 63875;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 10748;"
        "pseudo@0	@pseudo imm_1 = 17429;"
        "pseudo@0	@pseudo imm_2 = 53749;"
        "pseudo@0	@pseudo imm_3 = 10071;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 5;"
        "pseudo@0	@pseudo imm_2 = 20036;"
        "pseudo@0	@pseudo imm_3 = 33646;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 64551;"
        "pseudo@0	@pseudo imm_1 = 5417;"
        "pseudo@0	@pseudo imm_2 = 62772;"
        "pseudo@0	@pseudo imm_3 = 22481;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 6;"
        "pseudo@0	@pseudo imm_2 = 17429;"
        "pseudo@0	@pseudo imm_3 = 28238;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 10071;"
        "pseudo@0	@pseudo imm_1 = 10748;"
        "pseudo@0	@pseudo imm_2 = 13533;"
        "pseudo@0	@pseudo imm_3 = 53749;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 7;"
        "pseudo@0	@pseudo imm_2 = 5417;"
        "pseudo@0	@pseudo imm_3 = 20036;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 22481;"
        "pseudo@0	@pseudo imm_1 = 64551;"
        "pseudo@0	@pseudo imm_2 = 56768;"
        "pseudo@0	@pseudo imm_3 = 62772;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "pseudo@0	@pseudo imm_2 = 10748;"
        "pseudo@0	@pseudo imm_3 = 17429;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 53749;"
        "pseudo@0	@pseudo imm_1 = 10071;"
        "pseudo@0	@pseudo imm_2 = 49371;"
        "pseudo@0	@pseudo imm_3 = 13533;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 9;"
        "pseudo@0	@pseudo imm_2 = 64551;"
        "pseudo@0	@pseudo imm_3 = 5417;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 62772;"
        "pseudo@0	@pseudo imm_1 = 22481;"
        "pseudo@0	@pseudo imm_2 = 56162;"
        "pseudo@0	@pseudo imm_3 = 56768;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 10;"
        "pseudo@0	@pseudo imm_2 = 10071;"
        "pseudo@0	@pseudo imm_3 = 10748;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 13533;"
        "pseudo@0	@pseudo imm_1 = 53749;"
        "pseudo@0	@pseudo imm_2 = 25237;"
        "pseudo@0	@pseudo imm_3 = 49371;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 11;"
        "pseudo@0	@pseudo imm_2 = 22481;"
        "pseudo@0	@pseudo imm_3 = 64551;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 56768;"
        "pseudo@0	@pseudo imm_1 = 62772;"
        "pseudo@0	@pseudo imm_2 = 38297;"
        "pseudo@0	@pseudo imm_3 = 56162;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 12;"
        "pseudo@0	@pseudo imm_2 = 53749;"
        "pseudo@0	@pseudo imm_3 = 10071;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 49371;"
        "pseudo@0	@pseudo imm_1 = 13533;"
        "pseudo@0	@pseudo imm_2 = 39228;"
        "pseudo@0	@pseudo imm_3 = 25237;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 13;"
        "pseudo@0	@pseudo imm_2 = 62772;"
        "pseudo@0	@pseudo imm_3 = 22481;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 56162;"
        "pseudo@0	@pseudo imm_1 = 56768;"
        "pseudo@0	@pseudo imm_2 = 15427;"
        "pseudo@0	@pseudo imm_3 = 38297;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 14;"
        "pseudo@0	@pseudo imm_2 = 13533;"
        "pseudo@0	@pseudo imm_3 = 53749;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 25237;"
        "pseudo@0	@pseudo imm_1 = 49371;"
        "pseudo@0	@pseudo imm_2 = 17296;"
        "pseudo@0	@pseudo imm_3 = 39228;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 23;"
        "V1@(pr0)	vr1 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 7;"
        "V1@(pr0)	vr1 = and.u32 vr1, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 255;"
        "pseudo@0	@pseudo imm_2 = 128;"
        "V0@(pr0)	vr10 = and.u32 vr10, r44;"
        "V1@(pr0)	vr10 = or.u32 vr10, r38;"
        "}"
        "{"
        "V1@(pr0)	vr10 = shl.u32 vr10, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr10;"
        "V1@(pr0)	vr13 = mov.u32 vr28;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr14;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr12, r54;"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr12, r32;"
        "}"
        "{"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr13, r32;"
        "}"
        "{"
        "V1@(pr0)	vr15 = and.u32 vr12, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr16;"
        "V1@(pr0)	vr13 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 vr13;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr10;"
        "V1@(pr0)	vr13 = mov.u32 vr30;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr14;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr12, r54;"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr12, r32;"
        "}"
        "{"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr13, r32;"
        "}"
        "{"
        "V1@(pr0)	vr15 = and.u32 vr12, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr16;"
        "V1@(pr0)	vr13 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr30 = mov.u32 vr12;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr10;"
        "V1@(pr0)	vr13 = mov.u32 vr29;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr14;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr12, r54;"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr12, r32;"
        "}"
        "{"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr13, r32;"
        "}"
        "{"
        "V1@(pr0)	vr15 = and.u32 vr12, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr16;"
        "V1@(pr0)	vr13 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr12;"
        "V1@(pr0)	vr17 = mov.u32 vr13;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 vr28;"
        "V1@(pr0)	vr14 = mov.u32 vr30;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8192;"
        "V0@(pr0)	vr1 = add.s32 vr2, r32;"
        "V1@(pr0)	vr1 = and.u32 vr1, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 14;"
        "V1@(pr0)	vr1 = shr.u32 vr1, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 14;"
        "V1@(pr0)	vr3 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V1@(pr0)	vr2 = sub.s32 vr2, vr3;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr16 = and.u32 vr16, r54;"
        "V1@(pr0)	vr2 = shl.u32 vr2, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr16, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr1;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32768;"
        "V0@(pr0)	vr12 = mov.u32 r44;"
        "V1@(pr0)	vmsk7 = gteq.f32 vr16, r46;"
        "}"
        "{"
        "V1@(pr0)	vr12 = sel vmsk7 vr12, r46;"
        "}"
        "{"
        "V0@(pr0)	vmsk3 = eq.s32 vr17, r46;"
        "V1@(pr0)	vr6 = sub.s32 vr17, r48;"
        "}"
        "{"
        "V1@(pr0)	vr7 = sub.s32 vr16, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr6 = sel vmsk3 vr6, r44;"
        "V1@(pr0)	vr7 = sel vmsk3 vr16, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr6 = xor.u32 vr6, r44;"
        "V1@(pr0)	vr7 = xor.u32 vr7, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr17 = sel vmsk7 vr6, vr17;"
        "V1@(pr0)	vr16 = sel vmsk7 vr7, vr16;"
        "}"
        "{"
        "V0@(pr0)	vr2 = count.u32 vr16;"
        "V1@(pr0)	vr1 = count.u32 vr17;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vmsk6 = ls.s32 vr2, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V0@(pr0)	vr6 = mov.u32 r32;"
        "V1@(pr0)	vr1 = sub.s32 vr6, vr1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 63;"
        "V0@(pr0)	vr6 = mov.u32 r32;"
        "V1@(pr0)	vr2 = sub.s32 vr6, vr2;"
        "}"
        "{"
        "V1@(pr0)	vr13 = sel vmsk6 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr0 = mov.u32 vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1023;"
        "V0@(pr0)	vr13 = add.s32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr13 = shl.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 20;"
        "V0@(pr0)	vmsk5 = lseq.s32 vr0, r32;"
        "V1@(pr0)	vmsk4 = lseq.s32 vr0, r33;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr6 = sub.s32 vr0, r32;"
        "V1@(pr0)	vr7 = mov.u32 r48;"
        "}"
        "{"
        "V1@(pr0)	vr6 = shl.u32 vr7, vr6;"
        "}"
        "{"
        "V1@(pr0)	vr6 = sub.s32 vr16, vr6;"
        "}"
        "{"
        "V1@(pr0)	vr7 = shl.u32 vr7, vr0;"
        "}"
        "{"
        "V1@(pr0)	vr7 = sub.s32 vr17, vr7;"
        "}"
        "{"
        "V1@(pr0)	vr5 = sel vmsk4 vr6, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "V0@(pr0)	vr6 = sub.s32 vr0, r32;"
        "V1@(pr0)	vr3 = shr.u32 vr5, vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 84;"
        "V0@(pr0)	vr6 = mov.u32 r32;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sub.s32 vr6, vr0;"
        "V1@(pr0)	vr4 = shl.u32 vr5, vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "V0@(pr0)	vr6 = sub.s32 vr0, r32;"
        "V1@(pr0)	vr6 = shr.u32 vr17, vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V0@(pr0)	vr4 = add.s32 vr4, vr6;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 vr3;"
        "V1@(pr0)	vr14 = mov.u32 vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "V0@(pr0)	vr6 = mov.u32 r32;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sub.s32 vr6, vr0;"
        "V1@(pr0)	vr3 = shl.u32 vr5, vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V0@(pr0)	vr6 = sub.s32 vr0, r32;"
        "V1@(pr0)	vr6 = shr.u32 vr17, vr6;"
        "}"
        "{"
        "V1@(pr0)	vr3 = add.s32 vr3, vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "V0@(pr0)	vr7 = mov.u32 r32;"
        "}"
        "{"
        "V0@(pr0)	vr7 = sub.s32 vr7, vr0;"
        "V1@(pr0)	vr4 = shl.u32 vr17, vr7;"
        "}"
        "{"
        "V0@(pr0)	vr15 = sel vmsk5 vr15, vr3;"
        "V1@(pr0)	vr14 = sel vmsk5 vr14, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V0@(pr0)	vr6 = mov.u32 r32;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sub.s32 vr6, vr0;"
        "V1@(pr0)	vr3 = shl.u32 vr5, vr6;"
        "}"
        "{"
        "V0@(pr0)	vr15 = sel vmsk4 vr15, vr3;"
        "V1@(pr0)	vr14 = sel vmsk4 vr14, r46;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr12, vr13;"
        "V1@(pr0)	vr10 = or.u32 vr10, vr15;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8699;"
        "pseudo@0	@pseudo imm_1 = 15385;"
        "pseudo@0	@pseudo imm_2 = 11544;"
        "pseudo@0	@pseudo imm_3 = 21572;"
        "V0@(pr0)	vr12 = mov.u32 r44;"
        "V1@(pr0)	vr13 = mov.u32 r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr13;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr1 = and.u32 vr10, r44;"
        "V1@(pr0)	vr2 = and.u32 vr12, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "V0@(pr0)	vmsk1 = ls.f32 vr1, r44;"
        "V1@(pr0)	vmsk2 = ls.f32 vr2, r44;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, r46;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, r46;"
        "}"
        "{"
        "V0@(pr0)	vr12 = sel vmsk2 vr12, r46;"
        "V1@(pr0)	vr13 = sel vmsk2 vr13, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr6 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr7 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 15;"
        "V0@(pr0)	vr10 = and.u32 vr10, r44;"
        "V1@(pr0)	vr12 = and.u32 vr12, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "V0@(pr0)	vr10 = or.u32 vr10, r44;"
        "V1@(pr0)	vr12 = or.u32 vr12, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 11;"
        "V1@(pr0)	vr12 = shl.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 21;"
        "V1@(pr0)	vr1 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 11;"
        "V0@(pr0)	vr12 = or.u32 vr12, vr1;"
        "V1@(pr0)	vr13 = shl.u32 vr13, r32;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr13, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = and.u32 vr12, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr13, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr12, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = and.u32 vr12, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr12, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 r46;"
        "V1@(pr0)	vr10 = mov.u32 vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr10;"
        "V1@(pr0)	vr17 = mov.u32 vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = or.u32 vr1, vr2;"
        "V1@(pr0)	vr14 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr12 = mov.u32 r46;"
        "V1@(pr0)	vr13 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "V1@(pr0)	vr15 = shr.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 vr15;"
        "V1@(pr0)	vr29 = mov.u32 vr14;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr15;"
        "V1@(pr0)	vr15 = mov.u32 vr0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr16;"
        "V1@(pr0)	vr11 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr12;"
        "V1@(pr0)	vr17 = mov.u32 vr13;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr29;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 vr15;"
        "V1@(pr0)	vr29 = mov.u32 vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr16;"
        "V1@(pr0)	vr13 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr10;"
        "V1@(pr0)	vr17 = mov.u32 vr11;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr28;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr16;"
        "V1@(pr0)	vr11 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr29 = shl.u32 vr29, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr12;"
        "V1@(pr0)	vr17 = mov.u32 vr13;"
        "}"
        "{"
        "V0@(pr0)	vr30 = mov.u32 vr14;"
        "V1@(pr0)	vr14 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr31 = mov.u32 vr17;"
        "V1@(pr0)	vr17 = mov.u32 vr16;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr11;"
        "V1@(pr0)	vr15 = mov.u32 vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr16;"
        "V1@(pr0)	vr11 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr13;"
        "V1@(pr0)	vr13 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr28;"
        "V1@(pr0)	vr17 = mov.u32 vr29;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr30;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr13;"
        "V1@(pr0)	vr15 = mov.u32 vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr13 = mov.u32 vr17;"
        "V1@(pr0)	vr12 = mov.u32 vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "V0@(pr0)	vr3 = xor.u32 vr6, vr7;"
        "V1@(pr0)	vr3 = and.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr3 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 2047;"
        "pseudo@0	@pseudo imm_3 = 0;"
        "V0@(pr0)	vr7 = and.u32 vr7, r45;"
        "V1@(pr0)	vr6 = and.u32 vr6, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 1023;"
        "pseudo@0	@pseudo imm_3 = 0;"
        "V0@(pr0)	vr6 = add.s32 vr6, vr7;"
        "V1@(pr0)	vr6 = sub.s32 vr6, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 16;"
        "V0@(pr0)	vr1 = and.u32 vr10, r45;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr6, r48;"
        "V1@(pr0)	vmsk1 = eq.s32 vr1, r46;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr2, vr6;"
        "}"
        "{"
        "V1@(pr0)	vr2 = shl.u32 vr10, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V1@(pr0)	vr5 = shr.u32 vr11, r32;"
        "}"
        "{"
        "V0@(pr0)	vr2 = or.u32 vr2, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V1@(pr0)	vr5 = shl.u32 vr13, r32;"
        "}"
        "{"
        "V1@(pr0)	vr4 = shl.u32 vr11, r48;"
        "}"
        "{"
        "V0@(pr0)	vr5 = or.u32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr2;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr5;"
        "}"
        "{"
        "V1@(pr0)	vr2 = shl.u32 vr12, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V1@(pr0)	vr5 = shr.u32 vr13, r32;"
        "}"
        "{"
        "V0@(pr0)	vr2 = or.u32 vr2, vr5;"
        "}"
        "{"
        "V1@(pr0)	vr5 = shl.u32 vr13, r48;"
        "}"
        "{"
        "V0@(pr0)	vr12 = sel vmsk1 vr12, vr2;"
        "V1@(pr0)	vr13 = sel vmsk1 vr13, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 15;"
        "V0@(pr0)	vr1 = and.u32 vr10, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr2 = shl.u32 vr6, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = gt.s32 vr6, r46;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr10, vr3;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32768;"
        "V0@(pr0)	vr1 = add.s32 vr11, r48;"
        "V1@(pr0)	vmsk1 = gt.s32 vr12, r44;"
        "}"
        "{"
        "V0@(pr0)	vr11 = sel vmsk1 vr11, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr1 = and.u32 vr11, r48;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr11;"
        "V1@(pr0)	vr15 = mov.u32 vr10;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr1;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32768;"
        "V1@(pr0)	vmsk1 = eq.s32 vr12, r45;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr16;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr17;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr28 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr29 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr30 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr31 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr1 = and.u32 vr28, r44;"
        "V1@(pr0)	vr2 = and.u32 vr30, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = lseq.s32 vr1, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 8;"
        "V1@(pr0)	vr1 = or.u32 vr28, r45;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr1, vr10;"
        "V1@(pr0)	vr11 = sel vmsk1 vr29, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = lseq.s32 vr2, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 8;"
        "V1@(pr0)	vr1 = or.u32 vr30, r45;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr1, vr10;"
        "V1@(pr0)	vr11 = sel vmsk1 vr31, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32768;"
        "V0@(pr0)	vr3 = xor.u32 vr28, vr30;"
        "V1@(pr0)	vr3 = and.u32 vr3, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = eq.f32 vr1, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32760;"
        "V0@(pr0)	vr5 = mov.u32 r44;"
        "}"
        "{"
        "V0@(pr0)	vr4 = xor.u32 vr1, vr3;"
        "}"
        "{"
        "V1@(pr0)	vmsk2 = eq.s32 vr2, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr5;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = eq.f32 vr2, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32760;"
        "V0@(pr0)	vr5 = mov.u32 r44;"
        "}"
        "{"
        "V0@(pr0)	vr4 = xor.u32 vr2, vr3;"
        "}"
        "{"
        "V1@(pr0)	vmsk2 = eq.s32 vr1, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr5;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr11;"
        "}"
        "{"
        "V1@(pr0)	vmsk1 = eq.f32 vr1, r46;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr3;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, r46;"
        "}"
        "{"
        "V1@(pr0)	vmsk1 = eq.f32 vr2, r46;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr3;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr12 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr5 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr4 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr1 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr2 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr3 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 5120;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32768;"
        "V0@(pr0)	vmsk1 = eq.s32 vr4, r48;"
        "V1@(pr0)	vr6 = xor.u32 vr10, r36;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 17136;"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 20224;"
        "V0@(pr0)	vr6 = and.u32 vr5, r45;"
        "V1@(pr0)	vmsk4 = ls.f32 vr6, r44;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk4 vr10, vr2;"
        "V1@(pr0)	vr11 = sel vmsk4 vr11, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr13 = sel vmsk4 vr12, vr3;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 49152;"
        "pseudo@0	@pseudo imm_2 = 3;"
        "V0@(pr0)	vr4 = and.u32 vr10, r44;"
        "V1@(pr0)	vr1 = shl.u32 vr10, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 16383;"
        "V0@(pr0)	vr1 = and.u32 vr1, r44;"
        "V1@(pr0)	vr4 = or.u32 vr1, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 29;"
        "V1@(pr0)	vr1 = shr.u32 vr11, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = or.u32 vr1, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr13;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 29;"
        "V1@(pr0)	vr12 = shl.u32 vr4, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 49152;"
        "pseudo@0	@pseudo imm_2 = 3;"
        "V0@(pr0)	vr6 = and.u32 vr4, r44;"
        "V1@(pr0)	vr2 = shr.u32 vr4, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 2047;"
        "V0@(pr0)	vr2 = and.u32 vr2, r44;"
        "V1@(pr0)	vr6 = or.u32 vr2, vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 2048;"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 14336;"
        "V0@(pr0)	vr2 = and.u32 vr4, r44;"
        "V1@(pr0)	vr3 = or.u32 vr6, r45;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = eq.s32 vr2, r46;"
        "V1@(pr0)	vr6 = sel vmsk1 vr3, vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32768;"
        "V1@(pr0)	vr13 = xor.u32 vr6, r36;"
        "}"
        "{"
        "V0@(pr0)	vr2 = mov.u32 vr10;"
        "V1@(pr0)	vr10 = mov.u32 vr11;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 32767;"
        "V0@(pr0)	vr14 = and.u32 vr10, r44;"
        "V1@(pr0)	vr15 = and.u32 vr11, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 32767;"
        "V0@(pr0)	vr16 = and.u32 vr12, r44;"
        "V1@(pr0)	vr17 = and.u32 vr13, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8192;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 5120;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr15;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 6144;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 7168;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = gt.s32 vr17, vr15;"
        "V1@(pr0)	vmsk1 = eq.s32 vr17, vr15;"
        "}"
        "{"
        "V0@(pr0)	vmsk2 = gt.s32 vr16, vr14;"
        "}"
        "{"
        "V0@(pr0)	vr1 = sel vmsk2 vr13, vr11;"
        "V1@(pr0)	vr0 = sel vmsk2 vr12, vr10;"
        "}"
        "{"
        "V0@(pr0)	vr1 = sel vmsk1 vr13, vr1;"
        "V1@(pr0)	vr0 = sel vmsk1 vr12, vr0;"
        "}"
        "{"
        "V0@(pr0)	vr1 = sel vmsk0 vr1, vr11;"
        "V1@(pr0)	vr0 = sel vmsk0 vr0, vr10;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = eq.s32 vr1, vr11;"
        "V1@(pr0)	vmsk1 = eq.s32 vr0, vr10;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr7 = sel vmsk0 vr5, r48;"
        "V1@(pr0)	vr6 = sel vmsk1 vr5, r48;"
        "}"
        "{"
        "V0@(pr0)	vr7 = and.u32 vr6, vr7;"
        "V1@(pr0)	vmsk0 = eq.s32 vr7, r48;"
        "}"
        "{"
        "V0@(pr0)	vr15 = sel vmsk0 vr11, vr13;"
        "V1@(pr0)	vr14 = sel vmsk0 vr10, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr1;"
        "V1@(pr0)	vr16 = mov.u32 vr0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2047;"
        "pseudo@0	@pseudo imm_1 = 20;"
        "V0@(pr0)	vr7 = mov.u32 r32;"
        "V1@(pr0)	vr6 = mov.u32 r33;"
        "}"
        "{"
        "V1@(pr0)	vr0 = shr.u32 vr15, vr6;"
        "}"
        "{"
        "V0@(pr0)	vr0 = and.u32 vr0, vr7;"
        "}"
        "{"
        "V1@(pr0)	vr1 = shr.u32 vr17, vr6;"
        "}"
        "{"
        "V0@(pr0)	vr1 = and.u32 vr1, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 15;"
        "V0@(pr0)	vr2 = and.u32 vr14, r44;"
        "V1@(pr0)	vr3 = and.u32 vr15, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 15;"
        "V0@(pr0)	vr4 = and.u32 vr16, r44;"
        "V1@(pr0)	vr5 = and.u32 vr17, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32768;"
        "V0@(pr0)	vr6 = and.u32 vr15, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32768;"
        "V0@(pr0)	vr7 = xor.u32 vr15, vr17;"
        "V1@(pr0)	vr7 = and.u32 vr7, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "pseudo@0	@pseudo imm_2 = 3;"
        "V0@(pr0)	vr3 = or.u32 vr3, r44;"
        "V1@(pr0)	vr3 = shl.u32 vr3, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 57344;"
        "pseudo@0	@pseudo imm_2 = 29;"
        "V0@(pr0)	vr28 = and.u32 vr2, r44;"
        "V1@(pr0)	vr28 = shr.u32 vr28, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3;"
        "V0@(pr0)	vr3 = or.u32 vr3, vr28;"
        "V1@(pr0)	vr2 = shl.u32 vr2, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "pseudo@0	@pseudo imm_2 = 3;"
        "V0@(pr0)	vr5 = or.u32 vr5, r44;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 57344;"
        "pseudo@0	@pseudo imm_2 = 29;"
        "V0@(pr0)	vr28 = and.u32 vr4, r44;"
        "V1@(pr0)	vr28 = shr.u32 vr28, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3;"
        "V0@(pr0)	vr5 = or.u32 vr5, vr28;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 64;"
        "V0@(pr0)	vr28 = sub.s32 vr0, vr1;"
        "V1@(pr0)	vr29 = mov.u32 r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr29 = sub.s32 vr29, vr28;"
        "V1@(pr0)	vmsk0 = gt.s32 vr29, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr31 = mov.u32 r32;"
        "V1@(pr0)	vr31 = sub.s32 vr31, vr29;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr30 = mov.u32 r44;"
        "}"
        "{"
        "V1@(pr0)	vr30 = shl.u32 vr30, vr31;"
        "}"
        "{"
        "V0@(pr0)	vr30 = and.u32 vr4, vr30;"
        "V1@(pr0)	vr30 = shr.u32 vr30, vr31;"
        "}"
        "{"
        "V1@(pr0)	vr11 = shl.u32 vr5, vr29;"
        "}"
        "{"
        "V0@(pr0)	vr11 = or.u32 vr11, vr30;"
        "V1@(pr0)	vr10 = shl.u32 vr4, vr29;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V1@(pr0)	vr30 = sub.s32 vr29, r32;"
        "}"
        "{"
        "V1@(pr0)	vr12 = shl.u32 vr4, vr30;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk0 vr10, r46;"
        "V1@(pr0)	vr11 = sel vmsk0 vr11, vr12;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = eq.s32 vr10, r46;"
        "V1@(pr0)	vmsk1 = eq.s32 vr11, r46;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 r46;"
        "V1@(pr0)	vr17 = sel vmsk0 vr11, r48;"
        "}"
        "{"
        "V0@(pr0)	vr16 = sel vmsk1 vr11, r48;"
        "V1@(pr0)	vr17 = add.s32 vr16, vr17;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vmsk0 = eq.s32 vr17, r32;"
        "V1@(pr0)	vr17 = sel vmsk0 vr11, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V1@(pr0)	vmsk0 = gt.s32 vr28, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 65535;"
        "V0@(pr0)	vr31 = mov.u32 r45;"
        "V1@(pr0)	vr30 = mov.u32 r32;"
        "}"
        "{"
        "V0@(pr0)	vr30 = sub.s32 vr30, vr28;"
        "V1@(pr0)	vr31 = shr.u32 vr31, vr30;"
        "}"
        "{"
        "V0@(pr0)	vr31 = and.u32 vr31, vr5;"
        "V1@(pr0)	vr31 = shl.u32 vr31, vr30;"
        "}"
        "{"
        "V1@(pr0)	vr10 = shr.u32 vr4, vr28;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr10, vr31;"
        "V1@(pr0)	vr11 = shr.u32 vr5, vr28;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr31 = sub.s32 vr28, r32;"
        "V1@(pr0)	vr12 = shr.u32 vr5, vr31;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk0 vr10, vr12;"
        "V1@(pr0)	vr11 = sel vmsk0 vr11, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 64;"
        "V0@(pr0)	vr10 = or.u32 vr10, vr17;"
        "V1@(pr0)	vmsk0 = ls.s32 vr28, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 r46;"
        "V1@(pr0)	vr11 = sel vmsk0 vr17, vr11;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 r48;"
        "V1@(pr0)	vr10 = sel vmsk0 vr17, vr10;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = eq.s32 vr28, r36;"
        "V1@(pr0)	vr4 = sel vmsk0 vr10, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "V0@(pr0)	vr5 = sel vmsk0 vr11, vr5;"
        "V1@(pr0)	vr10 = and.u32 vr2, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr12 = and.u32 vr4, r33;"
        "V1@(pr0)	vr11 = shr.u32 vr2, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr13 = shr.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = gteq.s32 vr10, vr12;"
        "V1@(pr0)	vr28 = mov.u32 r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr17 = sel vmsk0 vr28, r46;"
        "V1@(pr0)	vr17 = shl.u32 vr17, r32;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr10, vr17;"
        "V1@(pr0)	vr14 = sub.s32 vr10, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr15 = add.s32 vr13, r48;"
        "V1@(pr0)	vr13 = sel vmsk0 vr15, vr13;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = gteq.s32 vr11, vr13;"
        "V1@(pr0)	vr17 = sel vmsk0 vr28, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr17 = shl.u32 vr17, r32;"
        "}"
        "{"
        "V0@(pr0)	vr11 = or.u32 vr11, vr17;"
        "V1@(pr0)	vr15 = sub.s32 vr11, vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr14, vr15;"
        "}"
        "{"
        "V0@(pr0)	vr17 = add.s32 vr5, r48;"
        "V1@(pr0)	vr17 = sel vmsk0 vr17, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr11 = sub.s32 vr3, vr17;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = eq.s32 vr11, r46;"
        "V1@(pr0)	vr31 = count.u32 vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr30 = count.u32 vr10;"
        "V1@(pr0)	vr30 = add.s32 vr30, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr31 = sel vmsk0 vr31, vr30;"
        "V1@(pr0)	vr31 = sub.s32 vr31, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vmsk6 = ls.s32 vr31, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr30 = mov.u32 r32;"
        "V1@(pr0)	vr30 = sub.s32 vr30, vr31;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr29 = mov.u32 r44;"
        "V1@(pr0)	vr29 = shl.u32 vr29, vr30;"
        "}"
        "{"
        "V0@(pr0)	vr29 = and.u32 vr29, vr10;"
        "V1@(pr0)	vr29 = shr.u32 vr29, vr30;"
        "}"
        "{"
        "V1@(pr0)	vr13 = shl.u32 vr11, vr31;"
        "}"
        "{"
        "V0@(pr0)	vr13 = or.u32 vr13, vr29;"
        "V1@(pr0)	vr12 = shl.u32 vr10, vr31;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr30 = mov.u32 r32;"
        "V1@(pr0)	vr30 = sub.s32 vr31, vr30;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr14 = shl.u32 vr10, vr30;"
        "}"
        "{"
        "V0@(pr0)	vr12 = sel vmsk6 vr15, vr12;"
        "V1@(pr0)	vr13 = sel vmsk6 vr14, vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 128;"
        "V0@(pr0)	vmsk0 = ls.s32 vr11, r36;"
        "V1@(pr0)	vr10 = sel vmsk0 vr10, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr15 = sub.s32 vr0, vr31;"
        "}"
        "{"
        "V0@(pr0)	vr11 = sel vmsk0 vr11, vr13;"
        "V1@(pr0)	vr15 = sel vmsk0 vr0, vr15;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr12 = and.u32 vr2, r33;"
        "V1@(pr0)	vr13 = shr.u32 vr2, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr16 = and.u32 vr4, r33;"
        "V1@(pr0)	vr17 = shr.u32 vr4, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr28 = add.s32 vr12, vr16;"
        "V1@(pr0)	vr14 = shr.u32 vr28, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "V0@(pr0)	vr28 = and.u32 vr28, r32;"
        "}"
        "{"
        "V0@(pr0)	vr29 = add.s32 vr13, vr17;"
        "V1@(pr0)	vr29 = add.s32 vr29, vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr29, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "V0@(pr0)	vr29 = and.u32 vr29, r32;"
        "V1@(pr0)	vr29 = shl.u32 vr29, r33;"
        "}"
        "{"
        "V0@(pr0)	vr12 = or.u32 vr29, vr28;"
        "V1@(pr0)	vr13 = add.s32 vr3, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr13 = add.s32 vr13, vr14;"
        "V1@(pr0)	vr28 = and.u32 vr12, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V0@(pr0)	vr14 = and.u32 vr13, r48;"
        "V1@(pr0)	vr14 = shl.u32 vr14, r32;"
        "}"
        "{"
        "V1@(pr0)	vr16 = shr.u32 vr12, r48;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr16, vr14;"
        "V1@(pr0)	vr17 = shr.u32 vr13, r48;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr16, vr28;"
        "V1@(pr0)	vr14 = add.s32 vr0, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 256;"
        "V0@(pr0)	vr28 = and.u32 vr13, r36;"
        "V1@(pr0)	vmsk0 = eq.s32 vr28, r46;"
        "}"
        "{"
        "V0@(pr0)	vr12 = sel vmsk0 vr16, vr12;"
        "V1@(pr0)	vr13 = sel vmsk0 vr17, vr13;"
        "}"
        "{"
        "V0@(pr0)	vr14 = sel vmsk0 vr14, vr0;"
        "V1@(pr0)	vmsk1 = eq.s32 vr7, r46;"
        "}"
        "{"
        "V0@(pr0)	vr2 = sel vmsk1 vr10, vr12;"
        "V1@(pr0)	vr3 = sel vmsk1 vr11, vr13;"
        "}"
        "{"
        "V0@(pr0)	vr0 = sel vmsk1 vr15, vr14;"
        "V1@(pr0)	vr31 = mov.u32 r48;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr2, r48;"
        "V1@(pr0)	vmsk0 = lseq.s32 vr0, r46;"
        "}"
        "{"
        "V0@(pr0)	vr2 = sel vmsk0 vr2, vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 7;"
        "pseudo@0	@pseudo imm_1 = 3;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 65535;"
        "V0@(pr0)	vr17 = and.u32 vr2, r32;"
        "V1@(pr0)	vr31 = mov.u32 r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 29;"
        "V1@(pr0)	vr31 = shr.u32 vr31, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 29;"
        "V0@(pr0)	vr31 = and.u32 vr3, vr31;"
        "V1@(pr0)	vr31 = shl.u32 vr31, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3;"
        "V1@(pr0)	vr10 = shr.u32 vr2, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3;"
        "V0@(pr0)	vr10 = or.u32 vr10, vr31;"
        "V1@(pr0)	vr11 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 15;"
        "V0@(pr0)	vr10 = and.u32 vr10, r44;"
        "V1@(pr0)	vr11 = and.u32 vr11, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr28 = shl.u32 vr0, r32;"
        "}"
        "{"
        "V0@(pr0)	vr11 = or.u32 vr11, vr28;"
        "V1@(pr0)	vr11 = or.u32 vr11, vr6;"
        "}"
        "{"
        "V0@(pr0)	vr12 = add.s32 vr10, r48;"
        "V1@(pr0)	vr13 = and.u32 vr10, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "V0@(pr0)	vmsk0 = gt.s32 vr17, r32;"
        "V1@(pr0)	vmsk1 = eq.s32 vr17, r32;"
        "}"
        "{"
        "V0@(pr0)	vr13 = add.s32 vr10, vr13;"
        "V1@(pr0)	vr13 = sel vmsk1 vr10, vr13;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk0 vr13, vr12;"
        "V1@(pr0)	vr0 = mov.u32 vr10;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mov.u32 vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr10 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr11 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr12 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr13 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 5120;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 6144;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr16 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 7168;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr17 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8192;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = eq.s32 vr17, r46;"
        "V1@(pr0)	vmsk1 = eq.s32 vr16, r46;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 r46;"
        "V1@(pr0)	vr7 = sel vmsk0 vr28, r48;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr28, r48;"
        "V1@(pr0)	vr7 = add.s32 vr6, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vmsk0 = eq.s32 vr7, r32;"
        "V1@(pr0)	vr30 = sel vmsk0 vr0, vr10;"
        "}"
        "{"
        "V0@(pr0)	vr31 = sel vmsk0 vr1, vr11;"
        "V1@(pr0)	vr6 = and.u32 vr10, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr7 = and.u32 vr11, vr13;"
        "V1@(pr0)	vr5 = sel vmsk0 vr13, vr7;"
        "}"
        "{"
        "V0@(pr0)	vr4 = sel vmsk0 vr12, vr6;"
        "V1@(pr0)	vmsk0 = eq.s32 vr14, r46;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = eq.s32 vr15, r46;"
        "V1@(pr0)	vr6 = sel vmsk0 vr28, r48;"
        "}"
        "{"
        "V0@(pr0)	vr7 = sel vmsk1 vr28, r48;"
        "V1@(pr0)	vr7 = add.s32 vr6, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vmsk0 = eq.s32 vr7, r32;"
        "V1@(pr0)	vr31 = sel vmsk0 vr31, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr30 = sel vmsk0 vr30, r34;"
        "V1@(pr0)	vr7 = mov.u32 r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 15;"
        "V0@(pr0)	vr6 = mov.u32 r44;"
        "V1@(pr0)	vr7 = xor.u32 vr7, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr6 = xor.u32 vr6, r44;"
        "V1@(pr0)	vmsk0 = eq.s32 vr16, vr6;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mov.u32 vr7;"
        "V1@(pr0)	vr4 = mov.u32 vr6;"
        "}"
        "{"
        "V0@(pr0)	vmsk6 = eq.s32 vr14, vr6;"
        "V1@(pr0)	vmsk7 = eq.s32 vr15, vr7;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = eq.s32 vr17, vr7;"
        "V1@(pr0)	vr7 = sel vmsk0 vr28, r48;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr28, r48;"
        "V1@(pr0)	vr7 = add.s32 vr6, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vmsk0 = eq.s32 vr7, r32;"
        "V1@(pr0)	vr31 = sel vmsk0 vr31, vr13;"
        "}"
        "{"
        "V0@(pr0)	vr30 = sel vmsk0 vr30, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr7 = sel vmsk7 vr28, r48;"
        "V1@(pr0)	vr6 = sel vmsk6 vr28, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vr7 = add.s32 vr6, vr7;"
        "V1@(pr0)	vmsk7 = eq.s32 vr7, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr3 = or.u32 vr5, r36;"
        "V1@(pr0)	vr6 = xor.u32 vr10, vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32768;"
        "V0@(pr0)	vr7 = xor.u32 vr11, vr13;"
        "V1@(pr0)	vmsk0 = eq.s32 vr7, r36;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = eq.s32 vr6, r46;"
        "V1@(pr0)	vr7 = sel vmsk0 vr28, r48;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr28, r48;"
        "V1@(pr0)	vr7 = add.s32 vr6, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vmsk0 = eq.s32 vr7, r32;"
        "V1@(pr0)	vr7 = sel vmsk0 vr11, vr3;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk0 vr10, vr4;"
        "V1@(pr0)	vr30 = sel vmsk7 vr30, vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr31 = sel vmsk7 vr31, vr7;"
        "V1@(pr0)	vr7 = or.u32 vr13, r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk0 = eq.s32 vr16, r46;"
        "V1@(pr0)	vr6 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "V0@(pr0)	vr28 = and.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32752;"
        "V0@(pr0)	vmsk1 = gt.s32 vr6, r32;"
        "V1@(pr0)	vmsk2 = eq.s32 vr28, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk0 vr28, r48;"
        "V1@(pr0)	vr4 = sel vmsk2 vr28, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vr5 = add.s32 vr4, vr5;"
        "V1@(pr0)	vmsk2 = eq.s32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr7, vr31;"
        "V1@(pr0)	vr4 = sel vmsk2 vr12, vr30;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32752;"
        "V0@(pr0)	vmsk2 = eq.s32 vr6, r32;"
        "V1@(pr0)	vr4 = sel vmsk2 vr30, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr31, vr5;"
        "V1@(pr0)	vr31 = sel vmsk1 vr5, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr30 = sel vmsk1 vr4, vr12;"
        "V1@(pr0)	vr7 = or.u32 vr11, r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk0 = eq.s32 vr14, r46;"
        "V1@(pr0)	vr6 = shr.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "V0@(pr0)	vr28 = and.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32752;"
        "V0@(pr0)	vmsk1 = gt.s32 vr6, r32;"
        "V1@(pr0)	vmsk2 = eq.s32 vr28, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk0 vr28, r48;"
        "V1@(pr0)	vr4 = sel vmsk2 vr28, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vr5 = add.s32 vr4, vr5;"
        "V1@(pr0)	vmsk2 = eq.s32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr7, vr31;"
        "V1@(pr0)	vr4 = sel vmsk2 vr10, vr30;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32752;"
        "V0@(pr0)	vmsk2 = eq.s32 vr6, r32;"
        "V1@(pr0)	vr4 = sel vmsk2 vr30, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr31, vr5;"
        "V1@(pr0)	vr11 = sel vmsk1 vr5, vr7;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr4, vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 49152;"
        "pseudo@0	@pseudo imm_2 = 3;"
        "V0@(pr0)	vr5 = and.u32 vr11, r44;"
        "V1@(pr0)	vr1 = shl.u32 vr11, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 16383;"
        "V0@(pr0)	vr1 = and.u32 vr1, r44;"
        "V1@(pr0)	vr5 = or.u32 vr1, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 29;"
        "V1@(pr0)	vr1 = shr.u32 vr10, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = or.u32 vr1, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr4 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr13 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr4;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr5;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr10 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3;"
        "V0@(pr0)	vr15 = and.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr4 = mul.f32 vr11, vr11;"
        "V1@(pr0)	vr7 = and.u32 vr11, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 39322;"
        "pseudo@0	@pseudo imm_1 = 16025;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vmsk1 = ls.s32 vr7, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 55118;"
        "pseudo@0	@pseudo imm_1 = 44359;"
        "pseudo@0	@pseudo imm_2 = 29942;"
        "pseudo@0	@pseudo imm_3 = 12559;"
        "V0@(pr0)	vr5 = mul.f32 vr4, r44;"
        "V1@(pr0)	vr5 = add.f32 vr5, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 62076;"
        "pseudo@0	@pseudo imm_1 = 46227;"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr5 = add.f32 vr5, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3329;"
        "pseudo@0	@pseudo imm_1 = 14288;"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr5 = add.f32 vr5, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2913;"
        "pseudo@0	@pseudo imm_1 = 47798;"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr5 = add.f32 vr5, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 43691;"
        "pseudo@0	@pseudo imm_1 = 15658;"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr5 = add.f32 vr5, r44;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr11, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr2 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr1 = sub.f32 vr2, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr2 = mul.f32 vr4, r50;"
        "V1@(pr0)	vr1 = sub.f32 vr2, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr2 = mov.u32 r49;"
        "V1@(pr0)	vr1 = sub.f32 vr2, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr0, vr1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16200;"
        "pseudo@0	@pseudo imm_1 = 256;"
        "V0@(pr0)	vmsk2 = gt.s32 vr7, r36;"
        "V1@(pr0)	vr1 = sub.s32 vr7, r37;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16016;"
        "V0@(pr0)	vr2 = mul.f32 vr4, r50;"
        "V1@(pr0)	vr1 = sel vmsk2 vr1, r44;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mov.u32 r49;"
        "V1@(pr0)	vr2 = sub.f32 vr2, vr1;"
        "}"
        "{"
        "V1@(pr0)	vr1 = sub.f32 vr3, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr11, vr12;"
        "V1@(pr0)	vr3 = add.f32 vr3, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr2 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr3 = sub.f32 vr3, vr2;"
        "}"
        "{"
        "V1@(pr0)	vr1 = sub.f32 vr1, vr3;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 12800;"
        "V0@(pr0)	vr6 = sel vmsk1 vr1, vr6;"
        "V1@(pr0)	vmsk1 = ls.s32 vr7, r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr2 = cvtftoint.s32 vr11, r44;"
        "}"
        "{"
        "V0@(pr0)	vr1 = sel vmsk1 vr6, r49;"
        "V1@(pr0)	vmsk1 = eq.s32 vr2, r46;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr6, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 vr6;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mul.f32 vr11, r57;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = eq.s32 vr15, r48;"
        "V1@(pr0)	vr14 = sel vmsk1 vr17, vr11;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = eq.s32 vr15, r46;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vmsk1 = eq.s32 vr15, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, vr17;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr0 = and.u32 vr11, r44;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr11, vr11;"
        "V1@(pr0)	vr7 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr4 = mul.f32 vr3, vr11;"
        "V1@(pr0)	vmsk1 = eq.s32 vr13, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 51667;"
        "pseudo@0	@pseudo imm_1 = 12078;"
        "pseudo@0	@pseudo imm_2 = 12084;"
        "pseudo@0	@pseudo imm_3 = 45783;"
        "V0@(pr0)	vr1 = mul.f32 vr3, r44;"
        "V1@(pr0)	vr1 = add.f32 vr1, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 61211;"
        "pseudo@0	@pseudo imm_1 = 13880;"
        "V0@(pr0)	vr1 = mul.f32 vr3, vr1;"
        "V1@(pr0)	vr1 = add.f32 vr1, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3329;"
        "pseudo@0	@pseudo imm_1 = 47440;"
        "V0@(pr0)	vr1 = mul.f32 vr3, vr1;"
        "V1@(pr0)	vr1 = add.f32 vr1, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 34953;"
        "pseudo@0	@pseudo imm_1 = 15368;"
        "V0@(pr0)	vr1 = mul.f32 vr3, vr1;"
        "V1@(pr0)	vr5 = add.f32 vr1, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 43691;"
        "pseudo@0	@pseudo imm_1 = 48682;"
        "V0@(pr0)	vr1 = mul.f32 vr3, vr5;"
        "V1@(pr0)	vr1 = add.f32 vr1, r44;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.f32 vr11, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr2 = mul.f32 vr12, r50;"
        "V1@(pr0)	vr6 = sel vmsk1 vr6, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr1 = sub.f32 vr2, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr3, vr1;"
        "V1@(pr0)	vr1 = sub.f32 vr1, vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 43691;"
        "pseudo@0	@pseudo imm_1 = 48682;"
        "V0@(pr0)	vr2 = mul.f32 vr4, r44;"
        "V1@(pr0)	vr1 = sub.f32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr2 = cvtftoint.s32 vr11, r44;"
        "V1@(pr0)	vr1 = sub.f32 vr11, vr1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 12800;"
        "V0@(pr0)	vr6 = sel vmsk1 vr1, vr6;"
        "V1@(pr0)	vmsk1 = ls.s32 vr0, r36;"
        "}"
        "{"
        "V0@(pr0)	vr1 = sel vmsk1 vr6, vr11;"
        "V1@(pr0)	vmsk1 = eq.s32 vr2, r46;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr6, vr1;"
        "V1@(pr0)	vr11 = mov.u32 vr6;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mul.f32 vr11, r57;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = eq.s32 vr15, r46;"
        "V1@(pr0)	vr14 = sel vmsk1 vr14, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vmsk1 = eq.s32 vr15, r32;"
        "V1@(pr0)	vr14 = sel vmsk1 vr14, vr17;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = eq.s32 vr15, r48;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, vr17;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3;"
        "V0@(pr0)	vmsk1 = eq.s32 vr15, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, vr11;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mul.f32 vr14, r57;"
        "V1@(pr0)	vmsk1 = ls.f32 vr10, r46;"
        "}"
        "{"
        "V0@(pr0)	vr14 = sel vmsk1 vr14, vr17;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 vr10;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 r46;"
        "V1@(pr0)	vr13 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr0 = and.u32 vr11, r44;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr11, vr11;"
        "V1@(pr0)	vr7 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr4 = mul.f32 vr3, vr11;"
        "V1@(pr0)	vmsk1 = eq.s32 vr13, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 51667;"
        "pseudo@0	@pseudo imm_1 = 12078;"
        "pseudo@0	@pseudo imm_2 = 12084;"
        "pseudo@0	@pseudo imm_3 = 45783;"
        "V0@(pr0)	vr1 = mul.f32 vr3, r44;"
        "V1@(pr0)	vr1 = add.f32 vr1, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 61211;"
        "pseudo@0	@pseudo imm_1 = 13880;"
        "V0@(pr0)	vr1 = mul.f32 vr3, vr1;"
        "V1@(pr0)	vr1 = add.f32 vr1, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3329;"
        "pseudo@0	@pseudo imm_1 = 47440;"
        "V0@(pr0)	vr1 = mul.f32 vr3, vr1;"
        "V1@(pr0)	vr1 = add.f32 vr1, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 34953;"
        "pseudo@0	@pseudo imm_1 = 15368;"
        "V0@(pr0)	vr1 = mul.f32 vr3, vr1;"
        "V1@(pr0)	vr5 = add.f32 vr1, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 43691;"
        "pseudo@0	@pseudo imm_1 = 48682;"
        "V0@(pr0)	vr1 = mul.f32 vr3, vr5;"
        "V1@(pr0)	vr1 = add.f32 vr1, r44;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.f32 vr11, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr2 = mul.f32 vr12, r50;"
        "V1@(pr0)	vr6 = sel vmsk1 vr6, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr1 = sub.f32 vr2, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr3, vr1;"
        "V1@(pr0)	vr1 = sub.f32 vr1, vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 43691;"
        "pseudo@0	@pseudo imm_1 = 48682;"
        "V0@(pr0)	vr2 = mul.f32 vr4, r44;"
        "V1@(pr0)	vr1 = sub.f32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr2 = cvtftoint.s32 vr11, r44;"
        "V1@(pr0)	vr1 = sub.f32 vr11, vr1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 12800;"
        "V0@(pr0)	vr6 = sel vmsk1 vr1, vr6;"
        "V1@(pr0)	vmsk1 = ls.s32 vr0, r36;"
        "}"
        "{"
        "V0@(pr0)	vr1 = sel vmsk1 vr6, vr11;"
        "V1@(pr0)	vmsk1 = eq.s32 vr2, r46;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr6, vr1;"
        "V1@(pr0)	vr11 = mov.u32 vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr1 = and.u32 vr10, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4056;"
        "pseudo@0	@pseudo imm_1 = 16201;"
        "V0@(pr0)	vmsk1 = ls.f32 vr1, r44;"
        "}"
        "{"
        "V1@(pr0)	vr14 = sel vmsk1 vr14, vr11;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 vr10;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 r46;"
        "V1@(pr0)	vr13 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr4 = mul.f32 vr11, vr11;"
        "V1@(pr0)	vr7 = and.u32 vr11, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 39322;"
        "pseudo@0	@pseudo imm_1 = 16025;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vmsk1 = ls.s32 vr7, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 55118;"
        "pseudo@0	@pseudo imm_1 = 44359;"
        "pseudo@0	@pseudo imm_2 = 29942;"
        "pseudo@0	@pseudo imm_3 = 12559;"
        "V0@(pr0)	vr5 = mul.f32 vr4, r44;"
        "V1@(pr0)	vr5 = add.f32 vr5, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 62076;"
        "pseudo@0	@pseudo imm_1 = 46227;"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr5 = add.f32 vr5, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3329;"
        "pseudo@0	@pseudo imm_1 = 14288;"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr5 = add.f32 vr5, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2913;"
        "pseudo@0	@pseudo imm_1 = 47798;"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr5 = add.f32 vr5, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 43691;"
        "pseudo@0	@pseudo imm_1 = 15658;"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr5 = add.f32 vr5, r44;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr11, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr2 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr1 = sub.f32 vr2, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr2 = mul.f32 vr4, r50;"
        "V1@(pr0)	vr1 = sub.f32 vr2, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr2 = mov.u32 r49;"
        "V1@(pr0)	vr1 = sub.f32 vr2, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr0, vr1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16200;"
        "pseudo@0	@pseudo imm_1 = 256;"
        "V0@(pr0)	vmsk2 = gt.s32 vr7, r36;"
        "V1@(pr0)	vr1 = sub.s32 vr7, r37;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16016;"
        "V0@(pr0)	vr2 = mul.f32 vr4, r50;"
        "V1@(pr0)	vr1 = sel vmsk2 vr1, r44;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mov.u32 r49;"
        "V1@(pr0)	vr2 = sub.f32 vr2, vr1;"
        "}"
        "{"
        "V1@(pr0)	vr1 = sub.f32 vr3, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr11, vr12;"
        "V1@(pr0)	vr3 = add.f32 vr3, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr2 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr3 = sub.f32 vr3, vr2;"
        "}"
        "{"
        "V1@(pr0)	vr1 = sub.f32 vr1, vr3;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 12800;"
        "V0@(pr0)	vr6 = sel vmsk1 vr1, vr6;"
        "V1@(pr0)	vmsk1 = ls.s32 vr7, r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr2 = cvtftoint.s32 vr11, r44;"
        "}"
        "{"
        "V0@(pr0)	vr1 = sel vmsk1 vr6, r49;"
        "V1@(pr0)	vmsk1 = eq.s32 vr2, r46;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr6, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr1 = and.u32 vr10, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4056;"
        "pseudo@0	@pseudo imm_1 = 16201;"
        "V0@(pr0)	vmsk1 = ls.f32 vr1, r44;"
        "}"
        "{"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr1 = and.u32 vr10, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32640;"
        "V0@(pr0)	vmsk1 = ls.f32 vr1, r44;"
        "V1@(pr0)	vr15 = sub.f32 vr10, vr10;"
        "}"
        "{"
        "V0@(pr0)	vr14 = sel vmsk1 vr15, vr14;"
        "V1@(pr0)	vr28 = sel vmsk1 vr15, vr28;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr14;"
        "V1@(pr0)	vr11 = mov.u32 vr28;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "V0@(pr0)	vr12 = mov.u32 vr10;"
        "V1@(pr0)	vr13 = mov.u32 vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r1 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VL@(pr0)	vr11 = ld [vmem:r0,$1,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VL@(pr0)	vr10 = ld [vmem:r0,$1,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r1 = add.s32 r1, r44;"
        "S1@(pr0)	[smem:r3] = st r1;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mul.f32 vr10, r51;"
        "}"
        "{"
        "S1@(pr0)	r1 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r1 = sub.s32 r1, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VS@(pr0)	 [vmem:r0,$1,$255] = st vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VS@(pr0)	 [vmem:r0,$1,$255] = st vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VS@(pr0)	 [vmem:r0,$1,$255] = st vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VS@(pr0)	 [vmem:r0,$1,$255] = st vr13;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r1;"
        "V0@(pr0)	vr10 = mov.u32 vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr10;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr1 = and.u32 vr10, r44;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 17136;"
        "V1@(pr0)	vmsk4 = ls.f32 vr2, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr10;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 29;"
        "V1@(pr0)	vr11 = shl.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 49152;"
        "pseudo@0	@pseudo imm_2 = 3;"
        "V0@(pr0)	vr1 = and.u32 vr10, r44;"
        "V1@(pr0)	vr2 = shr.u32 vr10, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 2047;"
        "V0@(pr0)	vr2 = and.u32 vr2, r44;"
        "V1@(pr0)	vr1 = or.u32 vr2, vr1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 2048;"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 14336;"
        "V0@(pr0)	vr2 = and.u32 vr10, r44;"
        "V1@(pr0)	vr3 = or.u32 vr1, r45;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = eq.s32 vr2, r46;"
        "V1@(pr0)	vr1 = sel vmsk1 vr3, vr1;"
        "}"
        "{"
        "V1@(pr0)	vr10 = mov.u32 vr1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr11;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 24368;"
        "pseudo@0	@pseudo imm_1 = 16740;"
        "pseudo@0	@pseudo imm_2 = 51331;"
        "pseudo@0	@pseudo imm_3 = 28105;"
        "V0@(pr0)	vr12 = mov.u32 r44;"
        "V1@(pr0)	vr13 = mov.u32 r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr13;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr1 = and.u32 vr10, r44;"
        "V1@(pr0)	vr2 = and.u32 vr12, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "V0@(pr0)	vmsk1 = ls.f32 vr1, r44;"
        "V1@(pr0)	vmsk2 = ls.f32 vr2, r44;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, r46;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, r46;"
        "}"
        "{"
        "V0@(pr0)	vr12 = sel vmsk2 vr12, r46;"
        "V1@(pr0)	vr13 = sel vmsk2 vr13, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr6 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr7 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 15;"
        "V0@(pr0)	vr10 = and.u32 vr10, r44;"
        "V1@(pr0)	vr12 = and.u32 vr12, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "V0@(pr0)	vr10 = or.u32 vr10, r44;"
        "V1@(pr0)	vr12 = or.u32 vr12, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 11;"
        "V1@(pr0)	vr12 = shl.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 21;"
        "V1@(pr0)	vr1 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 11;"
        "V0@(pr0)	vr12 = or.u32 vr12, vr1;"
        "V1@(pr0)	vr13 = shl.u32 vr13, r32;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr13, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = and.u32 vr12, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr13, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr12, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = and.u32 vr12, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr12, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 r46;"
        "V1@(pr0)	vr10 = mov.u32 vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr10;"
        "V1@(pr0)	vr17 = mov.u32 vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = or.u32 vr1, vr2;"
        "V1@(pr0)	vr14 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr12 = mov.u32 r46;"
        "V1@(pr0)	vr13 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "V1@(pr0)	vr15 = shr.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 vr15;"
        "V1@(pr0)	vr29 = mov.u32 vr14;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr15;"
        "V1@(pr0)	vr15 = mov.u32 vr0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr16;"
        "V1@(pr0)	vr11 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr12;"
        "V1@(pr0)	vr17 = mov.u32 vr13;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr29;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 vr15;"
        "V1@(pr0)	vr29 = mov.u32 vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr16;"
        "V1@(pr0)	vr13 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr10;"
        "V1@(pr0)	vr17 = mov.u32 vr11;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr28;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr16;"
        "V1@(pr0)	vr11 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr29 = shl.u32 vr29, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr12;"
        "V1@(pr0)	vr17 = mov.u32 vr13;"
        "}"
        "{"
        "V0@(pr0)	vr30 = mov.u32 vr14;"
        "V1@(pr0)	vr14 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr31 = mov.u32 vr17;"
        "V1@(pr0)	vr17 = mov.u32 vr16;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr11;"
        "V1@(pr0)	vr15 = mov.u32 vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr16;"
        "V1@(pr0)	vr11 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr13;"
        "V1@(pr0)	vr13 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr28;"
        "V1@(pr0)	vr17 = mov.u32 vr29;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr30;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr13;"
        "V1@(pr0)	vr15 = mov.u32 vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr13 = mov.u32 vr17;"
        "V1@(pr0)	vr12 = mov.u32 vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "V0@(pr0)	vr3 = xor.u32 vr6, vr7;"
        "V1@(pr0)	vr3 = and.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr3 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 2047;"
        "pseudo@0	@pseudo imm_3 = 0;"
        "V0@(pr0)	vr7 = and.u32 vr7, r45;"
        "V1@(pr0)	vr6 = and.u32 vr6, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 1023;"
        "pseudo@0	@pseudo imm_3 = 0;"
        "V0@(pr0)	vr6 = add.s32 vr6, vr7;"
        "V1@(pr0)	vr6 = sub.s32 vr6, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 16;"
        "V0@(pr0)	vr1 = and.u32 vr10, r45;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr6, r48;"
        "V1@(pr0)	vmsk1 = eq.s32 vr1, r46;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr2, vr6;"
        "}"
        "{"
        "V1@(pr0)	vr2 = shl.u32 vr10, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V1@(pr0)	vr5 = shr.u32 vr11, r32;"
        "}"
        "{"
        "V0@(pr0)	vr2 = or.u32 vr2, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V1@(pr0)	vr5 = shl.u32 vr13, r32;"
        "}"
        "{"
        "V1@(pr0)	vr4 = shl.u32 vr11, r48;"
        "}"
        "{"
        "V0@(pr0)	vr5 = or.u32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr2;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr5;"
        "}"
        "{"
        "V1@(pr0)	vr2 = shl.u32 vr12, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V1@(pr0)	vr5 = shr.u32 vr13, r32;"
        "}"
        "{"
        "V0@(pr0)	vr2 = or.u32 vr2, vr5;"
        "}"
        "{"
        "V1@(pr0)	vr5 = shl.u32 vr13, r48;"
        "}"
        "{"
        "V0@(pr0)	vr12 = sel vmsk1 vr12, vr2;"
        "V1@(pr0)	vr13 = sel vmsk1 vr13, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 15;"
        "V0@(pr0)	vr1 = and.u32 vr10, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr2 = shl.u32 vr6, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = gt.s32 vr6, r46;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr10, vr3;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32768;"
        "V0@(pr0)	vr1 = add.s32 vr11, r48;"
        "V1@(pr0)	vmsk1 = gt.s32 vr12, r44;"
        "}"
        "{"
        "V0@(pr0)	vr11 = sel vmsk1 vr11, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr1 = and.u32 vr11, r48;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr11;"
        "V1@(pr0)	vr15 = mov.u32 vr10;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr1;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32768;"
        "V1@(pr0)	vmsk1 = eq.s32 vr12, r45;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr16;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr17;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr28 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr29 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr30 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr31 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr1 = and.u32 vr28, r44;"
        "V1@(pr0)	vr2 = and.u32 vr30, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = lseq.s32 vr1, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 8;"
        "V1@(pr0)	vr1 = or.u32 vr28, r45;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr1, vr10;"
        "V1@(pr0)	vr11 = sel vmsk1 vr29, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = lseq.s32 vr2, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 8;"
        "V1@(pr0)	vr1 = or.u32 vr30, r45;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr1, vr10;"
        "V1@(pr0)	vr11 = sel vmsk1 vr31, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32768;"
        "V0@(pr0)	vr3 = xor.u32 vr28, vr30;"
        "V1@(pr0)	vr3 = and.u32 vr3, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = eq.f32 vr1, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32760;"
        "V0@(pr0)	vr5 = mov.u32 r44;"
        "}"
        "{"
        "V0@(pr0)	vr4 = xor.u32 vr1, vr3;"
        "}"
        "{"
        "V1@(pr0)	vmsk2 = eq.s32 vr2, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr5;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = eq.f32 vr2, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32760;"
        "V0@(pr0)	vr5 = mov.u32 r44;"
        "}"
        "{"
        "V0@(pr0)	vr4 = xor.u32 vr2, vr3;"
        "}"
        "{"
        "V1@(pr0)	vmsk2 = eq.s32 vr1, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr5;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr11;"
        "}"
        "{"
        "V1@(pr0)	vmsk1 = eq.f32 vr1, r46;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr3;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, r46;"
        "}"
        "{"
        "V1@(pr0)	vmsk1 = eq.f32 vr2, r46;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr3;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 49152;"
        "pseudo@0	@pseudo imm_2 = 3;"
        "V0@(pr0)	vr13 = and.u32 vr10, r44;"
        "V1@(pr0)	vr1 = shl.u32 vr10, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 16383;"
        "V0@(pr0)	vr1 = and.u32 vr1, r44;"
        "V1@(pr0)	vr13 = or.u32 vr1, vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 29;"
        "V1@(pr0)	vr1 = shr.u32 vr11, r32;"
        "}"
        "{"
        "V0@(pr0)	vr13 = or.u32 vr1, vr13;"
        "}"
        "{"
        "V0@(pr0)	vr13 = cvtftoint.s32 vr13, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 128;"
        "V0@(pr0)	vr13 = add.s32 vr13, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 24;"
        "V1@(pr0)	vr13 = shra.s32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr13;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr1 = cvtinttof.f32 vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 29;"
        "V1@(pr0)	vr11 = shl.u32 vr1, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 49152;"
        "pseudo@0	@pseudo imm_2 = 3;"
        "V0@(pr0)	vr10 = and.u32 vr1, r44;"
        "V1@(pr0)	vr2 = shr.u32 vr1, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 2047;"
        "V0@(pr0)	vr2 = and.u32 vr2, r44;"
        "V1@(pr0)	vr10 = or.u32 vr2, vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 2048;"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 14336;"
        "V0@(pr0)	vr2 = and.u32 vr1, r44;"
        "V1@(pr0)	vr3 = or.u32 vr10, r45;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = eq.s32 vr2, r46;"
        "V1@(pr0)	vr10 = sel vmsk1 vr3, vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8699;"
        "pseudo@0	@pseudo imm_1 = 16377;"
        "pseudo@0	@pseudo imm_2 = 11544;"
        "pseudo@0	@pseudo imm_3 = 21572;"
        "V0@(pr0)	vr12 = mov.u32 r44;"
        "V1@(pr0)	vr13 = mov.u32 r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr13;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr1 = and.u32 vr10, r44;"
        "V1@(pr0)	vr2 = and.u32 vr12, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "V0@(pr0)	vmsk1 = ls.f32 vr1, r44;"
        "V1@(pr0)	vmsk2 = ls.f32 vr2, r44;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, r46;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, r46;"
        "}"
        "{"
        "V0@(pr0)	vr12 = sel vmsk2 vr12, r46;"
        "V1@(pr0)	vr13 = sel vmsk2 vr13, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr6 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr7 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 15;"
        "V0@(pr0)	vr10 = and.u32 vr10, r44;"
        "V1@(pr0)	vr12 = and.u32 vr12, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "V0@(pr0)	vr10 = or.u32 vr10, r44;"
        "V1@(pr0)	vr12 = or.u32 vr12, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 11;"
        "V1@(pr0)	vr12 = shl.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 21;"
        "V1@(pr0)	vr1 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 11;"
        "V0@(pr0)	vr12 = or.u32 vr12, vr1;"
        "V1@(pr0)	vr13 = shl.u32 vr13, r32;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr13, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = and.u32 vr12, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr13, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr12, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = and.u32 vr12, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr12, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 r46;"
        "V1@(pr0)	vr10 = mov.u32 vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr10;"
        "V1@(pr0)	vr17 = mov.u32 vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = or.u32 vr1, vr2;"
        "V1@(pr0)	vr14 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr12 = mov.u32 r46;"
        "V1@(pr0)	vr13 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "V1@(pr0)	vr15 = shr.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 vr15;"
        "V1@(pr0)	vr29 = mov.u32 vr14;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr15;"
        "V1@(pr0)	vr15 = mov.u32 vr0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr16;"
        "V1@(pr0)	vr11 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr12;"
        "V1@(pr0)	vr17 = mov.u32 vr13;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr29;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 vr15;"
        "V1@(pr0)	vr29 = mov.u32 vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr16;"
        "V1@(pr0)	vr13 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr10;"
        "V1@(pr0)	vr17 = mov.u32 vr11;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr28;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr16;"
        "V1@(pr0)	vr11 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr29 = shl.u32 vr29, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr12;"
        "V1@(pr0)	vr17 = mov.u32 vr13;"
        "}"
        "{"
        "V0@(pr0)	vr30 = mov.u32 vr14;"
        "V1@(pr0)	vr14 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr31 = mov.u32 vr17;"
        "V1@(pr0)	vr17 = mov.u32 vr16;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr11;"
        "V1@(pr0)	vr15 = mov.u32 vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr16;"
        "V1@(pr0)	vr11 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr13;"
        "V1@(pr0)	vr13 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr28;"
        "V1@(pr0)	vr17 = mov.u32 vr29;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr30;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr13;"
        "V1@(pr0)	vr15 = mov.u32 vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr13 = mov.u32 vr17;"
        "V1@(pr0)	vr12 = mov.u32 vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "V0@(pr0)	vr3 = xor.u32 vr6, vr7;"
        "V1@(pr0)	vr3 = and.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr3 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 2047;"
        "pseudo@0	@pseudo imm_3 = 0;"
        "V0@(pr0)	vr7 = and.u32 vr7, r45;"
        "V1@(pr0)	vr6 = and.u32 vr6, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 1023;"
        "pseudo@0	@pseudo imm_3 = 0;"
        "V0@(pr0)	vr6 = add.s32 vr6, vr7;"
        "V1@(pr0)	vr6 = sub.s32 vr6, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 16;"
        "V0@(pr0)	vr1 = and.u32 vr10, r45;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr6, r48;"
        "V1@(pr0)	vmsk1 = eq.s32 vr1, r46;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr2, vr6;"
        "}"
        "{"
        "V1@(pr0)	vr2 = shl.u32 vr10, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V1@(pr0)	vr5 = shr.u32 vr11, r32;"
        "}"
        "{"
        "V0@(pr0)	vr2 = or.u32 vr2, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V1@(pr0)	vr5 = shl.u32 vr13, r32;"
        "}"
        "{"
        "V1@(pr0)	vr4 = shl.u32 vr11, r48;"
        "}"
        "{"
        "V0@(pr0)	vr5 = or.u32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr2;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr5;"
        "}"
        "{"
        "V1@(pr0)	vr2 = shl.u32 vr12, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V1@(pr0)	vr5 = shr.u32 vr13, r32;"
        "}"
        "{"
        "V0@(pr0)	vr2 = or.u32 vr2, vr5;"
        "}"
        "{"
        "V1@(pr0)	vr5 = shl.u32 vr13, r48;"
        "}"
        "{"
        "V0@(pr0)	vr12 = sel vmsk1 vr12, vr2;"
        "V1@(pr0)	vr13 = sel vmsk1 vr13, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 15;"
        "V0@(pr0)	vr1 = and.u32 vr10, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr2 = shl.u32 vr6, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = gt.s32 vr6, r46;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr10, vr3;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32768;"
        "V0@(pr0)	vr1 = add.s32 vr11, r48;"
        "V1@(pr0)	vmsk1 = gt.s32 vr12, r44;"
        "}"
        "{"
        "V0@(pr0)	vr11 = sel vmsk1 vr11, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr1 = and.u32 vr11, r48;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr11;"
        "V1@(pr0)	vr15 = mov.u32 vr10;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr1;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32768;"
        "V1@(pr0)	vmsk1 = eq.s32 vr12, r45;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr16;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr17;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr28 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr29 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr30 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr31 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr1 = and.u32 vr28, r44;"
        "V1@(pr0)	vr2 = and.u32 vr30, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = lseq.s32 vr1, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 8;"
        "V1@(pr0)	vr1 = or.u32 vr28, r45;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr1, vr10;"
        "V1@(pr0)	vr11 = sel vmsk1 vr29, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = lseq.s32 vr2, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 8;"
        "V1@(pr0)	vr1 = or.u32 vr30, r45;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr1, vr10;"
        "V1@(pr0)	vr11 = sel vmsk1 vr31, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32768;"
        "V0@(pr0)	vr3 = xor.u32 vr28, vr30;"
        "V1@(pr0)	vr3 = and.u32 vr3, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = eq.f32 vr1, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32760;"
        "V0@(pr0)	vr5 = mov.u32 r44;"
        "}"
        "{"
        "V0@(pr0)	vr4 = xor.u32 vr1, vr3;"
        "}"
        "{"
        "V1@(pr0)	vmsk2 = eq.s32 vr2, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr5;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = eq.f32 vr2, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32760;"
        "V0@(pr0)	vr5 = mov.u32 r44;"
        "}"
        "{"
        "V0@(pr0)	vr4 = xor.u32 vr2, vr3;"
        "}"
        "{"
        "V1@(pr0)	vmsk2 = eq.s32 vr1, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr5;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr11;"
        "}"
        "{"
        "V1@(pr0)	vmsk1 = eq.f32 vr1, r46;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr3;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, r46;"
        "}"
        "{"
        "V1@(pr0)	vmsk1 = eq.f32 vr2, r46;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr3;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32768;"
        "V0@(pr0)	vr12 = mov.u32 vr11;"
        "V1@(pr0)	vr13 = xor.u32 vr10, r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr1 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr11 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr10 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr1;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 32767;"
        "V0@(pr0)	vr14 = and.u32 vr10, r44;"
        "V1@(pr0)	vr15 = and.u32 vr11, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 32767;"
        "V0@(pr0)	vr16 = and.u32 vr12, r44;"
        "V1@(pr0)	vr17 = and.u32 vr13, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8192;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 5120;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr15;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 6144;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 7168;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = gt.s32 vr17, vr15;"
        "V1@(pr0)	vmsk1 = eq.s32 vr17, vr15;"
        "}"
        "{"
        "V0@(pr0)	vmsk2 = gt.s32 vr16, vr14;"
        "}"
        "{"
        "V0@(pr0)	vr1 = sel vmsk2 vr13, vr11;"
        "V1@(pr0)	vr0 = sel vmsk2 vr12, vr10;"
        "}"
        "{"
        "V0@(pr0)	vr1 = sel vmsk1 vr13, vr1;"
        "V1@(pr0)	vr0 = sel vmsk1 vr12, vr0;"
        "}"
        "{"
        "V0@(pr0)	vr1 = sel vmsk0 vr1, vr11;"
        "V1@(pr0)	vr0 = sel vmsk0 vr0, vr10;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = eq.s32 vr1, vr11;"
        "V1@(pr0)	vmsk1 = eq.s32 vr0, vr10;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr7 = sel vmsk0 vr5, r48;"
        "V1@(pr0)	vr6 = sel vmsk1 vr5, r48;"
        "}"
        "{"
        "V0@(pr0)	vr7 = and.u32 vr6, vr7;"
        "V1@(pr0)	vmsk0 = eq.s32 vr7, r48;"
        "}"
        "{"
        "V0@(pr0)	vr15 = sel vmsk0 vr11, vr13;"
        "V1@(pr0)	vr14 = sel vmsk0 vr10, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr1;"
        "V1@(pr0)	vr16 = mov.u32 vr0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2047;"
        "pseudo@0	@pseudo imm_1 = 20;"
        "V0@(pr0)	vr7 = mov.u32 r32;"
        "V1@(pr0)	vr6 = mov.u32 r33;"
        "}"
        "{"
        "V1@(pr0)	vr0 = shr.u32 vr15, vr6;"
        "}"
        "{"
        "V0@(pr0)	vr0 = and.u32 vr0, vr7;"
        "}"
        "{"
        "V1@(pr0)	vr1 = shr.u32 vr17, vr6;"
        "}"
        "{"
        "V0@(pr0)	vr1 = and.u32 vr1, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 15;"
        "V0@(pr0)	vr2 = and.u32 vr14, r44;"
        "V1@(pr0)	vr3 = and.u32 vr15, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 15;"
        "V0@(pr0)	vr4 = and.u32 vr16, r44;"
        "V1@(pr0)	vr5 = and.u32 vr17, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32768;"
        "V0@(pr0)	vr6 = and.u32 vr15, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32768;"
        "V0@(pr0)	vr7 = xor.u32 vr15, vr17;"
        "V1@(pr0)	vr7 = and.u32 vr7, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "pseudo@0	@pseudo imm_2 = 3;"
        "V0@(pr0)	vr3 = or.u32 vr3, r44;"
        "V1@(pr0)	vr3 = shl.u32 vr3, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 57344;"
        "pseudo@0	@pseudo imm_2 = 29;"
        "V0@(pr0)	vr28 = and.u32 vr2, r44;"
        "V1@(pr0)	vr28 = shr.u32 vr28, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3;"
        "V0@(pr0)	vr3 = or.u32 vr3, vr28;"
        "V1@(pr0)	vr2 = shl.u32 vr2, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "pseudo@0	@pseudo imm_2 = 3;"
        "V0@(pr0)	vr5 = or.u32 vr5, r44;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 57344;"
        "pseudo@0	@pseudo imm_2 = 29;"
        "V0@(pr0)	vr28 = and.u32 vr4, r44;"
        "V1@(pr0)	vr28 = shr.u32 vr28, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3;"
        "V0@(pr0)	vr5 = or.u32 vr5, vr28;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 64;"
        "V0@(pr0)	vr28 = sub.s32 vr0, vr1;"
        "V1@(pr0)	vr29 = mov.u32 r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr29 = sub.s32 vr29, vr28;"
        "V1@(pr0)	vmsk0 = gt.s32 vr29, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr31 = mov.u32 r32;"
        "V1@(pr0)	vr31 = sub.s32 vr31, vr29;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr30 = mov.u32 r44;"
        "}"
        "{"
        "V1@(pr0)	vr30 = shl.u32 vr30, vr31;"
        "}"
        "{"
        "V0@(pr0)	vr30 = and.u32 vr4, vr30;"
        "V1@(pr0)	vr30 = shr.u32 vr30, vr31;"
        "}"
        "{"
        "V1@(pr0)	vr11 = shl.u32 vr5, vr29;"
        "}"
        "{"
        "V0@(pr0)	vr11 = or.u32 vr11, vr30;"
        "V1@(pr0)	vr10 = shl.u32 vr4, vr29;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V1@(pr0)	vr30 = sub.s32 vr29, r32;"
        "}"
        "{"
        "V1@(pr0)	vr12 = shl.u32 vr4, vr30;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk0 vr10, r46;"
        "V1@(pr0)	vr11 = sel vmsk0 vr11, vr12;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = eq.s32 vr10, r46;"
        "V1@(pr0)	vmsk1 = eq.s32 vr11, r46;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 r46;"
        "V1@(pr0)	vr17 = sel vmsk0 vr11, r48;"
        "}"
        "{"
        "V0@(pr0)	vr16 = sel vmsk1 vr11, r48;"
        "V1@(pr0)	vr17 = add.s32 vr16, vr17;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vmsk0 = eq.s32 vr17, r32;"
        "V1@(pr0)	vr17 = sel vmsk0 vr11, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V1@(pr0)	vmsk0 = gt.s32 vr28, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 65535;"
        "V0@(pr0)	vr31 = mov.u32 r45;"
        "V1@(pr0)	vr30 = mov.u32 r32;"
        "}"
        "{"
        "V0@(pr0)	vr30 = sub.s32 vr30, vr28;"
        "V1@(pr0)	vr31 = shr.u32 vr31, vr30;"
        "}"
        "{"
        "V0@(pr0)	vr31 = and.u32 vr31, vr5;"
        "V1@(pr0)	vr31 = shl.u32 vr31, vr30;"
        "}"
        "{"
        "V1@(pr0)	vr10 = shr.u32 vr4, vr28;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr10, vr31;"
        "V1@(pr0)	vr11 = shr.u32 vr5, vr28;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr31 = sub.s32 vr28, r32;"
        "V1@(pr0)	vr12 = shr.u32 vr5, vr31;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk0 vr10, vr12;"
        "V1@(pr0)	vr11 = sel vmsk0 vr11, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 64;"
        "V0@(pr0)	vr10 = or.u32 vr10, vr17;"
        "V1@(pr0)	vmsk0 = ls.s32 vr28, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 r46;"
        "V1@(pr0)	vr11 = sel vmsk0 vr17, vr11;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 r48;"
        "V1@(pr0)	vr10 = sel vmsk0 vr17, vr10;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = eq.s32 vr28, r36;"
        "V1@(pr0)	vr4 = sel vmsk0 vr10, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "V0@(pr0)	vr5 = sel vmsk0 vr11, vr5;"
        "V1@(pr0)	vr10 = and.u32 vr2, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr12 = and.u32 vr4, r33;"
        "V1@(pr0)	vr11 = shr.u32 vr2, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr13 = shr.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = gteq.s32 vr10, vr12;"
        "V1@(pr0)	vr28 = mov.u32 r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr17 = sel vmsk0 vr28, r46;"
        "V1@(pr0)	vr17 = shl.u32 vr17, r32;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr10, vr17;"
        "V1@(pr0)	vr14 = sub.s32 vr10, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr15 = add.s32 vr13, r48;"
        "V1@(pr0)	vr13 = sel vmsk0 vr15, vr13;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = gteq.s32 vr11, vr13;"
        "V1@(pr0)	vr17 = sel vmsk0 vr28, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr17 = shl.u32 vr17, r32;"
        "}"
        "{"
        "V0@(pr0)	vr11 = or.u32 vr11, vr17;"
        "V1@(pr0)	vr15 = sub.s32 vr11, vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr14, vr15;"
        "}"
        "{"
        "V0@(pr0)	vr17 = add.s32 vr5, r48;"
        "V1@(pr0)	vr17 = sel vmsk0 vr17, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr11 = sub.s32 vr3, vr17;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = eq.s32 vr11, r46;"
        "V1@(pr0)	vr31 = count.u32 vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr30 = count.u32 vr10;"
        "V1@(pr0)	vr30 = add.s32 vr30, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr31 = sel vmsk0 vr31, vr30;"
        "V1@(pr0)	vr31 = sub.s32 vr31, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vmsk6 = ls.s32 vr31, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr30 = mov.u32 r32;"
        "V1@(pr0)	vr30 = sub.s32 vr30, vr31;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr29 = mov.u32 r44;"
        "V1@(pr0)	vr29 = shl.u32 vr29, vr30;"
        "}"
        "{"
        "V0@(pr0)	vr29 = and.u32 vr29, vr10;"
        "V1@(pr0)	vr29 = shr.u32 vr29, vr30;"
        "}"
        "{"
        "V1@(pr0)	vr13 = shl.u32 vr11, vr31;"
        "}"
        "{"
        "V0@(pr0)	vr13 = or.u32 vr13, vr29;"
        "V1@(pr0)	vr12 = shl.u32 vr10, vr31;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr30 = mov.u32 r32;"
        "V1@(pr0)	vr30 = sub.s32 vr31, vr30;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr14 = shl.u32 vr10, vr30;"
        "}"
        "{"
        "V0@(pr0)	vr12 = sel vmsk6 vr15, vr12;"
        "V1@(pr0)	vr13 = sel vmsk6 vr14, vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 128;"
        "V0@(pr0)	vmsk0 = ls.s32 vr11, r36;"
        "V1@(pr0)	vr10 = sel vmsk0 vr10, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr15 = sub.s32 vr0, vr31;"
        "}"
        "{"
        "V0@(pr0)	vr11 = sel vmsk0 vr11, vr13;"
        "V1@(pr0)	vr15 = sel vmsk0 vr0, vr15;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr12 = and.u32 vr2, r33;"
        "V1@(pr0)	vr13 = shr.u32 vr2, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr16 = and.u32 vr4, r33;"
        "V1@(pr0)	vr17 = shr.u32 vr4, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr28 = add.s32 vr12, vr16;"
        "V1@(pr0)	vr14 = shr.u32 vr28, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "V0@(pr0)	vr28 = and.u32 vr28, r32;"
        "}"
        "{"
        "V0@(pr0)	vr29 = add.s32 vr13, vr17;"
        "V1@(pr0)	vr29 = add.s32 vr29, vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr29, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "V0@(pr0)	vr29 = and.u32 vr29, r32;"
        "V1@(pr0)	vr29 = shl.u32 vr29, r33;"
        "}"
        "{"
        "V0@(pr0)	vr12 = or.u32 vr29, vr28;"
        "V1@(pr0)	vr13 = add.s32 vr3, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr13 = add.s32 vr13, vr14;"
        "V1@(pr0)	vr28 = and.u32 vr12, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V0@(pr0)	vr14 = and.u32 vr13, r48;"
        "V1@(pr0)	vr14 = shl.u32 vr14, r32;"
        "}"
        "{"
        "V1@(pr0)	vr16 = shr.u32 vr12, r48;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr16, vr14;"
        "V1@(pr0)	vr17 = shr.u32 vr13, r48;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr16, vr28;"
        "V1@(pr0)	vr14 = add.s32 vr0, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 256;"
        "V0@(pr0)	vr28 = and.u32 vr13, r36;"
        "V1@(pr0)	vmsk0 = eq.s32 vr28, r46;"
        "}"
        "{"
        "V0@(pr0)	vr12 = sel vmsk0 vr16, vr12;"
        "V1@(pr0)	vr13 = sel vmsk0 vr17, vr13;"
        "}"
        "{"
        "V0@(pr0)	vr14 = sel vmsk0 vr14, vr0;"
        "V1@(pr0)	vmsk1 = eq.s32 vr7, r46;"
        "}"
        "{"
        "V0@(pr0)	vr2 = sel vmsk1 vr10, vr12;"
        "V1@(pr0)	vr3 = sel vmsk1 vr11, vr13;"
        "}"
        "{"
        "V0@(pr0)	vr0 = sel vmsk1 vr15, vr14;"
        "V1@(pr0)	vr31 = mov.u32 r48;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr2, r48;"
        "V1@(pr0)	vmsk0 = lseq.s32 vr0, r46;"
        "}"
        "{"
        "V0@(pr0)	vr2 = sel vmsk0 vr2, vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 7;"
        "pseudo@0	@pseudo imm_1 = 3;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 65535;"
        "V0@(pr0)	vr17 = and.u32 vr2, r32;"
        "V1@(pr0)	vr31 = mov.u32 r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 29;"
        "V1@(pr0)	vr31 = shr.u32 vr31, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 29;"
        "V0@(pr0)	vr31 = and.u32 vr3, vr31;"
        "V1@(pr0)	vr31 = shl.u32 vr31, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3;"
        "V1@(pr0)	vr10 = shr.u32 vr2, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3;"
        "V0@(pr0)	vr10 = or.u32 vr10, vr31;"
        "V1@(pr0)	vr11 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 15;"
        "V0@(pr0)	vr10 = and.u32 vr10, r44;"
        "V1@(pr0)	vr11 = and.u32 vr11, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr28 = shl.u32 vr0, r32;"
        "}"
        "{"
        "V0@(pr0)	vr11 = or.u32 vr11, vr28;"
        "V1@(pr0)	vr11 = or.u32 vr11, vr6;"
        "}"
        "{"
        "V0@(pr0)	vr12 = add.s32 vr10, r48;"
        "V1@(pr0)	vr13 = and.u32 vr10, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "V0@(pr0)	vmsk0 = gt.s32 vr17, r32;"
        "V1@(pr0)	vmsk1 = eq.s32 vr17, r32;"
        "}"
        "{"
        "V0@(pr0)	vr13 = add.s32 vr10, vr13;"
        "V1@(pr0)	vr13 = sel vmsk1 vr10, vr13;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk0 vr13, vr12;"
        "V1@(pr0)	vr0 = mov.u32 vr10;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mov.u32 vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr10 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr11 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr12 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr13 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 5120;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 6144;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr16 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 7168;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr17 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8192;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = eq.s32 vr17, r46;"
        "V1@(pr0)	vmsk1 = eq.s32 vr16, r46;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 r46;"
        "V1@(pr0)	vr7 = sel vmsk0 vr28, r48;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr28, r48;"
        "V1@(pr0)	vr7 = add.s32 vr6, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vmsk0 = eq.s32 vr7, r32;"
        "V1@(pr0)	vr30 = sel vmsk0 vr0, vr10;"
        "}"
        "{"
        "V0@(pr0)	vr31 = sel vmsk0 vr1, vr11;"
        "V1@(pr0)	vr6 = and.u32 vr10, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr7 = and.u32 vr11, vr13;"
        "V1@(pr0)	vr5 = sel vmsk0 vr13, vr7;"
        "}"
        "{"
        "V0@(pr0)	vr4 = sel vmsk0 vr12, vr6;"
        "V1@(pr0)	vmsk0 = eq.s32 vr14, r46;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = eq.s32 vr15, r46;"
        "V1@(pr0)	vr6 = sel vmsk0 vr28, r48;"
        "}"
        "{"
        "V0@(pr0)	vr7 = sel vmsk1 vr28, r48;"
        "V1@(pr0)	vr7 = add.s32 vr6, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vmsk0 = eq.s32 vr7, r32;"
        "V1@(pr0)	vr31 = sel vmsk0 vr31, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr30 = sel vmsk0 vr30, r34;"
        "V1@(pr0)	vr7 = mov.u32 r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 15;"
        "V0@(pr0)	vr6 = mov.u32 r44;"
        "V1@(pr0)	vr7 = xor.u32 vr7, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr6 = xor.u32 vr6, r44;"
        "V1@(pr0)	vmsk0 = eq.s32 vr16, vr6;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mov.u32 vr7;"
        "V1@(pr0)	vr4 = mov.u32 vr6;"
        "}"
        "{"
        "V0@(pr0)	vmsk6 = eq.s32 vr14, vr6;"
        "V1@(pr0)	vmsk7 = eq.s32 vr15, vr7;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = eq.s32 vr17, vr7;"
        "V1@(pr0)	vr7 = sel vmsk0 vr28, r48;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr28, r48;"
        "V1@(pr0)	vr7 = add.s32 vr6, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vmsk0 = eq.s32 vr7, r32;"
        "V1@(pr0)	vr31 = sel vmsk0 vr31, vr13;"
        "}"
        "{"
        "V0@(pr0)	vr30 = sel vmsk0 vr30, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr7 = sel vmsk7 vr28, r48;"
        "V1@(pr0)	vr6 = sel vmsk6 vr28, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vr7 = add.s32 vr6, vr7;"
        "V1@(pr0)	vmsk7 = eq.s32 vr7, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr3 = or.u32 vr5, r36;"
        "V1@(pr0)	vr6 = xor.u32 vr10, vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32768;"
        "V0@(pr0)	vr7 = xor.u32 vr11, vr13;"
        "V1@(pr0)	vmsk0 = eq.s32 vr7, r36;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = eq.s32 vr6, r46;"
        "V1@(pr0)	vr7 = sel vmsk0 vr28, r48;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr28, r48;"
        "V1@(pr0)	vr7 = add.s32 vr6, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vmsk0 = eq.s32 vr7, r32;"
        "V1@(pr0)	vr7 = sel vmsk0 vr11, vr3;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk0 vr10, vr4;"
        "V1@(pr0)	vr30 = sel vmsk7 vr30, vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr31 = sel vmsk7 vr31, vr7;"
        "V1@(pr0)	vr7 = or.u32 vr13, r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk0 = eq.s32 vr16, r46;"
        "V1@(pr0)	vr6 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "V0@(pr0)	vr28 = and.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32752;"
        "V0@(pr0)	vmsk1 = gt.s32 vr6, r32;"
        "V1@(pr0)	vmsk2 = eq.s32 vr28, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk0 vr28, r48;"
        "V1@(pr0)	vr4 = sel vmsk2 vr28, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vr5 = add.s32 vr4, vr5;"
        "V1@(pr0)	vmsk2 = eq.s32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr7, vr31;"
        "V1@(pr0)	vr4 = sel vmsk2 vr12, vr30;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32752;"
        "V0@(pr0)	vmsk2 = eq.s32 vr6, r32;"
        "V1@(pr0)	vr4 = sel vmsk2 vr30, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr31, vr5;"
        "V1@(pr0)	vr31 = sel vmsk1 vr5, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr30 = sel vmsk1 vr4, vr12;"
        "V1@(pr0)	vr7 = or.u32 vr11, r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk0 = eq.s32 vr14, r46;"
        "V1@(pr0)	vr6 = shr.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "V0@(pr0)	vr28 = and.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32752;"
        "V0@(pr0)	vmsk1 = gt.s32 vr6, r32;"
        "V1@(pr0)	vmsk2 = eq.s32 vr28, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk0 vr28, r48;"
        "V1@(pr0)	vr4 = sel vmsk2 vr28, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vr5 = add.s32 vr4, vr5;"
        "V1@(pr0)	vmsk2 = eq.s32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr7, vr31;"
        "V1@(pr0)	vr4 = sel vmsk2 vr10, vr30;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32752;"
        "V0@(pr0)	vmsk2 = eq.s32 vr6, r32;"
        "V1@(pr0)	vr4 = sel vmsk2 vr30, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr31, vr5;"
        "V1@(pr0)	vr11 = sel vmsk1 vr5, vr7;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr4, vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr12 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr1 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V1@(pr0)	vr2 = shr.u32 vr1, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 5120;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr12;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 26;"
        "V1@(pr0)	vr1 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 15;"
        "V1@(pr0)	vr1 = and.u32 vr1, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 162;"
        "pseudo@0	@pseudo imm_3 = 0;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 28238;"
        "pseudo@0	@pseudo imm_1 = 63875;"
        "pseudo@0	@pseudo imm_2 = 10748;"
        "pseudo@0	@pseudo imm_3 = 17429;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "pseudo@0	@pseudo imm_2 = 41721;"
        "pseudo@0	@pseudo imm_3 = 0;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20036;"
        "pseudo@0	@pseudo imm_1 = 33646;"
        "pseudo@0	@pseudo imm_2 = 64551;"
        "pseudo@0	@pseudo imm_3 = 5417;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "pseudo@0	@pseudo imm_2 = 63875;"
        "pseudo@0	@pseudo imm_3 = 162;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 17429;"
        "pseudo@0	@pseudo imm_1 = 28238;"
        "pseudo@0	@pseudo imm_2 = 10071;"
        "pseudo@0	@pseudo imm_3 = 10748;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3;"
        "pseudo@0	@pseudo imm_2 = 33646;"
        "pseudo@0	@pseudo imm_3 = 41721;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 5417;"
        "pseudo@0	@pseudo imm_1 = 20036;"
        "pseudo@0	@pseudo imm_2 = 22481;"
        "pseudo@0	@pseudo imm_3 = 64551;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "pseudo@0	@pseudo imm_2 = 28238;"
        "pseudo@0	@pseudo imm_3 = 63875;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 10748;"
        "pseudo@0	@pseudo imm_1 = 17429;"
        "pseudo@0	@pseudo imm_2 = 53749;"
        "pseudo@0	@pseudo imm_3 = 10071;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 5;"
        "pseudo@0	@pseudo imm_2 = 20036;"
        "pseudo@0	@pseudo imm_3 = 33646;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 64551;"
        "pseudo@0	@pseudo imm_1 = 5417;"
        "pseudo@0	@pseudo imm_2 = 62772;"
        "pseudo@0	@pseudo imm_3 = 22481;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 6;"
        "pseudo@0	@pseudo imm_2 = 17429;"
        "pseudo@0	@pseudo imm_3 = 28238;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 10071;"
        "pseudo@0	@pseudo imm_1 = 10748;"
        "pseudo@0	@pseudo imm_2 = 13533;"
        "pseudo@0	@pseudo imm_3 = 53749;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 7;"
        "pseudo@0	@pseudo imm_2 = 5417;"
        "pseudo@0	@pseudo imm_3 = 20036;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 22481;"
        "pseudo@0	@pseudo imm_1 = 64551;"
        "pseudo@0	@pseudo imm_2 = 56768;"
        "pseudo@0	@pseudo imm_3 = 62772;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "pseudo@0	@pseudo imm_2 = 10748;"
        "pseudo@0	@pseudo imm_3 = 17429;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 53749;"
        "pseudo@0	@pseudo imm_1 = 10071;"
        "pseudo@0	@pseudo imm_2 = 49371;"
        "pseudo@0	@pseudo imm_3 = 13533;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 9;"
        "pseudo@0	@pseudo imm_2 = 64551;"
        "pseudo@0	@pseudo imm_3 = 5417;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 62772;"
        "pseudo@0	@pseudo imm_1 = 22481;"
        "pseudo@0	@pseudo imm_2 = 56162;"
        "pseudo@0	@pseudo imm_3 = 56768;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 10;"
        "pseudo@0	@pseudo imm_2 = 10071;"
        "pseudo@0	@pseudo imm_3 = 10748;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 13533;"
        "pseudo@0	@pseudo imm_1 = 53749;"
        "pseudo@0	@pseudo imm_2 = 25237;"
        "pseudo@0	@pseudo imm_3 = 49371;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 11;"
        "pseudo@0	@pseudo imm_2 = 22481;"
        "pseudo@0	@pseudo imm_3 = 64551;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 56768;"
        "pseudo@0	@pseudo imm_1 = 62772;"
        "pseudo@0	@pseudo imm_2 = 38297;"
        "pseudo@0	@pseudo imm_3 = 56162;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 12;"
        "pseudo@0	@pseudo imm_2 = 53749;"
        "pseudo@0	@pseudo imm_3 = 10071;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 49371;"
        "pseudo@0	@pseudo imm_1 = 13533;"
        "pseudo@0	@pseudo imm_2 = 39228;"
        "pseudo@0	@pseudo imm_3 = 25237;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 13;"
        "pseudo@0	@pseudo imm_2 = 62772;"
        "pseudo@0	@pseudo imm_3 = 22481;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 56162;"
        "pseudo@0	@pseudo imm_1 = 56768;"
        "pseudo@0	@pseudo imm_2 = 15427;"
        "pseudo@0	@pseudo imm_3 = 38297;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 14;"
        "pseudo@0	@pseudo imm_2 = 13533;"
        "pseudo@0	@pseudo imm_3 = 53749;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 25237;"
        "pseudo@0	@pseudo imm_1 = 49371;"
        "pseudo@0	@pseudo imm_2 = 17296;"
        "pseudo@0	@pseudo imm_3 = 39228;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 23;"
        "V1@(pr0)	vr1 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 7;"
        "V1@(pr0)	vr1 = and.u32 vr1, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 255;"
        "pseudo@0	@pseudo imm_2 = 128;"
        "V0@(pr0)	vr10 = and.u32 vr10, r44;"
        "V1@(pr0)	vr10 = or.u32 vr10, r38;"
        "}"
        "{"
        "V1@(pr0)	vr10 = shl.u32 vr10, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr10;"
        "V1@(pr0)	vr13 = mov.u32 vr28;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr14;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr12, r54;"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr12, r32;"
        "}"
        "{"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr13, r32;"
        "}"
        "{"
        "V1@(pr0)	vr15 = and.u32 vr12, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr16;"
        "V1@(pr0)	vr13 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 vr13;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr10;"
        "V1@(pr0)	vr13 = mov.u32 vr30;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr14;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr12, r54;"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr12, r32;"
        "}"
        "{"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr13, r32;"
        "}"
        "{"
        "V1@(pr0)	vr15 = and.u32 vr12, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr16;"
        "V1@(pr0)	vr13 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr30 = mov.u32 vr12;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr10;"
        "V1@(pr0)	vr13 = mov.u32 vr29;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr14;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr12, r54;"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr12, r32;"
        "}"
        "{"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr13, r32;"
        "}"
        "{"
        "V1@(pr0)	vr15 = and.u32 vr12, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr16;"
        "V1@(pr0)	vr13 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr12;"
        "V1@(pr0)	vr17 = mov.u32 vr13;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 vr28;"
        "V1@(pr0)	vr14 = mov.u32 vr30;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8192;"
        "V0@(pr0)	vr1 = add.s32 vr2, r32;"
        "V1@(pr0)	vr1 = and.u32 vr1, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 14;"
        "V1@(pr0)	vr1 = shr.u32 vr1, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 14;"
        "V1@(pr0)	vr3 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V1@(pr0)	vr2 = sub.s32 vr2, vr3;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr16 = and.u32 vr16, r54;"
        "V1@(pr0)	vr2 = shl.u32 vr2, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr16, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr1;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32768;"
        "V0@(pr0)	vr12 = mov.u32 r44;"
        "V1@(pr0)	vmsk7 = gteq.f32 vr16, r46;"
        "}"
        "{"
        "V1@(pr0)	vr12 = sel vmsk7 vr12, r46;"
        "}"
        "{"
        "V0@(pr0)	vmsk3 = eq.s32 vr17, r46;"
        "V1@(pr0)	vr6 = sub.s32 vr17, r48;"
        "}"
        "{"
        "V1@(pr0)	vr7 = sub.s32 vr16, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr6 = sel vmsk3 vr6, r44;"
        "V1@(pr0)	vr7 = sel vmsk3 vr16, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr6 = xor.u32 vr6, r44;"
        "V1@(pr0)	vr7 = xor.u32 vr7, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr17 = sel vmsk7 vr6, vr17;"
        "V1@(pr0)	vr16 = sel vmsk7 vr7, vr16;"
        "}"
        "{"
        "V0@(pr0)	vr2 = count.u32 vr16;"
        "V1@(pr0)	vr1 = count.u32 vr17;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vmsk6 = ls.s32 vr2, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V0@(pr0)	vr6 = mov.u32 r32;"
        "V1@(pr0)	vr1 = sub.s32 vr6, vr1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 63;"
        "V0@(pr0)	vr6 = mov.u32 r32;"
        "V1@(pr0)	vr2 = sub.s32 vr6, vr2;"
        "}"
        "{"
        "V1@(pr0)	vr13 = sel vmsk6 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr0 = mov.u32 vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1023;"
        "V0@(pr0)	vr13 = add.s32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr13 = shl.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 20;"
        "V0@(pr0)	vmsk5 = lseq.s32 vr0, r32;"
        "V1@(pr0)	vmsk4 = lseq.s32 vr0, r33;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr6 = sub.s32 vr0, r32;"
        "V1@(pr0)	vr7 = mov.u32 r48;"
        "}"
        "{"
        "V1@(pr0)	vr6 = shl.u32 vr7, vr6;"
        "}"
        "{"
        "V1@(pr0)	vr6 = sub.s32 vr16, vr6;"
        "}"
        "{"
        "V1@(pr0)	vr7 = shl.u32 vr7, vr0;"
        "}"
        "{"
        "V1@(pr0)	vr7 = sub.s32 vr17, vr7;"
        "}"
        "{"
        "V1@(pr0)	vr5 = sel vmsk4 vr6, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "V0@(pr0)	vr6 = sub.s32 vr0, r32;"
        "V1@(pr0)	vr3 = shr.u32 vr5, vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 84;"
        "V0@(pr0)	vr6 = mov.u32 r32;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sub.s32 vr6, vr0;"
        "V1@(pr0)	vr4 = shl.u32 vr5, vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "V0@(pr0)	vr6 = sub.s32 vr0, r32;"
        "V1@(pr0)	vr6 = shr.u32 vr17, vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V0@(pr0)	vr4 = add.s32 vr4, vr6;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 vr3;"
        "V1@(pr0)	vr14 = mov.u32 vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "V0@(pr0)	vr6 = mov.u32 r32;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sub.s32 vr6, vr0;"
        "V1@(pr0)	vr3 = shl.u32 vr5, vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V0@(pr0)	vr6 = sub.s32 vr0, r32;"
        "V1@(pr0)	vr6 = shr.u32 vr17, vr6;"
        "}"
        "{"
        "V1@(pr0)	vr3 = add.s32 vr3, vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "V0@(pr0)	vr7 = mov.u32 r32;"
        "}"
        "{"
        "V0@(pr0)	vr7 = sub.s32 vr7, vr0;"
        "V1@(pr0)	vr4 = shl.u32 vr17, vr7;"
        "}"
        "{"
        "V0@(pr0)	vr15 = sel vmsk5 vr15, vr3;"
        "V1@(pr0)	vr14 = sel vmsk5 vr14, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V0@(pr0)	vr6 = mov.u32 r32;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sub.s32 vr6, vr0;"
        "V1@(pr0)	vr3 = shl.u32 vr5, vr6;"
        "}"
        "{"
        "V0@(pr0)	vr15 = sel vmsk4 vr15, vr3;"
        "V1@(pr0)	vr14 = sel vmsk4 vr14, r46;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr12, vr13;"
        "V1@(pr0)	vr10 = or.u32 vr10, vr15;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8699;"
        "pseudo@0	@pseudo imm_1 = 15385;"
        "pseudo@0	@pseudo imm_2 = 11544;"
        "pseudo@0	@pseudo imm_3 = 21572;"
        "V0@(pr0)	vr12 = mov.u32 r44;"
        "V1@(pr0)	vr13 = mov.u32 r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr13;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr1 = and.u32 vr10, r44;"
        "V1@(pr0)	vr2 = and.u32 vr12, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "V0@(pr0)	vmsk1 = ls.f32 vr1, r44;"
        "V1@(pr0)	vmsk2 = ls.f32 vr2, r44;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, r46;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, r46;"
        "}"
        "{"
        "V0@(pr0)	vr12 = sel vmsk2 vr12, r46;"
        "V1@(pr0)	vr13 = sel vmsk2 vr13, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr6 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr7 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 15;"
        "V0@(pr0)	vr10 = and.u32 vr10, r44;"
        "V1@(pr0)	vr12 = and.u32 vr12, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "V0@(pr0)	vr10 = or.u32 vr10, r44;"
        "V1@(pr0)	vr12 = or.u32 vr12, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 11;"
        "V1@(pr0)	vr12 = shl.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 21;"
        "V1@(pr0)	vr1 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 11;"
        "V0@(pr0)	vr12 = or.u32 vr12, vr1;"
        "V1@(pr0)	vr13 = shl.u32 vr13, r32;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr13, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = and.u32 vr12, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr13, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr12, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = and.u32 vr12, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr12, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 r46;"
        "V1@(pr0)	vr10 = mov.u32 vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr10;"
        "V1@(pr0)	vr17 = mov.u32 vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = or.u32 vr1, vr2;"
        "V1@(pr0)	vr14 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr12 = mov.u32 r46;"
        "V1@(pr0)	vr13 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "V1@(pr0)	vr15 = shr.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 vr15;"
        "V1@(pr0)	vr29 = mov.u32 vr14;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr15;"
        "V1@(pr0)	vr15 = mov.u32 vr0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr16;"
        "V1@(pr0)	vr11 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr12;"
        "V1@(pr0)	vr17 = mov.u32 vr13;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr29;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 vr15;"
        "V1@(pr0)	vr29 = mov.u32 vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr16;"
        "V1@(pr0)	vr13 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr10;"
        "V1@(pr0)	vr17 = mov.u32 vr11;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr28;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr16;"
        "V1@(pr0)	vr11 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr29 = shl.u32 vr29, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr12;"
        "V1@(pr0)	vr17 = mov.u32 vr13;"
        "}"
        "{"
        "V0@(pr0)	vr30 = mov.u32 vr14;"
        "V1@(pr0)	vr14 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr31 = mov.u32 vr17;"
        "V1@(pr0)	vr17 = mov.u32 vr16;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr11;"
        "V1@(pr0)	vr15 = mov.u32 vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr16;"
        "V1@(pr0)	vr11 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr13;"
        "V1@(pr0)	vr13 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr28;"
        "V1@(pr0)	vr17 = mov.u32 vr29;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr30;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr13;"
        "V1@(pr0)	vr15 = mov.u32 vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr13 = mov.u32 vr17;"
        "V1@(pr0)	vr12 = mov.u32 vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "V0@(pr0)	vr3 = xor.u32 vr6, vr7;"
        "V1@(pr0)	vr3 = and.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr3 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 2047;"
        "pseudo@0	@pseudo imm_3 = 0;"
        "V0@(pr0)	vr7 = and.u32 vr7, r45;"
        "V1@(pr0)	vr6 = and.u32 vr6, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 1023;"
        "pseudo@0	@pseudo imm_3 = 0;"
        "V0@(pr0)	vr6 = add.s32 vr6, vr7;"
        "V1@(pr0)	vr6 = sub.s32 vr6, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 16;"
        "V0@(pr0)	vr1 = and.u32 vr10, r45;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr6, r48;"
        "V1@(pr0)	vmsk1 = eq.s32 vr1, r46;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr2, vr6;"
        "}"
        "{"
        "V1@(pr0)	vr2 = shl.u32 vr10, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V1@(pr0)	vr5 = shr.u32 vr11, r32;"
        "}"
        "{"
        "V0@(pr0)	vr2 = or.u32 vr2, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V1@(pr0)	vr5 = shl.u32 vr13, r32;"
        "}"
        "{"
        "V1@(pr0)	vr4 = shl.u32 vr11, r48;"
        "}"
        "{"
        "V0@(pr0)	vr5 = or.u32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr2;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr5;"
        "}"
        "{"
        "V1@(pr0)	vr2 = shl.u32 vr12, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V1@(pr0)	vr5 = shr.u32 vr13, r32;"
        "}"
        "{"
        "V0@(pr0)	vr2 = or.u32 vr2, vr5;"
        "}"
        "{"
        "V1@(pr0)	vr5 = shl.u32 vr13, r48;"
        "}"
        "{"
        "V0@(pr0)	vr12 = sel vmsk1 vr12, vr2;"
        "V1@(pr0)	vr13 = sel vmsk1 vr13, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 15;"
        "V0@(pr0)	vr1 = and.u32 vr10, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr2 = shl.u32 vr6, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = gt.s32 vr6, r46;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr10, vr3;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32768;"
        "V0@(pr0)	vr1 = add.s32 vr11, r48;"
        "V1@(pr0)	vmsk1 = gt.s32 vr12, r44;"
        "}"
        "{"
        "V0@(pr0)	vr11 = sel vmsk1 vr11, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr1 = and.u32 vr11, r48;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr11;"
        "V1@(pr0)	vr15 = mov.u32 vr10;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr1;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32768;"
        "V1@(pr0)	vmsk1 = eq.s32 vr12, r45;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr16;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr17;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr28 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr29 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr30 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr31 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr1 = and.u32 vr28, r44;"
        "V1@(pr0)	vr2 = and.u32 vr30, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = lseq.s32 vr1, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 8;"
        "V1@(pr0)	vr1 = or.u32 vr28, r45;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr1, vr10;"
        "V1@(pr0)	vr11 = sel vmsk1 vr29, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = lseq.s32 vr2, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 8;"
        "V1@(pr0)	vr1 = or.u32 vr30, r45;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr1, vr10;"
        "V1@(pr0)	vr11 = sel vmsk1 vr31, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32768;"
        "V0@(pr0)	vr3 = xor.u32 vr28, vr30;"
        "V1@(pr0)	vr3 = and.u32 vr3, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = eq.f32 vr1, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32760;"
        "V0@(pr0)	vr5 = mov.u32 r44;"
        "}"
        "{"
        "V0@(pr0)	vr4 = xor.u32 vr1, vr3;"
        "}"
        "{"
        "V1@(pr0)	vmsk2 = eq.s32 vr2, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr5;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = eq.f32 vr2, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32760;"
        "V0@(pr0)	vr5 = mov.u32 r44;"
        "}"
        "{"
        "V0@(pr0)	vr4 = xor.u32 vr2, vr3;"
        "}"
        "{"
        "V1@(pr0)	vmsk2 = eq.s32 vr1, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr5;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr11;"
        "}"
        "{"
        "V1@(pr0)	vmsk1 = eq.f32 vr1, r46;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr3;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, r46;"
        "}"
        "{"
        "V1@(pr0)	vmsk1 = eq.f32 vr2, r46;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr3;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr12 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr5 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr4 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr1 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr2 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr3 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 5120;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32768;"
        "V0@(pr0)	vmsk1 = eq.s32 vr4, r48;"
        "V1@(pr0)	vr6 = xor.u32 vr10, r36;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 17136;"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 20224;"
        "V0@(pr0)	vr6 = and.u32 vr5, r45;"
        "V1@(pr0)	vmsk4 = ls.f32 vr6, r44;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk4 vr10, vr2;"
        "V1@(pr0)	vr11 = sel vmsk4 vr11, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr13 = sel vmsk4 vr12, vr3;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 49152;"
        "pseudo@0	@pseudo imm_2 = 3;"
        "V0@(pr0)	vr4 = and.u32 vr10, r44;"
        "V1@(pr0)	vr1 = shl.u32 vr10, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 16383;"
        "V0@(pr0)	vr1 = and.u32 vr1, r44;"
        "V1@(pr0)	vr4 = or.u32 vr1, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 29;"
        "V1@(pr0)	vr1 = shr.u32 vr11, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = or.u32 vr1, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr13;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 29;"
        "V1@(pr0)	vr12 = shl.u32 vr4, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 49152;"
        "pseudo@0	@pseudo imm_2 = 3;"
        "V0@(pr0)	vr6 = and.u32 vr4, r44;"
        "V1@(pr0)	vr2 = shr.u32 vr4, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 2047;"
        "V0@(pr0)	vr2 = and.u32 vr2, r44;"
        "V1@(pr0)	vr6 = or.u32 vr2, vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 2048;"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 14336;"
        "V0@(pr0)	vr2 = and.u32 vr4, r44;"
        "V1@(pr0)	vr3 = or.u32 vr6, r45;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = eq.s32 vr2, r46;"
        "V1@(pr0)	vr6 = sel vmsk1 vr3, vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32768;"
        "V1@(pr0)	vr13 = xor.u32 vr6, r36;"
        "}"
        "{"
        "V0@(pr0)	vr2 = mov.u32 vr10;"
        "V1@(pr0)	vr10 = mov.u32 vr11;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 32767;"
        "V0@(pr0)	vr14 = and.u32 vr10, r44;"
        "V1@(pr0)	vr15 = and.u32 vr11, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 32767;"
        "V0@(pr0)	vr16 = and.u32 vr12, r44;"
        "V1@(pr0)	vr17 = and.u32 vr13, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8192;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 5120;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr15;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 6144;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 7168;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = gt.s32 vr17, vr15;"
        "V1@(pr0)	vmsk1 = eq.s32 vr17, vr15;"
        "}"
        "{"
        "V0@(pr0)	vmsk2 = gt.s32 vr16, vr14;"
        "}"
        "{"
        "V0@(pr0)	vr1 = sel vmsk2 vr13, vr11;"
        "V1@(pr0)	vr0 = sel vmsk2 vr12, vr10;"
        "}"
        "{"
        "V0@(pr0)	vr1 = sel vmsk1 vr13, vr1;"
        "V1@(pr0)	vr0 = sel vmsk1 vr12, vr0;"
        "}"
        "{"
        "V0@(pr0)	vr1 = sel vmsk0 vr1, vr11;"
        "V1@(pr0)	vr0 = sel vmsk0 vr0, vr10;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = eq.s32 vr1, vr11;"
        "V1@(pr0)	vmsk1 = eq.s32 vr0, vr10;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr7 = sel vmsk0 vr5, r48;"
        "V1@(pr0)	vr6 = sel vmsk1 vr5, r48;"
        "}"
        "{"
        "V0@(pr0)	vr7 = and.u32 vr6, vr7;"
        "V1@(pr0)	vmsk0 = eq.s32 vr7, r48;"
        "}"
        "{"
        "V0@(pr0)	vr15 = sel vmsk0 vr11, vr13;"
        "V1@(pr0)	vr14 = sel vmsk0 vr10, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr1;"
        "V1@(pr0)	vr16 = mov.u32 vr0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2047;"
        "pseudo@0	@pseudo imm_1 = 20;"
        "V0@(pr0)	vr7 = mov.u32 r32;"
        "V1@(pr0)	vr6 = mov.u32 r33;"
        "}"
        "{"
        "V1@(pr0)	vr0 = shr.u32 vr15, vr6;"
        "}"
        "{"
        "V0@(pr0)	vr0 = and.u32 vr0, vr7;"
        "}"
        "{"
        "V1@(pr0)	vr1 = shr.u32 vr17, vr6;"
        "}"
        "{"
        "V0@(pr0)	vr1 = and.u32 vr1, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 15;"
        "V0@(pr0)	vr2 = and.u32 vr14, r44;"
        "V1@(pr0)	vr3 = and.u32 vr15, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 15;"
        "V0@(pr0)	vr4 = and.u32 vr16, r44;"
        "V1@(pr0)	vr5 = and.u32 vr17, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32768;"
        "V0@(pr0)	vr6 = and.u32 vr15, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32768;"
        "V0@(pr0)	vr7 = xor.u32 vr15, vr17;"
        "V1@(pr0)	vr7 = and.u32 vr7, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "pseudo@0	@pseudo imm_2 = 3;"
        "V0@(pr0)	vr3 = or.u32 vr3, r44;"
        "V1@(pr0)	vr3 = shl.u32 vr3, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 57344;"
        "pseudo@0	@pseudo imm_2 = 29;"
        "V0@(pr0)	vr28 = and.u32 vr2, r44;"
        "V1@(pr0)	vr28 = shr.u32 vr28, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3;"
        "V0@(pr0)	vr3 = or.u32 vr3, vr28;"
        "V1@(pr0)	vr2 = shl.u32 vr2, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "pseudo@0	@pseudo imm_2 = 3;"
        "V0@(pr0)	vr5 = or.u32 vr5, r44;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 57344;"
        "pseudo@0	@pseudo imm_2 = 29;"
        "V0@(pr0)	vr28 = and.u32 vr4, r44;"
        "V1@(pr0)	vr28 = shr.u32 vr28, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3;"
        "V0@(pr0)	vr5 = or.u32 vr5, vr28;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 64;"
        "V0@(pr0)	vr28 = sub.s32 vr0, vr1;"
        "V1@(pr0)	vr29 = mov.u32 r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr29 = sub.s32 vr29, vr28;"
        "V1@(pr0)	vmsk0 = gt.s32 vr29, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr31 = mov.u32 r32;"
        "V1@(pr0)	vr31 = sub.s32 vr31, vr29;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr30 = mov.u32 r44;"
        "}"
        "{"
        "V1@(pr0)	vr30 = shl.u32 vr30, vr31;"
        "}"
        "{"
        "V0@(pr0)	vr30 = and.u32 vr4, vr30;"
        "V1@(pr0)	vr30 = shr.u32 vr30, vr31;"
        "}"
        "{"
        "V1@(pr0)	vr11 = shl.u32 vr5, vr29;"
        "}"
        "{"
        "V0@(pr0)	vr11 = or.u32 vr11, vr30;"
        "V1@(pr0)	vr10 = shl.u32 vr4, vr29;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V1@(pr0)	vr30 = sub.s32 vr29, r32;"
        "}"
        "{"
        "V1@(pr0)	vr12 = shl.u32 vr4, vr30;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk0 vr10, r46;"
        "V1@(pr0)	vr11 = sel vmsk0 vr11, vr12;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = eq.s32 vr10, r46;"
        "V1@(pr0)	vmsk1 = eq.s32 vr11, r46;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 r46;"
        "V1@(pr0)	vr17 = sel vmsk0 vr11, r48;"
        "}"
        "{"
        "V0@(pr0)	vr16 = sel vmsk1 vr11, r48;"
        "V1@(pr0)	vr17 = add.s32 vr16, vr17;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vmsk0 = eq.s32 vr17, r32;"
        "V1@(pr0)	vr17 = sel vmsk0 vr11, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V1@(pr0)	vmsk0 = gt.s32 vr28, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 65535;"
        "V0@(pr0)	vr31 = mov.u32 r45;"
        "V1@(pr0)	vr30 = mov.u32 r32;"
        "}"
        "{"
        "V0@(pr0)	vr30 = sub.s32 vr30, vr28;"
        "V1@(pr0)	vr31 = shr.u32 vr31, vr30;"
        "}"
        "{"
        "V0@(pr0)	vr31 = and.u32 vr31, vr5;"
        "V1@(pr0)	vr31 = shl.u32 vr31, vr30;"
        "}"
        "{"
        "V1@(pr0)	vr10 = shr.u32 vr4, vr28;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr10, vr31;"
        "V1@(pr0)	vr11 = shr.u32 vr5, vr28;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr31 = sub.s32 vr28, r32;"
        "V1@(pr0)	vr12 = shr.u32 vr5, vr31;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk0 vr10, vr12;"
        "V1@(pr0)	vr11 = sel vmsk0 vr11, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 64;"
        "V0@(pr0)	vr10 = or.u32 vr10, vr17;"
        "V1@(pr0)	vmsk0 = ls.s32 vr28, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 r46;"
        "V1@(pr0)	vr11 = sel vmsk0 vr17, vr11;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 r48;"
        "V1@(pr0)	vr10 = sel vmsk0 vr17, vr10;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = eq.s32 vr28, r36;"
        "V1@(pr0)	vr4 = sel vmsk0 vr10, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "V0@(pr0)	vr5 = sel vmsk0 vr11, vr5;"
        "V1@(pr0)	vr10 = and.u32 vr2, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr12 = and.u32 vr4, r33;"
        "V1@(pr0)	vr11 = shr.u32 vr2, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr13 = shr.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = gteq.s32 vr10, vr12;"
        "V1@(pr0)	vr28 = mov.u32 r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr17 = sel vmsk0 vr28, r46;"
        "V1@(pr0)	vr17 = shl.u32 vr17, r32;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr10, vr17;"
        "V1@(pr0)	vr14 = sub.s32 vr10, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr15 = add.s32 vr13, r48;"
        "V1@(pr0)	vr13 = sel vmsk0 vr15, vr13;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = gteq.s32 vr11, vr13;"
        "V1@(pr0)	vr17 = sel vmsk0 vr28, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr17 = shl.u32 vr17, r32;"
        "}"
        "{"
        "V0@(pr0)	vr11 = or.u32 vr11, vr17;"
        "V1@(pr0)	vr15 = sub.s32 vr11, vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr14, vr15;"
        "}"
        "{"
        "V0@(pr0)	vr17 = add.s32 vr5, r48;"
        "V1@(pr0)	vr17 = sel vmsk0 vr17, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr11 = sub.s32 vr3, vr17;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = eq.s32 vr11, r46;"
        "V1@(pr0)	vr31 = count.u32 vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr30 = count.u32 vr10;"
        "V1@(pr0)	vr30 = add.s32 vr30, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr31 = sel vmsk0 vr31, vr30;"
        "V1@(pr0)	vr31 = sub.s32 vr31, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vmsk6 = ls.s32 vr31, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr30 = mov.u32 r32;"
        "V1@(pr0)	vr30 = sub.s32 vr30, vr31;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr29 = mov.u32 r44;"
        "V1@(pr0)	vr29 = shl.u32 vr29, vr30;"
        "}"
        "{"
        "V0@(pr0)	vr29 = and.u32 vr29, vr10;"
        "V1@(pr0)	vr29 = shr.u32 vr29, vr30;"
        "}"
        "{"
        "V1@(pr0)	vr13 = shl.u32 vr11, vr31;"
        "}"
        "{"
        "V0@(pr0)	vr13 = or.u32 vr13, vr29;"
        "V1@(pr0)	vr12 = shl.u32 vr10, vr31;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr30 = mov.u32 r32;"
        "V1@(pr0)	vr30 = sub.s32 vr31, vr30;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr14 = shl.u32 vr10, vr30;"
        "}"
        "{"
        "V0@(pr0)	vr12 = sel vmsk6 vr15, vr12;"
        "V1@(pr0)	vr13 = sel vmsk6 vr14, vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 128;"
        "V0@(pr0)	vmsk0 = ls.s32 vr11, r36;"
        "V1@(pr0)	vr10 = sel vmsk0 vr10, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr15 = sub.s32 vr0, vr31;"
        "}"
        "{"
        "V0@(pr0)	vr11 = sel vmsk0 vr11, vr13;"
        "V1@(pr0)	vr15 = sel vmsk0 vr0, vr15;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr12 = and.u32 vr2, r33;"
        "V1@(pr0)	vr13 = shr.u32 vr2, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr16 = and.u32 vr4, r33;"
        "V1@(pr0)	vr17 = shr.u32 vr4, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr28 = add.s32 vr12, vr16;"
        "V1@(pr0)	vr14 = shr.u32 vr28, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "V0@(pr0)	vr28 = and.u32 vr28, r32;"
        "}"
        "{"
        "V0@(pr0)	vr29 = add.s32 vr13, vr17;"
        "V1@(pr0)	vr29 = add.s32 vr29, vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr29, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "V0@(pr0)	vr29 = and.u32 vr29, r32;"
        "V1@(pr0)	vr29 = shl.u32 vr29, r33;"
        "}"
        "{"
        "V0@(pr0)	vr12 = or.u32 vr29, vr28;"
        "V1@(pr0)	vr13 = add.s32 vr3, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr13 = add.s32 vr13, vr14;"
        "V1@(pr0)	vr28 = and.u32 vr12, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V0@(pr0)	vr14 = and.u32 vr13, r48;"
        "V1@(pr0)	vr14 = shl.u32 vr14, r32;"
        "}"
        "{"
        "V1@(pr0)	vr16 = shr.u32 vr12, r48;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr16, vr14;"
        "V1@(pr0)	vr17 = shr.u32 vr13, r48;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr16, vr28;"
        "V1@(pr0)	vr14 = add.s32 vr0, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 256;"
        "V0@(pr0)	vr28 = and.u32 vr13, r36;"
        "V1@(pr0)	vmsk0 = eq.s32 vr28, r46;"
        "}"
        "{"
        "V0@(pr0)	vr12 = sel vmsk0 vr16, vr12;"
        "V1@(pr0)	vr13 = sel vmsk0 vr17, vr13;"
        "}"
        "{"
        "V0@(pr0)	vr14 = sel vmsk0 vr14, vr0;"
        "V1@(pr0)	vmsk1 = eq.s32 vr7, r46;"
        "}"
        "{"
        "V0@(pr0)	vr2 = sel vmsk1 vr10, vr12;"
        "V1@(pr0)	vr3 = sel vmsk1 vr11, vr13;"
        "}"
        "{"
        "V0@(pr0)	vr0 = sel vmsk1 vr15, vr14;"
        "V1@(pr0)	vr31 = mov.u32 r48;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr2, r48;"
        "V1@(pr0)	vmsk0 = lseq.s32 vr0, r46;"
        "}"
        "{"
        "V0@(pr0)	vr2 = sel vmsk0 vr2, vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 7;"
        "pseudo@0	@pseudo imm_1 = 3;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 65535;"
        "V0@(pr0)	vr17 = and.u32 vr2, r32;"
        "V1@(pr0)	vr31 = mov.u32 r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 29;"
        "V1@(pr0)	vr31 = shr.u32 vr31, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 29;"
        "V0@(pr0)	vr31 = and.u32 vr3, vr31;"
        "V1@(pr0)	vr31 = shl.u32 vr31, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3;"
        "V1@(pr0)	vr10 = shr.u32 vr2, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3;"
        "V0@(pr0)	vr10 = or.u32 vr10, vr31;"
        "V1@(pr0)	vr11 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 15;"
        "V0@(pr0)	vr10 = and.u32 vr10, r44;"
        "V1@(pr0)	vr11 = and.u32 vr11, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr28 = shl.u32 vr0, r32;"
        "}"
        "{"
        "V0@(pr0)	vr11 = or.u32 vr11, vr28;"
        "V1@(pr0)	vr11 = or.u32 vr11, vr6;"
        "}"
        "{"
        "V0@(pr0)	vr12 = add.s32 vr10, r48;"
        "V1@(pr0)	vr13 = and.u32 vr10, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "V0@(pr0)	vmsk0 = gt.s32 vr17, r32;"
        "V1@(pr0)	vmsk1 = eq.s32 vr17, r32;"
        "}"
        "{"
        "V0@(pr0)	vr13 = add.s32 vr10, vr13;"
        "V1@(pr0)	vr13 = sel vmsk1 vr10, vr13;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk0 vr13, vr12;"
        "V1@(pr0)	vr0 = mov.u32 vr10;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mov.u32 vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr10 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr11 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr12 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr13 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 5120;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 6144;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr16 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 7168;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr17 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8192;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = eq.s32 vr17, r46;"
        "V1@(pr0)	vmsk1 = eq.s32 vr16, r46;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 r46;"
        "V1@(pr0)	vr7 = sel vmsk0 vr28, r48;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr28, r48;"
        "V1@(pr0)	vr7 = add.s32 vr6, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vmsk0 = eq.s32 vr7, r32;"
        "V1@(pr0)	vr30 = sel vmsk0 vr0, vr10;"
        "}"
        "{"
        "V0@(pr0)	vr31 = sel vmsk0 vr1, vr11;"
        "V1@(pr0)	vr6 = and.u32 vr10, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr7 = and.u32 vr11, vr13;"
        "V1@(pr0)	vr5 = sel vmsk0 vr13, vr7;"
        "}"
        "{"
        "V0@(pr0)	vr4 = sel vmsk0 vr12, vr6;"
        "V1@(pr0)	vmsk0 = eq.s32 vr14, r46;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = eq.s32 vr15, r46;"
        "V1@(pr0)	vr6 = sel vmsk0 vr28, r48;"
        "}"
        "{"
        "V0@(pr0)	vr7 = sel vmsk1 vr28, r48;"
        "V1@(pr0)	vr7 = add.s32 vr6, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vmsk0 = eq.s32 vr7, r32;"
        "V1@(pr0)	vr31 = sel vmsk0 vr31, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr30 = sel vmsk0 vr30, r34;"
        "V1@(pr0)	vr7 = mov.u32 r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 15;"
        "V0@(pr0)	vr6 = mov.u32 r44;"
        "V1@(pr0)	vr7 = xor.u32 vr7, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr6 = xor.u32 vr6, r44;"
        "V1@(pr0)	vmsk0 = eq.s32 vr16, vr6;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mov.u32 vr7;"
        "V1@(pr0)	vr4 = mov.u32 vr6;"
        "}"
        "{"
        "V0@(pr0)	vmsk6 = eq.s32 vr14, vr6;"
        "V1@(pr0)	vmsk7 = eq.s32 vr15, vr7;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = eq.s32 vr17, vr7;"
        "V1@(pr0)	vr7 = sel vmsk0 vr28, r48;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr28, r48;"
        "V1@(pr0)	vr7 = add.s32 vr6, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vmsk0 = eq.s32 vr7, r32;"
        "V1@(pr0)	vr31 = sel vmsk0 vr31, vr13;"
        "}"
        "{"
        "V0@(pr0)	vr30 = sel vmsk0 vr30, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr7 = sel vmsk7 vr28, r48;"
        "V1@(pr0)	vr6 = sel vmsk6 vr28, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vr7 = add.s32 vr6, vr7;"
        "V1@(pr0)	vmsk7 = eq.s32 vr7, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr3 = or.u32 vr5, r36;"
        "V1@(pr0)	vr6 = xor.u32 vr10, vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32768;"
        "V0@(pr0)	vr7 = xor.u32 vr11, vr13;"
        "V1@(pr0)	vmsk0 = eq.s32 vr7, r36;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = eq.s32 vr6, r46;"
        "V1@(pr0)	vr7 = sel vmsk0 vr28, r48;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr28, r48;"
        "V1@(pr0)	vr7 = add.s32 vr6, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vmsk0 = eq.s32 vr7, r32;"
        "V1@(pr0)	vr7 = sel vmsk0 vr11, vr3;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk0 vr10, vr4;"
        "V1@(pr0)	vr30 = sel vmsk7 vr30, vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr31 = sel vmsk7 vr31, vr7;"
        "V1@(pr0)	vr7 = or.u32 vr13, r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk0 = eq.s32 vr16, r46;"
        "V1@(pr0)	vr6 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "V0@(pr0)	vr28 = and.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32752;"
        "V0@(pr0)	vmsk1 = gt.s32 vr6, r32;"
        "V1@(pr0)	vmsk2 = eq.s32 vr28, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk0 vr28, r48;"
        "V1@(pr0)	vr4 = sel vmsk2 vr28, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vr5 = add.s32 vr4, vr5;"
        "V1@(pr0)	vmsk2 = eq.s32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr7, vr31;"
        "V1@(pr0)	vr4 = sel vmsk2 vr12, vr30;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32752;"
        "V0@(pr0)	vmsk2 = eq.s32 vr6, r32;"
        "V1@(pr0)	vr4 = sel vmsk2 vr30, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr31, vr5;"
        "V1@(pr0)	vr31 = sel vmsk1 vr5, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr30 = sel vmsk1 vr4, vr12;"
        "V1@(pr0)	vr7 = or.u32 vr11, r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk0 = eq.s32 vr14, r46;"
        "V1@(pr0)	vr6 = shr.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "V0@(pr0)	vr28 = and.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32752;"
        "V0@(pr0)	vmsk1 = gt.s32 vr6, r32;"
        "V1@(pr0)	vmsk2 = eq.s32 vr28, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk0 vr28, r48;"
        "V1@(pr0)	vr4 = sel vmsk2 vr28, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vr5 = add.s32 vr4, vr5;"
        "V1@(pr0)	vmsk2 = eq.s32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr7, vr31;"
        "V1@(pr0)	vr4 = sel vmsk2 vr10, vr30;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32752;"
        "V0@(pr0)	vmsk2 = eq.s32 vr6, r32;"
        "V1@(pr0)	vr4 = sel vmsk2 vr30, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr31, vr5;"
        "V1@(pr0)	vr11 = sel vmsk1 vr5, vr7;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr4, vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 49152;"
        "pseudo@0	@pseudo imm_2 = 3;"
        "V0@(pr0)	vr5 = and.u32 vr11, r44;"
        "V1@(pr0)	vr1 = shl.u32 vr11, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 16383;"
        "V0@(pr0)	vr1 = and.u32 vr1, r44;"
        "V1@(pr0)	vr5 = or.u32 vr1, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 29;"
        "V1@(pr0)	vr1 = shr.u32 vr10, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = or.u32 vr1, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr4 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr13 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr4;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr5;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr10 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3;"
        "V0@(pr0)	vr15 = and.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr0 = and.u32 vr11, r44;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr11, vr11;"
        "V1@(pr0)	vr7 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr4 = mul.f32 vr3, vr11;"
        "V1@(pr0)	vmsk1 = eq.s32 vr13, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 51667;"
        "pseudo@0	@pseudo imm_1 = 12078;"
        "pseudo@0	@pseudo imm_2 = 12084;"
        "pseudo@0	@pseudo imm_3 = 45783;"
        "V0@(pr0)	vr1 = mul.f32 vr3, r44;"
        "V1@(pr0)	vr1 = add.f32 vr1, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 61211;"
        "pseudo@0	@pseudo imm_1 = 13880;"
        "V0@(pr0)	vr1 = mul.f32 vr3, vr1;"
        "V1@(pr0)	vr1 = add.f32 vr1, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3329;"
        "pseudo@0	@pseudo imm_1 = 47440;"
        "V0@(pr0)	vr1 = mul.f32 vr3, vr1;"
        "V1@(pr0)	vr1 = add.f32 vr1, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 34953;"
        "pseudo@0	@pseudo imm_1 = 15368;"
        "V0@(pr0)	vr1 = mul.f32 vr3, vr1;"
        "V1@(pr0)	vr5 = add.f32 vr1, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 43691;"
        "pseudo@0	@pseudo imm_1 = 48682;"
        "V0@(pr0)	vr1 = mul.f32 vr3, vr5;"
        "V1@(pr0)	vr1 = add.f32 vr1, r44;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.f32 vr11, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr2 = mul.f32 vr12, r50;"
        "V1@(pr0)	vr6 = sel vmsk1 vr6, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr1 = sub.f32 vr2, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr3, vr1;"
        "V1@(pr0)	vr1 = sub.f32 vr1, vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 43691;"
        "pseudo@0	@pseudo imm_1 = 48682;"
        "V0@(pr0)	vr2 = mul.f32 vr4, r44;"
        "V1@(pr0)	vr1 = sub.f32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr2 = cvtftoint.s32 vr11, r44;"
        "V1@(pr0)	vr1 = sub.f32 vr11, vr1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 12800;"
        "V0@(pr0)	vr6 = sel vmsk1 vr1, vr6;"
        "V1@(pr0)	vmsk1 = ls.s32 vr0, r36;"
        "}"
        "{"
        "V0@(pr0)	vr1 = sel vmsk1 vr6, vr11;"
        "V1@(pr0)	vmsk1 = eq.s32 vr2, r46;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr6, vr1;"
        "V1@(pr0)	vr11 = mov.u32 vr6;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mul.f32 vr11, r57;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V1@(pr0)	vmsk1 = eq.s32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr14 = sel vmsk1 vr11, vr17;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr4 = mul.f32 vr11, vr11;"
        "V1@(pr0)	vr7 = and.u32 vr11, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 39322;"
        "pseudo@0	@pseudo imm_1 = 16025;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vmsk1 = ls.s32 vr7, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 55118;"
        "pseudo@0	@pseudo imm_1 = 44359;"
        "pseudo@0	@pseudo imm_2 = 29942;"
        "pseudo@0	@pseudo imm_3 = 12559;"
        "V0@(pr0)	vr5 = mul.f32 vr4, r44;"
        "V1@(pr0)	vr5 = add.f32 vr5, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 62076;"
        "pseudo@0	@pseudo imm_1 = 46227;"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr5 = add.f32 vr5, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3329;"
        "pseudo@0	@pseudo imm_1 = 14288;"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr5 = add.f32 vr5, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2913;"
        "pseudo@0	@pseudo imm_1 = 47798;"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr5 = add.f32 vr5, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 43691;"
        "pseudo@0	@pseudo imm_1 = 15658;"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr5 = add.f32 vr5, r44;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr11, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr2 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr1 = sub.f32 vr2, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr2 = mul.f32 vr4, r50;"
        "V1@(pr0)	vr1 = sub.f32 vr2, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr2 = mov.u32 r49;"
        "V1@(pr0)	vr1 = sub.f32 vr2, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr0, vr1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16200;"
        "pseudo@0	@pseudo imm_1 = 256;"
        "V0@(pr0)	vmsk2 = gt.s32 vr7, r36;"
        "V1@(pr0)	vr1 = sub.s32 vr7, r37;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16016;"
        "V0@(pr0)	vr2 = mul.f32 vr4, r50;"
        "V1@(pr0)	vr1 = sel vmsk2 vr1, r44;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mov.u32 r49;"
        "V1@(pr0)	vr2 = sub.f32 vr2, vr1;"
        "}"
        "{"
        "V1@(pr0)	vr1 = sub.f32 vr3, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr11, vr12;"
        "V1@(pr0)	vr3 = add.f32 vr3, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr2 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr3 = sub.f32 vr3, vr2;"
        "}"
        "{"
        "V1@(pr0)	vr1 = sub.f32 vr1, vr3;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 12800;"
        "V0@(pr0)	vr6 = sel vmsk1 vr1, vr6;"
        "V1@(pr0)	vmsk1 = ls.s32 vr7, r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr2 = cvtftoint.s32 vr11, r44;"
        "}"
        "{"
        "V0@(pr0)	vr1 = sel vmsk1 vr6, r49;"
        "V1@(pr0)	vmsk1 = eq.s32 vr2, r46;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr6, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 vr6;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mul.f32 vr11, r57;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "V1@(pr0)	vmsk1 = eq.s32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr14 = sel vmsk1 vr14, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V1@(pr0)	vmsk1 = eq.s32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr14 = sel vmsk1 vr14, vr17;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 vr10;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 r46;"
        "V1@(pr0)	vr13 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr4 = mul.f32 vr11, vr11;"
        "V1@(pr0)	vr7 = and.u32 vr11, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 39322;"
        "pseudo@0	@pseudo imm_1 = 16025;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vmsk1 = ls.s32 vr7, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 55118;"
        "pseudo@0	@pseudo imm_1 = 44359;"
        "pseudo@0	@pseudo imm_2 = 29942;"
        "pseudo@0	@pseudo imm_3 = 12559;"
        "V0@(pr0)	vr5 = mul.f32 vr4, r44;"
        "V1@(pr0)	vr5 = add.f32 vr5, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 62076;"
        "pseudo@0	@pseudo imm_1 = 46227;"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr5 = add.f32 vr5, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3329;"
        "pseudo@0	@pseudo imm_1 = 14288;"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr5 = add.f32 vr5, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2913;"
        "pseudo@0	@pseudo imm_1 = 47798;"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr5 = add.f32 vr5, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 43691;"
        "pseudo@0	@pseudo imm_1 = 15658;"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr5 = add.f32 vr5, r44;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr11, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr2 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr1 = sub.f32 vr2, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr2 = mul.f32 vr4, r50;"
        "V1@(pr0)	vr1 = sub.f32 vr2, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr2 = mov.u32 r49;"
        "V1@(pr0)	vr1 = sub.f32 vr2, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr0, vr1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16200;"
        "pseudo@0	@pseudo imm_1 = 256;"
        "V0@(pr0)	vmsk2 = gt.s32 vr7, r36;"
        "V1@(pr0)	vr1 = sub.s32 vr7, r37;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16016;"
        "V0@(pr0)	vr2 = mul.f32 vr4, r50;"
        "V1@(pr0)	vr1 = sel vmsk2 vr1, r44;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mov.u32 r49;"
        "V1@(pr0)	vr2 = sub.f32 vr2, vr1;"
        "}"
        "{"
        "V1@(pr0)	vr1 = sub.f32 vr3, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr11, vr12;"
        "V1@(pr0)	vr3 = add.f32 vr3, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr2 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr3 = sub.f32 vr3, vr2;"
        "}"
        "{"
        "V1@(pr0)	vr1 = sub.f32 vr1, vr3;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 12800;"
        "V0@(pr0)	vr6 = sel vmsk1 vr1, vr6;"
        "V1@(pr0)	vmsk1 = ls.s32 vr7, r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr2 = cvtftoint.s32 vr11, r44;"
        "}"
        "{"
        "V0@(pr0)	vr1 = sel vmsk1 vr6, r49;"
        "V1@(pr0)	vmsk1 = eq.s32 vr2, r46;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr6, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr1 = and.u32 vr10, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4056;"
        "pseudo@0	@pseudo imm_1 = 16201;"
        "V0@(pr0)	vmsk1 = ls.f32 vr1, r44;"
        "}"
        "{"
        "V1@(pr0)	vr14 = sel vmsk1 vr14, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr1 = and.u32 vr10, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32640;"
        "V0@(pr0)	vmsk1 = ls.f32 vr1, r44;"
        "V1@(pr0)	vr15 = sub.f32 vr10, vr10;"
        "}"
        "{"
        "V1@(pr0)	vr14 = sel vmsk1 vr15, vr14;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "V0@(pr0)	vr14 = mov.u32 vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r1 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VL@(pr0)	vr11 = ld [vmem:r0,$1,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VL@(pr0)	vr10 = ld [vmem:r0,$1,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VL@(pr0)	vr12 = ld [vmem:r0,$1,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VL@(pr0)	vr13 = ld [vmem:r0,$1,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r1 = add.s32 r1, r44;"
        "S1@(pr0)	[smem:r3] = st r1;"
        "V1@(pr0)	vr28 = sub.f32 vr12, vr13;"
        "}"
        "{"
        "V0@(pr0)	vr0 = mul.f32 vr12, vr13;"
        "V1@(pr0)	vr29 = add.f32 vr12, vr13;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mul.f32 vr14, r57;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr17 = and.u32 vr29, r44;"
        "V1@(pr0)	vmsk1 = ls.f32 vr0, r46;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr17, r50;"
        "V1@(pr0)	vr2 = mov.u32 vr17;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 23007;"
        "pseudo@0	@pseudo imm_1 = 24375;"
        "V0@(pr0)	vr3 = mov.u32 r44;"
        "V1@(pr0)	vr2 = shr.u32 vr2, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16320;"
        "V0@(pr0)	vr2 = sub.s32 vr3, vr2;"
        "V1@(pr0)	vr4 = mov.u32 r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32768;"
        "V0@(pr0)	vr3 = mul.f32 vr2, vr2;"
        "V1@(pr0)	vr6 = and.u32 vr29, r36;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr1, vr3;"
        "V1@(pr0)	vr5 = sub.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr5, vr2;"
        "V1@(pr0)	vr2 = mov.u32 vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr2, vr2;"
        "V1@(pr0)	vmsk0 = eq.f32 vr29, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr1, vr3;"
        "V1@(pr0)	vr5 = sub.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr5, vr2;"
        "V1@(pr0)	vr2 = mov.u32 vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr3 = mul.f32 vr2, vr2;"
        "V1@(pr0)	vr7 = or.u32 vr6, r44;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr1, vr3;"
        "V1@(pr0)	vr5 = sub.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr5, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr5, vr5;"
        "V1@(pr0)	vr1 = or.u32 vr6, vr1;"
        "}"
        "{"
        "V1@(pr0)	vr30 = sel vmsk0 vr1, vr7;"
        "}"
        "{"
        "V0@(pr0)	vr30 = mul.f32 vr14, vr30;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr17 = and.u32 vr28, r44;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr17, r50;"
        "V1@(pr0)	vr2 = mov.u32 vr17;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 23007;"
        "pseudo@0	@pseudo imm_1 = 24375;"
        "V0@(pr0)	vr3 = mov.u32 r44;"
        "V1@(pr0)	vr2 = shr.u32 vr2, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16320;"
        "V0@(pr0)	vr2 = sub.s32 vr3, vr2;"
        "V1@(pr0)	vr4 = mov.u32 r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32768;"
        "V0@(pr0)	vr3 = mul.f32 vr2, vr2;"
        "V1@(pr0)	vr6 = and.u32 vr28, r36;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr1, vr3;"
        "V1@(pr0)	vr5 = sub.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr5, vr2;"
        "V1@(pr0)	vr2 = mov.u32 vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr2, vr2;"
        "V1@(pr0)	vmsk0 = eq.f32 vr28, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr1, vr3;"
        "V1@(pr0)	vr5 = sub.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr5, vr2;"
        "V1@(pr0)	vr2 = mov.u32 vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr3 = mul.f32 vr2, vr2;"
        "V1@(pr0)	vr7 = or.u32 vr6, r44;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr1, vr3;"
        "V1@(pr0)	vr5 = sub.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr5, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr5, vr5;"
        "V1@(pr0)	vr1 = or.u32 vr6, vr1;"
        "}"
        "{"
        "V1@(pr0)	vr31 = sel vmsk0 vr1, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32624;"
        "V0@(pr0)	vr31 = mul.f32 vr14, vr31;"
        "V1@(pr0)	vmsk0 = lseq.f32 vr10, r36;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = ls.f32 vr0, r46;"
        "}"
        "{"
        "V0@(pr0)	vr1 = sel vmsk0 vr31, vr28;"
        "V1@(pr0)	vr28 = sel vmsk1 vr1, vr28;"
        "}"
        "{"
        "V0@(pr0)	vr2 = sel vmsk0 vr30, vr29;"
        "V1@(pr0)	vr29 = sel vmsk1 vr29, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr10, r50;"
        "V1@(pr0)	vr2 = mov.u32 vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 23007;"
        "pseudo@0	@pseudo imm_1 = 24375;"
        "V0@(pr0)	vr3 = mov.u32 r44;"
        "V1@(pr0)	vr2 = shr.u32 vr2, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16320;"
        "V0@(pr0)	vr2 = sub.s32 vr3, vr2;"
        "V1@(pr0)	vr4 = mov.u32 r36;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr2, vr2;"
        "V1@(pr0)	vmsk0 = lseq.f32 vr10, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr1, vr3;"
        "V1@(pr0)	vr5 = sub.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr5, vr2;"
        "V1@(pr0)	vr2 = mov.u32 vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr2, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr1, vr3;"
        "V1@(pr0)	vr5 = sub.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr5, vr2;"
        "V1@(pr0)	vr2 = mov.u32 vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr2, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr1, vr3;"
        "V1@(pr0)	vr5 = sub.f32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr13 = mul.f32 vr5, vr2;"
        "V1@(pr0)	vr13 = sel vmsk0 vr13, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 28347;"
        "pseudo@0	@pseudo imm_1 = 16144;"
        "V0@(pr0)	vr0 = mul.f32 vr29, r44;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mul.f32 vr0, vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "pseudo@0	@pseudo imm_2 = 56168;"
        "pseudo@0	@pseudo imm_3 = 16438;"
        "V0@(pr0)	vr17 = and.u32 vr10, r44;"
        "V1@(pr0)	vmsk0 = gteq.f32 vr17, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 39095;"
        "pseudo@0	@pseudo imm_1 = 46014;"
        "pseudo@0	@pseudo imm_2 = 2075;"
        "pseudo@0	@pseudo imm_3 = 45359;"
        "V0@(pr0)	vr0 = mov.u32 r44;"
        "V1@(pr0)	vr0 = sel vmsk0 vr0, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 64274;"
        "pseudo@0	@pseudo imm_1 = 48527;"
        "pseudo@0	@pseudo imm_2 = 65464;"
        "pseudo@0	@pseudo imm_3 = 48527;"
        "V0@(pr0)	vr1 = mov.u32 r44;"
        "V1@(pr0)	vr1 = sel vmsk0 vr1, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 45516;"
        "pseudo@0	@pseudo imm_1 = 49081;"
        "pseudo@0	@pseudo imm_2 = 11669;"
        "pseudo@0	@pseudo imm_3 = 49178;"
        "V0@(pr0)	vr2 = mov.u32 r44;"
        "V1@(pr0)	vr2 = sel vmsk0 vr2, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 22431;"
        "pseudo@0	@pseudo imm_1 = 49396;"
        "pseudo@0	@pseudo imm_2 = 47698;"
        "pseudo@0	@pseudo imm_3 = 49583;"
        "V0@(pr0)	vr3 = mov.u32 r44;"
        "V1@(pr0)	vr3 = sel vmsk0 vr3, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 5942;"
        "pseudo@0	@pseudo imm_1 = 49459;"
        "pseudo@0	@pseudo imm_2 = 20754;"
        "pseudo@0	@pseudo imm_3 = 49768;"
        "V0@(pr0)	vr4 = mov.u32 r44;"
        "V1@(pr0)	vr4 = sel vmsk0 vr4, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 62477;"
        "pseudo@0	@pseudo imm_1 = 49230;"
        "pseudo@0	@pseudo imm_2 = 38245;"
        "pseudo@0	@pseudo imm_3 = 49659;"
        "V0@(pr0)	vr5 = mov.u32 r44;"
        "V1@(pr0)	vr5 = sel vmsk0 vr5, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 7256;"
        "pseudo@0	@pseudo imm_1 = 16631;"
        "pseudo@0	@pseudo imm_2 = 50570;"
        "pseudo@0	@pseudo imm_3 = 44360;"
        "V0@(pr0)	vmsk0 = gteq.f32 vr17, r44;"
        "V1@(pr0)	vr0 = sel vmsk0 vr0, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 48527;"
        "pseudo@0	@pseudo imm_2 = 7048;"
        "pseudo@0	@pseudo imm_3 = 49285;"
        "V0@(pr0)	vr1 = sel vmsk0 vr1, r44;"
        "V1@(pr0)	vr2 = sel vmsk0 vr2, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 22907;"
        "pseudo@0	@pseudo imm_1 = 49799;"
        "pseudo@0	@pseudo imm_2 = 40347;"
        "pseudo@0	@pseudo imm_3 = 50085;"
        "V0@(pr0)	vr3 = sel vmsk0 vr3, r44;"
        "V1@(pr0)	vr4 = sel vmsk0 vr4, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 14201;"
        "pseudo@0	@pseudo imm_1 = 50093;"
        "V0@(pr0)	vr5 = sel vmsk0 vr5, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16640;"
        "V0@(pr0)	vmsk0 = gteq.f32 vr17, r36;"
        "V1@(pr0)	vr0 = sel vmsk0 vr0, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 48528;"
        "pseudo@0	@pseudo imm_2 = 20102;"
        "pseudo@0	@pseudo imm_3 = 49409;"
        "V0@(pr0)	vr1 = sel vmsk0 vr1, r44;"
        "V1@(pr0)	vr2 = sel vmsk0 vr2, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 34836;"
        "pseudo@0	@pseudo imm_1 = 50048;"
        "pseudo@0	@pseudo imm_2 = 21366;"
        "pseudo@0	@pseudo imm_3 = 50459;"
        "V0@(pr0)	vr3 = sel vmsk0 vr3, r44;"
        "V1@(pr0)	vr4 = sel vmsk0 vr4, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 10330;"
        "pseudo@0	@pseudo imm_1 = 50596;"
        "V0@(pr0)	vr7 = mul.f32 vr10, vr10;"
        "V1@(pr0)	vr5 = sel vmsk0 vr5, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r1 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 6144;"
        "S1@(pr0)	r1 = sub.s32 r1, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VS@(pr0)	 [vmem:r0,$1,$255] = st vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VS@(pr0)	 [vmem:r0,$1,$255] = st vr1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VS@(pr0)	 [vmem:r0,$1,$255] = st vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VS@(pr0)	 [vmem:r0,$1,$255] = st vr3;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VS@(pr0)	 [vmem:r0,$1,$255] = st vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 5120;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VS@(pr0)	 [vmem:r0,$1,$255] = st vr0;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32640;"
        "V0@(pr0)	vr0 = and.u32 vr7, r44;"
        "V1@(pr0)	vmsk1 = eq.s32 vr0, r45;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr0, r50;"
        "V1@(pr0)	vr2 = mov.u32 vr0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 23007;"
        "pseudo@0	@pseudo imm_1 = 24375;"
        "V0@(pr0)	vr3 = mov.u32 r44;"
        "V1@(pr0)	vr2 = shr.u32 vr2, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16320;"
        "V0@(pr0)	vr2 = sub.s32 vr3, vr2;"
        "V1@(pr0)	vr4 = mov.u32 r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32768;"
        "V0@(pr0)	vr3 = mul.f32 vr2, vr2;"
        "V1@(pr0)	vr6 = and.u32 vr7, r36;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr1, vr3;"
        "V1@(pr0)	vr5 = sub.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr5, vr2;"
        "V1@(pr0)	vr2 = mov.u32 vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr2, vr2;"
        "V1@(pr0)	vmsk0 = eq.f32 vr7, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr1, vr3;"
        "V1@(pr0)	vr5 = sub.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr5, vr2;"
        "V1@(pr0)	vr2 = mov.u32 vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32640;"
        "V0@(pr0)	vr3 = mul.f32 vr2, vr2;"
        "V1@(pr0)	vr7 = or.u32 vr6, r44;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr1, vr3;"
        "V1@(pr0)	vr5 = sub.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr5, vr2;"
        "V1@(pr0)	vr2 = or.u32 vr6, r46;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr5, vr5;"
        "V1@(pr0)	vr1 = or.u32 vr6, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr7 = sel vmsk0 vr1, vr7;"
        "V1@(pr0)	vr7 = sel vmsk1 vr7, vr2;"
        "}"
        "{"
        "S1@(pr0)	r1 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VL@(pr0)	vr5 = ld [vmem:r0,$1,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VL@(pr0)	vr1 = ld [vmem:r0,$1,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VL@(pr0)	vr2 = ld [vmem:r0,$1,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VL@(pr0)	vr3 = ld [vmem:r0,$1,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VL@(pr0)	vr4 = ld [vmem:r0,$1,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 5120;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VL@(pr0)	vr0 = ld [vmem:r0,$1,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 6144;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r1 = add.s32 r1, r44;"
        "S1@(pr0)	[smem:r3] = st r1;"
        "}"
        "{"
        "V0@(pr0)	vr6 = mul.f32 vr5, vr7;"
        "V1@(pr0)	vr6 = add.f32 vr6, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr6 = mul.f32 vr6, vr7;"
        "V1@(pr0)	vr6 = add.f32 vr6, vr3;"
        "}"
        "{"
        "V0@(pr0)	vr6 = mul.f32 vr6, vr7;"
        "V1@(pr0)	vr6 = add.f32 vr6, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr6 = mul.f32 vr6, vr7;"
        "V1@(pr0)	vr6 = add.f32 vr6, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr6 = mul.f32 vr6, vr7;"
        "V1@(pr0)	vr6 = add.f32 vr6, vr0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 56168;"
        "pseudo@0	@pseudo imm_3 = 16438;"
        "V0@(pr0)	vmsk0 = gteq.f32 vr17, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 49965;"
        "pseudo@0	@pseudo imm_1 = 16817;"
        "pseudo@0	@pseudo imm_2 = 27796;"
        "pseudo@0	@pseudo imm_3 = 16911;"
        "V0@(pr0)	vr0 = mov.u32 r44;"
        "V1@(pr0)	vr0 = sel vmsk0 vr0, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 13552;"
        "pseudo@0	@pseudo imm_1 = 17160;"
        "pseudo@0	@pseudo imm_2 = 49610;"
        "pseudo@0	@pseudo imm_3 = 17332;"
        "V0@(pr0)	vr1 = mov.u32 r44;"
        "V1@(pr0)	vr1 = sel vmsk0 vr1, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 15410;"
        "pseudo@0	@pseudo imm_1 = 17287;"
        "pseudo@0	@pseudo imm_2 = 13171;"
        "pseudo@0	@pseudo imm_3 = 17557;"
        "V0@(pr0)	vr2 = mov.u32 r44;"
        "V1@(pr0)	vr2 = sel vmsk0 vr2, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 57370;"
        "pseudo@0	@pseudo imm_1 = 17177;"
        "pseudo@0	@pseudo imm_2 = 65510;"
        "pseudo@0	@pseudo imm_3 = 17548;"
        "V0@(pr0)	vr3 = mov.u32 r44;"
        "V1@(pr0)	vr3 = sel vmsk0 vr3, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 34202;"
        "pseudo@0	@pseudo imm_1 = 16746;"
        "pseudo@0	@pseudo imm_2 = 38072;"
        "pseudo@0	@pseudo imm_3 = 17197;"
        "V0@(pr0)	vr4 = mov.u32 r44;"
        "V1@(pr0)	vr4 = sel vmsk0 vr4, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 7256;"
        "pseudo@0	@pseudo imm_1 = 16631;"
        "pseudo@0	@pseudo imm_2 = 1032;"
        "pseudo@0	@pseudo imm_3 = 17011;"
        "V0@(pr0)	vmsk0 = gteq.f32 vr17, r44;"
        "V1@(pr0)	vr0 = sel vmsk0 vr0, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 26643;"
        "pseudo@0	@pseudo imm_1 = 17539;"
        "pseudo@0	@pseudo imm_2 = 55236;"
        "pseudo@0	@pseudo imm_3 = 17850;"
        "V0@(pr0)	vr1 = sel vmsk0 vr1, r44;"
        "V1@(pr0)	vr2 = sel vmsk0 vr2, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 26056;"
        "pseudo@0	@pseudo imm_1 = 17942;"
        "pseudo@0	@pseudo imm_2 = 24814;"
        "pseudo@0	@pseudo imm_3 = 17686;"
        "V0@(pr0)	vr3 = sel vmsk0 vr3, r44;"
        "V1@(pr0)	vr4 = sel vmsk0 vr4, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16640;"
        "pseudo@0	@pseudo imm_2 = 4504;"
        "pseudo@0	@pseudo imm_3 = 17129;"
        "V0@(pr0)	vmsk0 = gteq.f32 vr17, r36;"
        "V1@(pr0)	vr0 = sel vmsk0 vr0, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 39915;"
        "pseudo@0	@pseudo imm_1 = 17775;"
        "pseudo@0	@pseudo imm_2 = 38363;"
        "pseudo@0	@pseudo imm_3 = 18206;"
        "V0@(pr0)	vr1 = sel vmsk0 vr1, r44;"
        "V1@(pr0)	vr2 = sel vmsk0 vr2, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2172;"
        "pseudo@0	@pseudo imm_1 = 18404;"
        "pseudo@0	@pseudo imm_2 = 3002;"
        "pseudo@0	@pseudo imm_3 = 18234;"
        "V0@(pr0)	vr3 = sel vmsk0 vr3, r44;"
        "V1@(pr0)	vr4 = sel vmsk0 vr4, r45;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr7;"
        "V1@(pr0)	vr5 = add.f32 vr5, vr3;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr5, vr7;"
        "V1@(pr0)	vr5 = add.f32 vr5, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr5, vr7;"
        "V1@(pr0)	vr5 = add.f32 vr5, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr5, vr7;"
        "V1@(pr0)	vr5 = add.f32 vr5, vr0;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr5, vr7;"
        "V1@(pr0)	vr5 = add.f32 vr5, r49;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32640;"
        "V0@(pr0)	vr0 = and.u32 vr5, r44;"
        "V1@(pr0)	vmsk1 = eq.s32 vr0, r45;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr0, r50;"
        "V1@(pr0)	vr2 = mov.u32 vr0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 23007;"
        "pseudo@0	@pseudo imm_1 = 24375;"
        "V0@(pr0)	vr3 = mov.u32 r44;"
        "V1@(pr0)	vr2 = shr.u32 vr2, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16320;"
        "V0@(pr0)	vr2 = sub.s32 vr3, vr2;"
        "V1@(pr0)	vr4 = mov.u32 r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32768;"
        "V0@(pr0)	vr3 = mul.f32 vr2, vr2;"
        "V1@(pr0)	vr17 = and.u32 vr5, r36;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mul.f32 vr1, vr3;"
        "V1@(pr0)	vr15 = sub.f32 vr4, vr15;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mul.f32 vr15, vr2;"
        "V1@(pr0)	vr2 = mov.u32 vr15;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr2, vr2;"
        "V1@(pr0)	vmsk0 = eq.f32 vr5, r46;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mul.f32 vr1, vr3;"
        "V1@(pr0)	vr15 = sub.f32 vr4, vr15;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mul.f32 vr15, vr2;"
        "V1@(pr0)	vr2 = mov.u32 vr15;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr3 = mul.f32 vr2, vr2;"
        "V1@(pr0)	vr7 = or.u32 vr17, r44;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mul.f32 vr1, vr3;"
        "V1@(pr0)	vr15 = sub.f32 vr4, vr15;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mul.f32 vr15, vr2;"
        "V1@(pr0)	vr2 = or.u32 vr17, r46;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr15, vr15;"
        "V1@(pr0)	vr1 = or.u32 vr17, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk0 vr1, vr7;"
        "V1@(pr0)	vr7 = sel vmsk1 vr7, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr5, vr6;"
        "V1@(pr0)	vr30 = add.f32 vr5, r49;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "pseudo@0	@pseudo imm_2 = 56168;"
        "pseudo@0	@pseudo imm_3 = 16438;"
        "V0@(pr0)	vr17 = and.u32 vr10, r44;"
        "V1@(pr0)	vmsk0 = gteq.f32 vr17, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 35291;"
        "pseudo@0	@pseudo imm_1 = 13345;"
        "pseudo@0	@pseudo imm_2 = 26651;"
        "pseudo@0	@pseudo imm_3 = 12694;"
        "V0@(pr0)	vr0 = mov.u32 r44;"
        "V1@(pr0)	vr0 = sel vmsk0 vr0, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 63018;"
        "pseudo@0	@pseudo imm_1 = 15765;"
        "pseudo@0	@pseudo imm_2 = 65392;"
        "pseudo@0	@pseudo imm_3 = 15765;"
        "V0@(pr0)	vr1 = mov.u32 r44;"
        "V1@(pr0)	vr1 = sel vmsk0 vr1, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 50367;"
        "pseudo@0	@pseudo imm_1 = 16383;"
        "pseudo@0	@pseudo imm_2 = 2019;"
        "pseudo@0	@pseudo imm_3 = 16470;"
        "V0@(pr0)	vr2 = mov.u32 r44;"
        "V1@(pr0)	vr2 = sel vmsk0 vr2, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 60925;"
        "pseudo@0	@pseudo imm_1 = 16743;"
        "pseudo@0	@pseudo imm_2 = 31941;"
        "pseudo@0	@pseudo imm_3 = 16938;"
        "V0@(pr0)	vr3 = mov.u32 r44;"
        "V1@(pr0)	vr3 = sel vmsk0 vr3, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 21617;"
        "pseudo@0	@pseudo imm_1 = 16893;"
        "pseudo@0	@pseudo imm_2 = 52959;"
        "pseudo@0	@pseudo imm_3 = 17194;"
        "V0@(pr0)	vr4 = mov.u32 r44;"
        "V1@(pr0)	vr4 = sel vmsk0 vr4, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1420;"
        "pseudo@0	@pseudo imm_1 = 16770;"
        "pseudo@0	@pseudo imm_2 = 48100;"
        "pseudo@0	@pseudo imm_3 = 17190;"
        "V0@(pr0)	vr5 = mov.u32 r44;"
        "V1@(pr0)	vr5 = sel vmsk0 vr5, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 7256;"
        "pseudo@0	@pseudo imm_1 = 16631;"
        "pseudo@0	@pseudo imm_2 = 60537;"
        "pseudo@0	@pseudo imm_3 = 11681;"
        "V0@(pr0)	vmsk0 = gteq.f32 vr17, r44;"
        "V1@(pr0)	vr0 = sel vmsk0 vr0, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 15765;"
        "pseudo@0	@pseudo imm_2 = 48518;"
        "pseudo@0	@pseudo imm_3 = 16570;"
        "V0@(pr0)	vr1 = sel vmsk0 vr1, r44;"
        "V1@(pr0)	vr2 = sel vmsk0 vr2, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 7312;"
        "pseudo@0	@pseudo imm_1 = 17159;"
        "pseudo@0	@pseudo imm_2 = 26573;"
        "pseudo@0	@pseudo imm_3 = 17536;"
        "V0@(pr0)	vr3 = sel vmsk0 vr3, r44;"
        "V1@(pr0)	vr4 = sel vmsk0 vr4, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48971;"
        "pseudo@0	@pseudo imm_1 = 17656;"
        "V0@(pr0)	vr5 = sel vmsk0 vr5, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16640;"
        "V0@(pr0)	vmsk0 = gteq.f32 vr17, r36;"
        "V1@(pr0)	vr0 = sel vmsk0 vr0, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 15766;"
        "pseudo@0	@pseudo imm_2 = 19091;"
        "pseudo@0	@pseudo imm_3 = 16700;"
        "V0@(pr0)	vr1 = sel vmsk0 vr1, r44;"
        "V1@(pr0)	vr2 = sel vmsk0 vr2, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 27417;"
        "pseudo@0	@pseudo imm_1 = 17419;"
        "pseudo@0	@pseudo imm_2 = 27850;"
        "pseudo@0	@pseudo imm_3 = 17930;"
        "V0@(pr0)	vr3 = sel vmsk0 vr3, r44;"
        "V1@(pr0)	vr4 = sel vmsk0 vr4, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 38560;"
        "pseudo@0	@pseudo imm_1 = 18192;"
        "V0@(pr0)	vr16 = mul.f32 vr10, vr10;"
        "V1@(pr0)	vr5 = sel vmsk0 vr5, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r1 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 6144;"
        "S1@(pr0)	r1 = sub.s32 r1, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VS@(pr0)	 [vmem:r0,$1,$255] = st vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VS@(pr0)	 [vmem:r0,$1,$255] = st vr1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VS@(pr0)	 [vmem:r0,$1,$255] = st vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VS@(pr0)	 [vmem:r0,$1,$255] = st vr3;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VS@(pr0)	 [vmem:r0,$1,$255] = st vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 5120;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VS@(pr0)	 [vmem:r0,$1,$255] = st vr0;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr0 = and.u32 vr16, r44;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr0, r50;"
        "V1@(pr0)	vr2 = mov.u32 vr0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 23007;"
        "pseudo@0	@pseudo imm_1 = 24375;"
        "V0@(pr0)	vr3 = mov.u32 r44;"
        "V1@(pr0)	vr2 = shr.u32 vr2, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16320;"
        "V0@(pr0)	vr2 = sub.s32 vr3, vr2;"
        "V1@(pr0)	vr4 = mov.u32 r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32768;"
        "V0@(pr0)	vr3 = mul.f32 vr2, vr2;"
        "V1@(pr0)	vr6 = and.u32 vr16, r36;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr1, vr3;"
        "V1@(pr0)	vr5 = sub.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr5, vr2;"
        "V1@(pr0)	vr2 = mov.u32 vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr2, vr2;"
        "V1@(pr0)	vmsk0 = eq.f32 vr16, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr1, vr3;"
        "V1@(pr0)	vr5 = sub.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr5, vr2;"
        "V1@(pr0)	vr2 = mov.u32 vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr3 = mul.f32 vr2, vr2;"
        "V1@(pr0)	vr7 = or.u32 vr6, r44;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr1, vr3;"
        "V1@(pr0)	vr5 = sub.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr5, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr5, vr5;"
        "V1@(pr0)	vr1 = or.u32 vr6, vr1;"
        "}"
        "{"
        "V1@(pr0)	vr16 = sel vmsk0 vr1, vr7;"
        "}"
        "{"
        "S1@(pr0)	r1 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VL@(pr0)	vr5 = ld [vmem:r0,$1,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VL@(pr0)	vr1 = ld [vmem:r0,$1,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VL@(pr0)	vr2 = ld [vmem:r0,$1,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VL@(pr0)	vr3 = ld [vmem:r0,$1,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VL@(pr0)	vr4 = ld [vmem:r0,$1,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 5120;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VL@(pr0)	vr0 = ld [vmem:r0,$1,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 6144;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r1 = add.s32 r1, r44;"
        "S1@(pr0)	[smem:r3] = st r1;"
        "}"
        "{"
        "V0@(pr0)	vr6 = mul.f32 vr5, vr16;"
        "V1@(pr0)	vr6 = add.f32 vr6, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr6 = mul.f32 vr6, vr16;"
        "V1@(pr0)	vr6 = add.f32 vr6, vr3;"
        "}"
        "{"
        "V0@(pr0)	vr6 = mul.f32 vr6, vr16;"
        "V1@(pr0)	vr6 = add.f32 vr6, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr6 = mul.f32 vr6, vr16;"
        "V1@(pr0)	vr6 = add.f32 vr6, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr6 = mul.f32 vr6, vr16;"
        "V1@(pr0)	vr6 = add.f32 vr6, vr0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 56168;"
        "pseudo@0	@pseudo imm_3 = 16438;"
        "V0@(pr0)	vmsk0 = gteq.f32 vr17, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 60600;"
        "pseudo@0	@pseudo imm_1 = 16882;"
        "pseudo@0	@pseudo imm_2 = 2326;"
        "pseudo@0	@pseudo imm_3 = 16963;"
        "V0@(pr0)	vr0 = mov.u32 r44;"
        "V1@(pr0)	vr0 = sel vmsk0 vr0, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 44175;"
        "pseudo@0	@pseudo imm_1 = 17286;"
        "pseudo@0	@pseudo imm_2 = 27676;"
        "pseudo@0	@pseudo imm_3 = 17457;"
        "V0@(pr0)	vr1 = mov.u32 r44;"
        "V1@(pr0)	vr1 = sel vmsk0 vr1, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 12841;"
        "pseudo@0	@pseudo imm_1 = 17491;"
        "pseudo@0	@pseudo imm_2 = 33375;"
        "pseudo@0	@pseudo imm_3 = 17767;"
        "V0@(pr0)	vr2 = mov.u32 r44;"
        "V1@(pr0)	vr2 = sel vmsk0 vr2, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48101;"
        "pseudo@0	@pseudo imm_1 = 17500;"
        "pseudo@0	@pseudo imm_2 = 58215;"
        "pseudo@0	@pseudo imm_3 = 17865;"
        "V0@(pr0)	vr3 = mov.u32 r44;"
        "V1@(pr0)	vr3 = sel vmsk0 vr3, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 43672;"
        "pseudo@0	@pseudo imm_1 = 17236;"
        "pseudo@0	@pseudo imm_2 = 17751;"
        "pseudo@0	@pseudo imm_3 = 17693;"
        "V0@(pr0)	vr4 = mov.u32 r44;"
        "V1@(pr0)	vr4 = sel vmsk0 vr4, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 62296;"
        "pseudo@0	@pseudo imm_1 = 49321;"
        "pseudo@0	@pseudo imm_2 = 16217;"
        "pseudo@0	@pseudo imm_3 = 49941;"
        "V0@(pr0)	vr5 = mov.u32 r44;"
        "V1@(pr0)	vr5 = sel vmsk0 vr5, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 7256;"
        "pseudo@0	@pseudo imm_1 = 16631;"
        "pseudo@0	@pseudo imm_2 = 36256;"
        "pseudo@0	@pseudo imm_3 = 17061;"
        "V0@(pr0)	vmsk0 = gteq.f32 vr17, r44;"
        "V1@(pr0)	vr0 = sel vmsk0 vr0, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 56583;"
        "pseudo@0	@pseudo imm_1 = 17665;"
        "pseudo@0	@pseudo imm_2 = 16020;"
        "pseudo@0	@pseudo imm_3 = 18067;"
        "V0@(pr0)	vr1 = sel vmsk0 vr1, r44;"
        "V1@(pr0)	vr2 = sel vmsk0 vr2, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 44829;"
        "pseudo@0	@pseudo imm_1 = 18269;"
        "pseudo@0	@pseudo imm_2 = 35009;"
        "pseudo@0	@pseudo imm_3 = 18188;"
        "V0@(pr0)	vr3 = sel vmsk0 vr3, r44;"
        "V1@(pr0)	vr4 = sel vmsk0 vr4, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 21182;"
        "pseudo@0	@pseudo imm_1 = 50599;"
        "V0@(pr0)	vr5 = sel vmsk0 vr5, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16640;"
        "pseudo@0	@pseudo imm_2 = 50858;"
        "pseudo@0	@pseudo imm_3 = 17187;"
        "V0@(pr0)	vmsk0 = gteq.f32 vr17, r36;"
        "V1@(pr0)	vr0 = sel vmsk0 vr0, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4802;"
        "pseudo@0	@pseudo imm_1 = 17917;"
        "pseudo@0	@pseudo imm_2 = 12947;"
        "pseudo@0	@pseudo imm_3 = 18443;"
        "V0@(pr0)	vr1 = sel vmsk0 vr1, r44;"
        "V1@(pr0)	vr2 = sel vmsk0 vr2, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 7892;"
        "pseudo@0	@pseudo imm_1 = 18756;"
        "pseudo@0	@pseudo imm_2 = 13145;"
        "pseudo@0	@pseudo imm_3 = 18765;"
        "V0@(pr0)	vr3 = sel vmsk0 vr3, r44;"
        "V1@(pr0)	vr4 = sel vmsk0 vr4, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 60265;"
        "pseudo@0	@pseudo imm_1 = 51367;"
        "V1@(pr0)	vr5 = sel vmsk0 vr5, r44;"
        "}"
        "{"
        "V0@(pr0)	vr7 = mul.f32 vr5, vr16;"
        "V1@(pr0)	vr7 = add.f32 vr7, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr7 = mul.f32 vr7, vr16;"
        "V1@(pr0)	vr7 = add.f32 vr7, vr3;"
        "}"
        "{"
        "V0@(pr0)	vr7 = mul.f32 vr7, vr16;"
        "V1@(pr0)	vr7 = add.f32 vr7, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr7 = mul.f32 vr7, vr16;"
        "V1@(pr0)	vr7 = add.f32 vr7, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr7 = mul.f32 vr7, vr16;"
        "V1@(pr0)	vr7 = add.f32 vr7, vr0;"
        "}"
        "{"
        "V0@(pr0)	vr7 = mul.f32 vr7, vr16;"
        "V1@(pr0)	vr7 = add.f32 vr7, r49;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32640;"
        "V0@(pr0)	vr0 = and.u32 vr7, r44;"
        "V1@(pr0)	vmsk1 = eq.s32 vr0, r45;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr0, r50;"
        "V1@(pr0)	vr2 = mov.u32 vr0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 23007;"
        "pseudo@0	@pseudo imm_1 = 24375;"
        "V0@(pr0)	vr3 = mov.u32 r44;"
        "V1@(pr0)	vr2 = shr.u32 vr2, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16320;"
        "V0@(pr0)	vr2 = sub.s32 vr3, vr2;"
        "V1@(pr0)	vr4 = mov.u32 r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32768;"
        "V0@(pr0)	vr3 = mul.f32 vr2, vr2;"
        "V1@(pr0)	vr17 = and.u32 vr7, r36;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr1, vr3;"
        "V1@(pr0)	vr5 = sub.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr5, vr2;"
        "V1@(pr0)	vr2 = mov.u32 vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr2, vr2;"
        "V1@(pr0)	vmsk0 = eq.f32 vr7, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr1, vr3;"
        "V1@(pr0)	vr5 = sub.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr5, vr2;"
        "V1@(pr0)	vr2 = mov.u32 vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32640;"
        "V0@(pr0)	vr3 = mul.f32 vr2, vr2;"
        "V1@(pr0)	vr7 = or.u32 vr17, r44;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr1, vr3;"
        "V1@(pr0)	vr5 = sub.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr5, vr2;"
        "V1@(pr0)	vr2 = or.u32 vr17, r46;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr5, vr5;"
        "V1@(pr0)	vr1 = or.u32 vr17, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr15 = sel vmsk0 vr1, vr7;"
        "V1@(pr0)	vr15 = sel vmsk1 vr15, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr16 = and.u32 vr10, r44;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr16, r50;"
        "V1@(pr0)	vr2 = mov.u32 vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 23007;"
        "pseudo@0	@pseudo imm_1 = 24375;"
        "V0@(pr0)	vr3 = mov.u32 r44;"
        "V1@(pr0)	vr2 = shr.u32 vr2, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16320;"
        "V0@(pr0)	vr2 = sub.s32 vr3, vr2;"
        "V1@(pr0)	vr4 = mov.u32 r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32768;"
        "V0@(pr0)	vr3 = mul.f32 vr2, vr2;"
        "V1@(pr0)	vr17 = and.u32 vr10, r36;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr1, vr3;"
        "V1@(pr0)	vr5 = sub.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr5, vr2;"
        "V1@(pr0)	vr2 = mov.u32 vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr2, vr2;"
        "V1@(pr0)	vmsk0 = eq.f32 vr10, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr1, vr3;"
        "V1@(pr0)	vr5 = sub.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr5, vr2;"
        "V1@(pr0)	vr2 = mov.u32 vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr3 = mul.f32 vr2, vr2;"
        "V1@(pr0)	vr7 = or.u32 vr17, r44;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr1, vr3;"
        "V1@(pr0)	vr5 = sub.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr5, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr5, vr5;"
        "V1@(pr0)	vr1 = or.u32 vr17, vr1;"
        "}"
        "{"
        "V1@(pr0)	vr16 = sel vmsk0 vr1, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48640;"
        "V0@(pr0)	vr15 = mul.f32 vr15, vr6;"
        "V1@(pr0)	vr15 = add.f32 vr15, r36;"
        "}"
        "{"
        "V0@(pr0)	vr31 = mul.f32 vr16, vr15;"
        "}"
        "{"
        "V0@(pr0)	vr30 = mul.f32 vr30, vr29;"
        "}"
        "{"
        "V0@(pr0)	vr31 = mul.f32 vr31, vr28;"
        "V1@(pr0)	vr0 = sub.f32 vr30, vr31;"
        "}"
        "{"
        "V0@(pr0)	vr0 = mul.f32 vr0, vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 28347;"
        "pseudo@0	@pseudo imm_1 = 16144;"
        "V0@(pr0)	vr13 = mul.f32 vr0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 23552;"
        "V0@(pr0)	vr16 = and.u32 vr10, r44;"
        "V1@(pr0)	vmsk0 = gt.f32 vr16, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 16384;"
        "V0@(pr0)	vr12 = sel vmsk0 vr13, vr12;"
        "V1@(pr0)	vmsk0 = gt.f32 vr16, r45;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk0 vr11, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr2 = mov.u32 vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r1 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VL@(pr0)	vr11 = ld [vmem:r0,$1,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VL@(pr0)	vr10 = ld [vmem:r0,$1,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r1 = add.s32 r1, r44;"
        "S1@(pr0)	[smem:r3] = st r1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r1 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r1 = sub.s32 r1, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VS@(pr0)	 [vmem:r0,$1,$255] = st vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VS@(pr0)	 [vmem:r0,$1,$255] = st vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VS@(pr0)	 [vmem:r0,$1,$255] = st vr2;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r1;"
        "V0@(pr0)	vr10 = mov.u32 vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r1 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r1 = sub.s32 r1, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VS@(pr0)	 [vmem:r0,$1,$255] = st vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "S1@(pr0)	[smem:r3] = st r1;"
        "V0@(pr0)	vr10 = and.u32 vr10, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr10;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr1 = and.u32 vr10, r44;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 17136;"
        "V1@(pr0)	vmsk4 = ls.f32 vr2, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr10;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 29;"
        "V1@(pr0)	vr11 = shl.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 49152;"
        "pseudo@0	@pseudo imm_2 = 3;"
        "V0@(pr0)	vr1 = and.u32 vr10, r44;"
        "V1@(pr0)	vr2 = shr.u32 vr10, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 2047;"
        "V0@(pr0)	vr2 = and.u32 vr2, r44;"
        "V1@(pr0)	vr1 = or.u32 vr2, vr1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 2048;"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 14336;"
        "V0@(pr0)	vr2 = and.u32 vr10, r44;"
        "V1@(pr0)	vr3 = or.u32 vr1, r45;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = eq.s32 vr2, r46;"
        "V1@(pr0)	vr1 = sel vmsk1 vr3, vr1;"
        "}"
        "{"
        "V1@(pr0)	vr10 = mov.u32 vr1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr11;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 24368;"
        "pseudo@0	@pseudo imm_1 = 16740;"
        "pseudo@0	@pseudo imm_2 = 51331;"
        "pseudo@0	@pseudo imm_3 = 28105;"
        "V0@(pr0)	vr12 = mov.u32 r44;"
        "V1@(pr0)	vr13 = mov.u32 r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr13;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr1 = and.u32 vr10, r44;"
        "V1@(pr0)	vr2 = and.u32 vr12, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "V0@(pr0)	vmsk1 = ls.f32 vr1, r44;"
        "V1@(pr0)	vmsk2 = ls.f32 vr2, r44;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, r46;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, r46;"
        "}"
        "{"
        "V0@(pr0)	vr12 = sel vmsk2 vr12, r46;"
        "V1@(pr0)	vr13 = sel vmsk2 vr13, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr6 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr7 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 15;"
        "V0@(pr0)	vr10 = and.u32 vr10, r44;"
        "V1@(pr0)	vr12 = and.u32 vr12, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "V0@(pr0)	vr10 = or.u32 vr10, r44;"
        "V1@(pr0)	vr12 = or.u32 vr12, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 11;"
        "V1@(pr0)	vr12 = shl.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 21;"
        "V1@(pr0)	vr1 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 11;"
        "V0@(pr0)	vr12 = or.u32 vr12, vr1;"
        "V1@(pr0)	vr13 = shl.u32 vr13, r32;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr13, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = and.u32 vr12, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr13, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr12, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = and.u32 vr12, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr12, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 r46;"
        "V1@(pr0)	vr10 = mov.u32 vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr10;"
        "V1@(pr0)	vr17 = mov.u32 vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = or.u32 vr1, vr2;"
        "V1@(pr0)	vr14 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr12 = mov.u32 r46;"
        "V1@(pr0)	vr13 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "V1@(pr0)	vr15 = shr.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 vr15;"
        "V1@(pr0)	vr29 = mov.u32 vr14;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr15;"
        "V1@(pr0)	vr15 = mov.u32 vr0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr16;"
        "V1@(pr0)	vr11 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr12;"
        "V1@(pr0)	vr17 = mov.u32 vr13;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr29;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 vr15;"
        "V1@(pr0)	vr29 = mov.u32 vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr16;"
        "V1@(pr0)	vr13 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr10;"
        "V1@(pr0)	vr17 = mov.u32 vr11;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr28;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr16;"
        "V1@(pr0)	vr11 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr29 = shl.u32 vr29, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr12;"
        "V1@(pr0)	vr17 = mov.u32 vr13;"
        "}"
        "{"
        "V0@(pr0)	vr30 = mov.u32 vr14;"
        "V1@(pr0)	vr14 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr31 = mov.u32 vr17;"
        "V1@(pr0)	vr17 = mov.u32 vr16;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr11;"
        "V1@(pr0)	vr15 = mov.u32 vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr16;"
        "V1@(pr0)	vr11 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr13;"
        "V1@(pr0)	vr13 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr28;"
        "V1@(pr0)	vr17 = mov.u32 vr29;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr30;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr13;"
        "V1@(pr0)	vr15 = mov.u32 vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr13 = mov.u32 vr17;"
        "V1@(pr0)	vr12 = mov.u32 vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "V0@(pr0)	vr3 = xor.u32 vr6, vr7;"
        "V1@(pr0)	vr3 = and.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr3 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 2047;"
        "pseudo@0	@pseudo imm_3 = 0;"
        "V0@(pr0)	vr7 = and.u32 vr7, r45;"
        "V1@(pr0)	vr6 = and.u32 vr6, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 1023;"
        "pseudo@0	@pseudo imm_3 = 0;"
        "V0@(pr0)	vr6 = add.s32 vr6, vr7;"
        "V1@(pr0)	vr6 = sub.s32 vr6, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 16;"
        "V0@(pr0)	vr1 = and.u32 vr10, r45;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr6, r48;"
        "V1@(pr0)	vmsk1 = eq.s32 vr1, r46;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr2, vr6;"
        "}"
        "{"
        "V1@(pr0)	vr2 = shl.u32 vr10, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V1@(pr0)	vr5 = shr.u32 vr11, r32;"
        "}"
        "{"
        "V0@(pr0)	vr2 = or.u32 vr2, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V1@(pr0)	vr5 = shl.u32 vr13, r32;"
        "}"
        "{"
        "V1@(pr0)	vr4 = shl.u32 vr11, r48;"
        "}"
        "{"
        "V0@(pr0)	vr5 = or.u32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr2;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr5;"
        "}"
        "{"
        "V1@(pr0)	vr2 = shl.u32 vr12, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V1@(pr0)	vr5 = shr.u32 vr13, r32;"
        "}"
        "{"
        "V0@(pr0)	vr2 = or.u32 vr2, vr5;"
        "}"
        "{"
        "V1@(pr0)	vr5 = shl.u32 vr13, r48;"
        "}"
        "{"
        "V0@(pr0)	vr12 = sel vmsk1 vr12, vr2;"
        "V1@(pr0)	vr13 = sel vmsk1 vr13, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 15;"
        "V0@(pr0)	vr1 = and.u32 vr10, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr2 = shl.u32 vr6, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = gt.s32 vr6, r46;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr10, vr3;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32768;"
        "V0@(pr0)	vr1 = add.s32 vr11, r48;"
        "V1@(pr0)	vmsk1 = gt.s32 vr12, r44;"
        "}"
        "{"
        "V0@(pr0)	vr11 = sel vmsk1 vr11, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr1 = and.u32 vr11, r48;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr11;"
        "V1@(pr0)	vr15 = mov.u32 vr10;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr1;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32768;"
        "V1@(pr0)	vmsk1 = eq.s32 vr12, r45;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr16;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr17;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr28 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr29 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr30 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr31 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr1 = and.u32 vr28, r44;"
        "V1@(pr0)	vr2 = and.u32 vr30, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = lseq.s32 vr1, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 8;"
        "V1@(pr0)	vr1 = or.u32 vr28, r45;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr1, vr10;"
        "V1@(pr0)	vr11 = sel vmsk1 vr29, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = lseq.s32 vr2, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 8;"
        "V1@(pr0)	vr1 = or.u32 vr30, r45;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr1, vr10;"
        "V1@(pr0)	vr11 = sel vmsk1 vr31, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32768;"
        "V0@(pr0)	vr3 = xor.u32 vr28, vr30;"
        "V1@(pr0)	vr3 = and.u32 vr3, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = eq.f32 vr1, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32760;"
        "V0@(pr0)	vr5 = mov.u32 r44;"
        "}"
        "{"
        "V0@(pr0)	vr4 = xor.u32 vr1, vr3;"
        "}"
        "{"
        "V1@(pr0)	vmsk2 = eq.s32 vr2, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr5;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = eq.f32 vr2, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32760;"
        "V0@(pr0)	vr5 = mov.u32 r44;"
        "}"
        "{"
        "V0@(pr0)	vr4 = xor.u32 vr2, vr3;"
        "}"
        "{"
        "V1@(pr0)	vmsk2 = eq.s32 vr1, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr5;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr11;"
        "}"
        "{"
        "V1@(pr0)	vmsk1 = eq.f32 vr1, r46;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr3;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, r46;"
        "}"
        "{"
        "V1@(pr0)	vmsk1 = eq.f32 vr2, r46;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr3;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 49152;"
        "pseudo@0	@pseudo imm_2 = 3;"
        "V0@(pr0)	vr13 = and.u32 vr10, r44;"
        "V1@(pr0)	vr1 = shl.u32 vr10, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 16383;"
        "V0@(pr0)	vr1 = and.u32 vr1, r44;"
        "V1@(pr0)	vr13 = or.u32 vr1, vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 29;"
        "V1@(pr0)	vr1 = shr.u32 vr11, r32;"
        "}"
        "{"
        "V0@(pr0)	vr13 = or.u32 vr1, vr13;"
        "}"
        "{"
        "V0@(pr0)	vr13 = cvtftoint.s32 vr13, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 128;"
        "V0@(pr0)	vr13 = add.s32 vr13, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 24;"
        "V1@(pr0)	vr13 = shra.s32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr13;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr1 = cvtinttof.f32 vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 29;"
        "V1@(pr0)	vr11 = shl.u32 vr1, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 49152;"
        "pseudo@0	@pseudo imm_2 = 3;"
        "V0@(pr0)	vr10 = and.u32 vr1, r44;"
        "V1@(pr0)	vr2 = shr.u32 vr1, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 2047;"
        "V0@(pr0)	vr2 = and.u32 vr2, r44;"
        "V1@(pr0)	vr10 = or.u32 vr2, vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 2048;"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 14336;"
        "V0@(pr0)	vr2 = and.u32 vr1, r44;"
        "V1@(pr0)	vr3 = or.u32 vr10, r45;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = eq.s32 vr2, r46;"
        "V1@(pr0)	vr10 = sel vmsk1 vr3, vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8699;"
        "pseudo@0	@pseudo imm_1 = 16377;"
        "pseudo@0	@pseudo imm_2 = 11544;"
        "pseudo@0	@pseudo imm_3 = 21572;"
        "V0@(pr0)	vr12 = mov.u32 r44;"
        "V1@(pr0)	vr13 = mov.u32 r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr13;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr1 = and.u32 vr10, r44;"
        "V1@(pr0)	vr2 = and.u32 vr12, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "V0@(pr0)	vmsk1 = ls.f32 vr1, r44;"
        "V1@(pr0)	vmsk2 = ls.f32 vr2, r44;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, r46;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, r46;"
        "}"
        "{"
        "V0@(pr0)	vr12 = sel vmsk2 vr12, r46;"
        "V1@(pr0)	vr13 = sel vmsk2 vr13, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr6 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr7 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 15;"
        "V0@(pr0)	vr10 = and.u32 vr10, r44;"
        "V1@(pr0)	vr12 = and.u32 vr12, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "V0@(pr0)	vr10 = or.u32 vr10, r44;"
        "V1@(pr0)	vr12 = or.u32 vr12, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 11;"
        "V1@(pr0)	vr12 = shl.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 21;"
        "V1@(pr0)	vr1 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 11;"
        "V0@(pr0)	vr12 = or.u32 vr12, vr1;"
        "V1@(pr0)	vr13 = shl.u32 vr13, r32;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr13, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = and.u32 vr12, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr13, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr12, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = and.u32 vr12, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr12, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 r46;"
        "V1@(pr0)	vr10 = mov.u32 vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr10;"
        "V1@(pr0)	vr17 = mov.u32 vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = or.u32 vr1, vr2;"
        "V1@(pr0)	vr14 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr12 = mov.u32 r46;"
        "V1@(pr0)	vr13 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "V1@(pr0)	vr15 = shr.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 vr15;"
        "V1@(pr0)	vr29 = mov.u32 vr14;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr15;"
        "V1@(pr0)	vr15 = mov.u32 vr0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr16;"
        "V1@(pr0)	vr11 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr12;"
        "V1@(pr0)	vr17 = mov.u32 vr13;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr29;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 vr15;"
        "V1@(pr0)	vr29 = mov.u32 vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr16;"
        "V1@(pr0)	vr13 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr10;"
        "V1@(pr0)	vr17 = mov.u32 vr11;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr28;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr16;"
        "V1@(pr0)	vr11 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr29 = shl.u32 vr29, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr12;"
        "V1@(pr0)	vr17 = mov.u32 vr13;"
        "}"
        "{"
        "V0@(pr0)	vr30 = mov.u32 vr14;"
        "V1@(pr0)	vr14 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr31 = mov.u32 vr17;"
        "V1@(pr0)	vr17 = mov.u32 vr16;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr11;"
        "V1@(pr0)	vr15 = mov.u32 vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr16;"
        "V1@(pr0)	vr11 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr13;"
        "V1@(pr0)	vr13 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr28;"
        "V1@(pr0)	vr17 = mov.u32 vr29;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr30;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr13;"
        "V1@(pr0)	vr15 = mov.u32 vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr13 = mov.u32 vr17;"
        "V1@(pr0)	vr12 = mov.u32 vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "V0@(pr0)	vr3 = xor.u32 vr6, vr7;"
        "V1@(pr0)	vr3 = and.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr3 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 2047;"
        "pseudo@0	@pseudo imm_3 = 0;"
        "V0@(pr0)	vr7 = and.u32 vr7, r45;"
        "V1@(pr0)	vr6 = and.u32 vr6, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 1023;"
        "pseudo@0	@pseudo imm_3 = 0;"
        "V0@(pr0)	vr6 = add.s32 vr6, vr7;"
        "V1@(pr0)	vr6 = sub.s32 vr6, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 16;"
        "V0@(pr0)	vr1 = and.u32 vr10, r45;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr6, r48;"
        "V1@(pr0)	vmsk1 = eq.s32 vr1, r46;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr2, vr6;"
        "}"
        "{"
        "V1@(pr0)	vr2 = shl.u32 vr10, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V1@(pr0)	vr5 = shr.u32 vr11, r32;"
        "}"
        "{"
        "V0@(pr0)	vr2 = or.u32 vr2, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V1@(pr0)	vr5 = shl.u32 vr13, r32;"
        "}"
        "{"
        "V1@(pr0)	vr4 = shl.u32 vr11, r48;"
        "}"
        "{"
        "V0@(pr0)	vr5 = or.u32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr2;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr5;"
        "}"
        "{"
        "V1@(pr0)	vr2 = shl.u32 vr12, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V1@(pr0)	vr5 = shr.u32 vr13, r32;"
        "}"
        "{"
        "V0@(pr0)	vr2 = or.u32 vr2, vr5;"
        "}"
        "{"
        "V1@(pr0)	vr5 = shl.u32 vr13, r48;"
        "}"
        "{"
        "V0@(pr0)	vr12 = sel vmsk1 vr12, vr2;"
        "V1@(pr0)	vr13 = sel vmsk1 vr13, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 15;"
        "V0@(pr0)	vr1 = and.u32 vr10, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr2 = shl.u32 vr6, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = gt.s32 vr6, r46;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr10, vr3;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32768;"
        "V0@(pr0)	vr1 = add.s32 vr11, r48;"
        "V1@(pr0)	vmsk1 = gt.s32 vr12, r44;"
        "}"
        "{"
        "V0@(pr0)	vr11 = sel vmsk1 vr11, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr1 = and.u32 vr11, r48;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr11;"
        "V1@(pr0)	vr15 = mov.u32 vr10;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr1;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32768;"
        "V1@(pr0)	vmsk1 = eq.s32 vr12, r45;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr16;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr17;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr28 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr29 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr30 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr31 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr1 = and.u32 vr28, r44;"
        "V1@(pr0)	vr2 = and.u32 vr30, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = lseq.s32 vr1, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 8;"
        "V1@(pr0)	vr1 = or.u32 vr28, r45;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr1, vr10;"
        "V1@(pr0)	vr11 = sel vmsk1 vr29, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = lseq.s32 vr2, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 8;"
        "V1@(pr0)	vr1 = or.u32 vr30, r45;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr1, vr10;"
        "V1@(pr0)	vr11 = sel vmsk1 vr31, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32768;"
        "V0@(pr0)	vr3 = xor.u32 vr28, vr30;"
        "V1@(pr0)	vr3 = and.u32 vr3, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = eq.f32 vr1, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32760;"
        "V0@(pr0)	vr5 = mov.u32 r44;"
        "}"
        "{"
        "V0@(pr0)	vr4 = xor.u32 vr1, vr3;"
        "}"
        "{"
        "V1@(pr0)	vmsk2 = eq.s32 vr2, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr5;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = eq.f32 vr2, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32760;"
        "V0@(pr0)	vr5 = mov.u32 r44;"
        "}"
        "{"
        "V0@(pr0)	vr4 = xor.u32 vr2, vr3;"
        "}"
        "{"
        "V1@(pr0)	vmsk2 = eq.s32 vr1, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr5;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr11;"
        "}"
        "{"
        "V1@(pr0)	vmsk1 = eq.f32 vr1, r46;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr3;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, r46;"
        "}"
        "{"
        "V1@(pr0)	vmsk1 = eq.f32 vr2, r46;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr3;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32768;"
        "V0@(pr0)	vr12 = mov.u32 vr11;"
        "V1@(pr0)	vr13 = xor.u32 vr10, r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr1 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr11 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr10 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr1;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 32767;"
        "V0@(pr0)	vr14 = and.u32 vr10, r44;"
        "V1@(pr0)	vr15 = and.u32 vr11, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 32767;"
        "V0@(pr0)	vr16 = and.u32 vr12, r44;"
        "V1@(pr0)	vr17 = and.u32 vr13, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8192;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 5120;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr15;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 6144;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 7168;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = gt.s32 vr17, vr15;"
        "V1@(pr0)	vmsk1 = eq.s32 vr17, vr15;"
        "}"
        "{"
        "V0@(pr0)	vmsk2 = gt.s32 vr16, vr14;"
        "}"
        "{"
        "V0@(pr0)	vr1 = sel vmsk2 vr13, vr11;"
        "V1@(pr0)	vr0 = sel vmsk2 vr12, vr10;"
        "}"
        "{"
        "V0@(pr0)	vr1 = sel vmsk1 vr13, vr1;"
        "V1@(pr0)	vr0 = sel vmsk1 vr12, vr0;"
        "}"
        "{"
        "V0@(pr0)	vr1 = sel vmsk0 vr1, vr11;"
        "V1@(pr0)	vr0 = sel vmsk0 vr0, vr10;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = eq.s32 vr1, vr11;"
        "V1@(pr0)	vmsk1 = eq.s32 vr0, vr10;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr7 = sel vmsk0 vr5, r48;"
        "V1@(pr0)	vr6 = sel vmsk1 vr5, r48;"
        "}"
        "{"
        "V0@(pr0)	vr7 = and.u32 vr6, vr7;"
        "V1@(pr0)	vmsk0 = eq.s32 vr7, r48;"
        "}"
        "{"
        "V0@(pr0)	vr15 = sel vmsk0 vr11, vr13;"
        "V1@(pr0)	vr14 = sel vmsk0 vr10, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr1;"
        "V1@(pr0)	vr16 = mov.u32 vr0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2047;"
        "pseudo@0	@pseudo imm_1 = 20;"
        "V0@(pr0)	vr7 = mov.u32 r32;"
        "V1@(pr0)	vr6 = mov.u32 r33;"
        "}"
        "{"
        "V1@(pr0)	vr0 = shr.u32 vr15, vr6;"
        "}"
        "{"
        "V0@(pr0)	vr0 = and.u32 vr0, vr7;"
        "}"
        "{"
        "V1@(pr0)	vr1 = shr.u32 vr17, vr6;"
        "}"
        "{"
        "V0@(pr0)	vr1 = and.u32 vr1, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 15;"
        "V0@(pr0)	vr2 = and.u32 vr14, r44;"
        "V1@(pr0)	vr3 = and.u32 vr15, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 15;"
        "V0@(pr0)	vr4 = and.u32 vr16, r44;"
        "V1@(pr0)	vr5 = and.u32 vr17, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32768;"
        "V0@(pr0)	vr6 = and.u32 vr15, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32768;"
        "V0@(pr0)	vr7 = xor.u32 vr15, vr17;"
        "V1@(pr0)	vr7 = and.u32 vr7, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "pseudo@0	@pseudo imm_2 = 3;"
        "V0@(pr0)	vr3 = or.u32 vr3, r44;"
        "V1@(pr0)	vr3 = shl.u32 vr3, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 57344;"
        "pseudo@0	@pseudo imm_2 = 29;"
        "V0@(pr0)	vr28 = and.u32 vr2, r44;"
        "V1@(pr0)	vr28 = shr.u32 vr28, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3;"
        "V0@(pr0)	vr3 = or.u32 vr3, vr28;"
        "V1@(pr0)	vr2 = shl.u32 vr2, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "pseudo@0	@pseudo imm_2 = 3;"
        "V0@(pr0)	vr5 = or.u32 vr5, r44;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 57344;"
        "pseudo@0	@pseudo imm_2 = 29;"
        "V0@(pr0)	vr28 = and.u32 vr4, r44;"
        "V1@(pr0)	vr28 = shr.u32 vr28, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3;"
        "V0@(pr0)	vr5 = or.u32 vr5, vr28;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 64;"
        "V0@(pr0)	vr28 = sub.s32 vr0, vr1;"
        "V1@(pr0)	vr29 = mov.u32 r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr29 = sub.s32 vr29, vr28;"
        "V1@(pr0)	vmsk0 = gt.s32 vr29, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr31 = mov.u32 r32;"
        "V1@(pr0)	vr31 = sub.s32 vr31, vr29;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr30 = mov.u32 r44;"
        "}"
        "{"
        "V1@(pr0)	vr30 = shl.u32 vr30, vr31;"
        "}"
        "{"
        "V0@(pr0)	vr30 = and.u32 vr4, vr30;"
        "V1@(pr0)	vr30 = shr.u32 vr30, vr31;"
        "}"
        "{"
        "V1@(pr0)	vr11 = shl.u32 vr5, vr29;"
        "}"
        "{"
        "V0@(pr0)	vr11 = or.u32 vr11, vr30;"
        "V1@(pr0)	vr10 = shl.u32 vr4, vr29;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V1@(pr0)	vr30 = sub.s32 vr29, r32;"
        "}"
        "{"
        "V1@(pr0)	vr12 = shl.u32 vr4, vr30;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk0 vr10, r46;"
        "V1@(pr0)	vr11 = sel vmsk0 vr11, vr12;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = eq.s32 vr10, r46;"
        "V1@(pr0)	vmsk1 = eq.s32 vr11, r46;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 r46;"
        "V1@(pr0)	vr17 = sel vmsk0 vr11, r48;"
        "}"
        "{"
        "V0@(pr0)	vr16 = sel vmsk1 vr11, r48;"
        "V1@(pr0)	vr17 = add.s32 vr16, vr17;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vmsk0 = eq.s32 vr17, r32;"
        "V1@(pr0)	vr17 = sel vmsk0 vr11, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V1@(pr0)	vmsk0 = gt.s32 vr28, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 65535;"
        "V0@(pr0)	vr31 = mov.u32 r45;"
        "V1@(pr0)	vr30 = mov.u32 r32;"
        "}"
        "{"
        "V0@(pr0)	vr30 = sub.s32 vr30, vr28;"
        "V1@(pr0)	vr31 = shr.u32 vr31, vr30;"
        "}"
        "{"
        "V0@(pr0)	vr31 = and.u32 vr31, vr5;"
        "V1@(pr0)	vr31 = shl.u32 vr31, vr30;"
        "}"
        "{"
        "V1@(pr0)	vr10 = shr.u32 vr4, vr28;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr10, vr31;"
        "V1@(pr0)	vr11 = shr.u32 vr5, vr28;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr31 = sub.s32 vr28, r32;"
        "V1@(pr0)	vr12 = shr.u32 vr5, vr31;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk0 vr10, vr12;"
        "V1@(pr0)	vr11 = sel vmsk0 vr11, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 64;"
        "V0@(pr0)	vr10 = or.u32 vr10, vr17;"
        "V1@(pr0)	vmsk0 = ls.s32 vr28, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 r46;"
        "V1@(pr0)	vr11 = sel vmsk0 vr17, vr11;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 r48;"
        "V1@(pr0)	vr10 = sel vmsk0 vr17, vr10;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = eq.s32 vr28, r36;"
        "V1@(pr0)	vr4 = sel vmsk0 vr10, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "V0@(pr0)	vr5 = sel vmsk0 vr11, vr5;"
        "V1@(pr0)	vr10 = and.u32 vr2, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr12 = and.u32 vr4, r33;"
        "V1@(pr0)	vr11 = shr.u32 vr2, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr13 = shr.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = gteq.s32 vr10, vr12;"
        "V1@(pr0)	vr28 = mov.u32 r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr17 = sel vmsk0 vr28, r46;"
        "V1@(pr0)	vr17 = shl.u32 vr17, r32;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr10, vr17;"
        "V1@(pr0)	vr14 = sub.s32 vr10, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr15 = add.s32 vr13, r48;"
        "V1@(pr0)	vr13 = sel vmsk0 vr15, vr13;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = gteq.s32 vr11, vr13;"
        "V1@(pr0)	vr17 = sel vmsk0 vr28, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr17 = shl.u32 vr17, r32;"
        "}"
        "{"
        "V0@(pr0)	vr11 = or.u32 vr11, vr17;"
        "V1@(pr0)	vr15 = sub.s32 vr11, vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr14, vr15;"
        "}"
        "{"
        "V0@(pr0)	vr17 = add.s32 vr5, r48;"
        "V1@(pr0)	vr17 = sel vmsk0 vr17, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr11 = sub.s32 vr3, vr17;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = eq.s32 vr11, r46;"
        "V1@(pr0)	vr31 = count.u32 vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr30 = count.u32 vr10;"
        "V1@(pr0)	vr30 = add.s32 vr30, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr31 = sel vmsk0 vr31, vr30;"
        "V1@(pr0)	vr31 = sub.s32 vr31, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vmsk6 = ls.s32 vr31, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr30 = mov.u32 r32;"
        "V1@(pr0)	vr30 = sub.s32 vr30, vr31;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr29 = mov.u32 r44;"
        "V1@(pr0)	vr29 = shl.u32 vr29, vr30;"
        "}"
        "{"
        "V0@(pr0)	vr29 = and.u32 vr29, vr10;"
        "V1@(pr0)	vr29 = shr.u32 vr29, vr30;"
        "}"
        "{"
        "V1@(pr0)	vr13 = shl.u32 vr11, vr31;"
        "}"
        "{"
        "V0@(pr0)	vr13 = or.u32 vr13, vr29;"
        "V1@(pr0)	vr12 = shl.u32 vr10, vr31;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr30 = mov.u32 r32;"
        "V1@(pr0)	vr30 = sub.s32 vr31, vr30;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr14 = shl.u32 vr10, vr30;"
        "}"
        "{"
        "V0@(pr0)	vr12 = sel vmsk6 vr15, vr12;"
        "V1@(pr0)	vr13 = sel vmsk6 vr14, vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 128;"
        "V0@(pr0)	vmsk0 = ls.s32 vr11, r36;"
        "V1@(pr0)	vr10 = sel vmsk0 vr10, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr15 = sub.s32 vr0, vr31;"
        "}"
        "{"
        "V0@(pr0)	vr11 = sel vmsk0 vr11, vr13;"
        "V1@(pr0)	vr15 = sel vmsk0 vr0, vr15;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr12 = and.u32 vr2, r33;"
        "V1@(pr0)	vr13 = shr.u32 vr2, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr16 = and.u32 vr4, r33;"
        "V1@(pr0)	vr17 = shr.u32 vr4, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr28 = add.s32 vr12, vr16;"
        "V1@(pr0)	vr14 = shr.u32 vr28, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "V0@(pr0)	vr28 = and.u32 vr28, r32;"
        "}"
        "{"
        "V0@(pr0)	vr29 = add.s32 vr13, vr17;"
        "V1@(pr0)	vr29 = add.s32 vr29, vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr29, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "V0@(pr0)	vr29 = and.u32 vr29, r32;"
        "V1@(pr0)	vr29 = shl.u32 vr29, r33;"
        "}"
        "{"
        "V0@(pr0)	vr12 = or.u32 vr29, vr28;"
        "V1@(pr0)	vr13 = add.s32 vr3, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr13 = add.s32 vr13, vr14;"
        "V1@(pr0)	vr28 = and.u32 vr12, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V0@(pr0)	vr14 = and.u32 vr13, r48;"
        "V1@(pr0)	vr14 = shl.u32 vr14, r32;"
        "}"
        "{"
        "V1@(pr0)	vr16 = shr.u32 vr12, r48;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr16, vr14;"
        "V1@(pr0)	vr17 = shr.u32 vr13, r48;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr16, vr28;"
        "V1@(pr0)	vr14 = add.s32 vr0, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 256;"
        "V0@(pr0)	vr28 = and.u32 vr13, r36;"
        "V1@(pr0)	vmsk0 = eq.s32 vr28, r46;"
        "}"
        "{"
        "V0@(pr0)	vr12 = sel vmsk0 vr16, vr12;"
        "V1@(pr0)	vr13 = sel vmsk0 vr17, vr13;"
        "}"
        "{"
        "V0@(pr0)	vr14 = sel vmsk0 vr14, vr0;"
        "V1@(pr0)	vmsk1 = eq.s32 vr7, r46;"
        "}"
        "{"
        "V0@(pr0)	vr2 = sel vmsk1 vr10, vr12;"
        "V1@(pr0)	vr3 = sel vmsk1 vr11, vr13;"
        "}"
        "{"
        "V0@(pr0)	vr0 = sel vmsk1 vr15, vr14;"
        "V1@(pr0)	vr31 = mov.u32 r48;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr2, r48;"
        "V1@(pr0)	vmsk0 = lseq.s32 vr0, r46;"
        "}"
        "{"
        "V0@(pr0)	vr2 = sel vmsk0 vr2, vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 7;"
        "pseudo@0	@pseudo imm_1 = 3;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 65535;"
        "V0@(pr0)	vr17 = and.u32 vr2, r32;"
        "V1@(pr0)	vr31 = mov.u32 r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 29;"
        "V1@(pr0)	vr31 = shr.u32 vr31, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 29;"
        "V0@(pr0)	vr31 = and.u32 vr3, vr31;"
        "V1@(pr0)	vr31 = shl.u32 vr31, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3;"
        "V1@(pr0)	vr10 = shr.u32 vr2, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3;"
        "V0@(pr0)	vr10 = or.u32 vr10, vr31;"
        "V1@(pr0)	vr11 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 15;"
        "V0@(pr0)	vr10 = and.u32 vr10, r44;"
        "V1@(pr0)	vr11 = and.u32 vr11, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr28 = shl.u32 vr0, r32;"
        "}"
        "{"
        "V0@(pr0)	vr11 = or.u32 vr11, vr28;"
        "V1@(pr0)	vr11 = or.u32 vr11, vr6;"
        "}"
        "{"
        "V0@(pr0)	vr12 = add.s32 vr10, r48;"
        "V1@(pr0)	vr13 = and.u32 vr10, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "V0@(pr0)	vmsk0 = gt.s32 vr17, r32;"
        "V1@(pr0)	vmsk1 = eq.s32 vr17, r32;"
        "}"
        "{"
        "V0@(pr0)	vr13 = add.s32 vr10, vr13;"
        "V1@(pr0)	vr13 = sel vmsk1 vr10, vr13;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk0 vr13, vr12;"
        "V1@(pr0)	vr0 = mov.u32 vr10;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mov.u32 vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr10 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr11 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr12 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr13 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 5120;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 6144;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr16 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 7168;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr17 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8192;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = eq.s32 vr17, r46;"
        "V1@(pr0)	vmsk1 = eq.s32 vr16, r46;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 r46;"
        "V1@(pr0)	vr7 = sel vmsk0 vr28, r48;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr28, r48;"
        "V1@(pr0)	vr7 = add.s32 vr6, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vmsk0 = eq.s32 vr7, r32;"
        "V1@(pr0)	vr30 = sel vmsk0 vr0, vr10;"
        "}"
        "{"
        "V0@(pr0)	vr31 = sel vmsk0 vr1, vr11;"
        "V1@(pr0)	vr6 = and.u32 vr10, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr7 = and.u32 vr11, vr13;"
        "V1@(pr0)	vr5 = sel vmsk0 vr13, vr7;"
        "}"
        "{"
        "V0@(pr0)	vr4 = sel vmsk0 vr12, vr6;"
        "V1@(pr0)	vmsk0 = eq.s32 vr14, r46;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = eq.s32 vr15, r46;"
        "V1@(pr0)	vr6 = sel vmsk0 vr28, r48;"
        "}"
        "{"
        "V0@(pr0)	vr7 = sel vmsk1 vr28, r48;"
        "V1@(pr0)	vr7 = add.s32 vr6, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vmsk0 = eq.s32 vr7, r32;"
        "V1@(pr0)	vr31 = sel vmsk0 vr31, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr30 = sel vmsk0 vr30, r34;"
        "V1@(pr0)	vr7 = mov.u32 r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 15;"
        "V0@(pr0)	vr6 = mov.u32 r44;"
        "V1@(pr0)	vr7 = xor.u32 vr7, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr6 = xor.u32 vr6, r44;"
        "V1@(pr0)	vmsk0 = eq.s32 vr16, vr6;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mov.u32 vr7;"
        "V1@(pr0)	vr4 = mov.u32 vr6;"
        "}"
        "{"
        "V0@(pr0)	vmsk6 = eq.s32 vr14, vr6;"
        "V1@(pr0)	vmsk7 = eq.s32 vr15, vr7;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = eq.s32 vr17, vr7;"
        "V1@(pr0)	vr7 = sel vmsk0 vr28, r48;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr28, r48;"
        "V1@(pr0)	vr7 = add.s32 vr6, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vmsk0 = eq.s32 vr7, r32;"
        "V1@(pr0)	vr31 = sel vmsk0 vr31, vr13;"
        "}"
        "{"
        "V0@(pr0)	vr30 = sel vmsk0 vr30, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr7 = sel vmsk7 vr28, r48;"
        "V1@(pr0)	vr6 = sel vmsk6 vr28, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vr7 = add.s32 vr6, vr7;"
        "V1@(pr0)	vmsk7 = eq.s32 vr7, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr3 = or.u32 vr5, r36;"
        "V1@(pr0)	vr6 = xor.u32 vr10, vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32768;"
        "V0@(pr0)	vr7 = xor.u32 vr11, vr13;"
        "V1@(pr0)	vmsk0 = eq.s32 vr7, r36;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = eq.s32 vr6, r46;"
        "V1@(pr0)	vr7 = sel vmsk0 vr28, r48;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr28, r48;"
        "V1@(pr0)	vr7 = add.s32 vr6, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vmsk0 = eq.s32 vr7, r32;"
        "V1@(pr0)	vr7 = sel vmsk0 vr11, vr3;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk0 vr10, vr4;"
        "V1@(pr0)	vr30 = sel vmsk7 vr30, vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr31 = sel vmsk7 vr31, vr7;"
        "V1@(pr0)	vr7 = or.u32 vr13, r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk0 = eq.s32 vr16, r46;"
        "V1@(pr0)	vr6 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "V0@(pr0)	vr28 = and.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32752;"
        "V0@(pr0)	vmsk1 = gt.s32 vr6, r32;"
        "V1@(pr0)	vmsk2 = eq.s32 vr28, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk0 vr28, r48;"
        "V1@(pr0)	vr4 = sel vmsk2 vr28, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vr5 = add.s32 vr4, vr5;"
        "V1@(pr0)	vmsk2 = eq.s32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr7, vr31;"
        "V1@(pr0)	vr4 = sel vmsk2 vr12, vr30;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32752;"
        "V0@(pr0)	vmsk2 = eq.s32 vr6, r32;"
        "V1@(pr0)	vr4 = sel vmsk2 vr30, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr31, vr5;"
        "V1@(pr0)	vr31 = sel vmsk1 vr5, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr30 = sel vmsk1 vr4, vr12;"
        "V1@(pr0)	vr7 = or.u32 vr11, r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk0 = eq.s32 vr14, r46;"
        "V1@(pr0)	vr6 = shr.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "V0@(pr0)	vr28 = and.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32752;"
        "V0@(pr0)	vmsk1 = gt.s32 vr6, r32;"
        "V1@(pr0)	vmsk2 = eq.s32 vr28, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk0 vr28, r48;"
        "V1@(pr0)	vr4 = sel vmsk2 vr28, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vr5 = add.s32 vr4, vr5;"
        "V1@(pr0)	vmsk2 = eq.s32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr7, vr31;"
        "V1@(pr0)	vr4 = sel vmsk2 vr10, vr30;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32752;"
        "V0@(pr0)	vmsk2 = eq.s32 vr6, r32;"
        "V1@(pr0)	vr4 = sel vmsk2 vr30, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr31, vr5;"
        "V1@(pr0)	vr11 = sel vmsk1 vr5, vr7;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr4, vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr12 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr1 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V1@(pr0)	vr2 = shr.u32 vr1, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 5120;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr12;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 26;"
        "V1@(pr0)	vr1 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 15;"
        "V1@(pr0)	vr1 = and.u32 vr1, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 162;"
        "pseudo@0	@pseudo imm_3 = 0;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 28238;"
        "pseudo@0	@pseudo imm_1 = 63875;"
        "pseudo@0	@pseudo imm_2 = 10748;"
        "pseudo@0	@pseudo imm_3 = 17429;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "pseudo@0	@pseudo imm_2 = 41721;"
        "pseudo@0	@pseudo imm_3 = 0;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20036;"
        "pseudo@0	@pseudo imm_1 = 33646;"
        "pseudo@0	@pseudo imm_2 = 64551;"
        "pseudo@0	@pseudo imm_3 = 5417;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "pseudo@0	@pseudo imm_2 = 63875;"
        "pseudo@0	@pseudo imm_3 = 162;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 17429;"
        "pseudo@0	@pseudo imm_1 = 28238;"
        "pseudo@0	@pseudo imm_2 = 10071;"
        "pseudo@0	@pseudo imm_3 = 10748;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3;"
        "pseudo@0	@pseudo imm_2 = 33646;"
        "pseudo@0	@pseudo imm_3 = 41721;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 5417;"
        "pseudo@0	@pseudo imm_1 = 20036;"
        "pseudo@0	@pseudo imm_2 = 22481;"
        "pseudo@0	@pseudo imm_3 = 64551;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "pseudo@0	@pseudo imm_2 = 28238;"
        "pseudo@0	@pseudo imm_3 = 63875;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 10748;"
        "pseudo@0	@pseudo imm_1 = 17429;"
        "pseudo@0	@pseudo imm_2 = 53749;"
        "pseudo@0	@pseudo imm_3 = 10071;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 5;"
        "pseudo@0	@pseudo imm_2 = 20036;"
        "pseudo@0	@pseudo imm_3 = 33646;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 64551;"
        "pseudo@0	@pseudo imm_1 = 5417;"
        "pseudo@0	@pseudo imm_2 = 62772;"
        "pseudo@0	@pseudo imm_3 = 22481;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 6;"
        "pseudo@0	@pseudo imm_2 = 17429;"
        "pseudo@0	@pseudo imm_3 = 28238;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 10071;"
        "pseudo@0	@pseudo imm_1 = 10748;"
        "pseudo@0	@pseudo imm_2 = 13533;"
        "pseudo@0	@pseudo imm_3 = 53749;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 7;"
        "pseudo@0	@pseudo imm_2 = 5417;"
        "pseudo@0	@pseudo imm_3 = 20036;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 22481;"
        "pseudo@0	@pseudo imm_1 = 64551;"
        "pseudo@0	@pseudo imm_2 = 56768;"
        "pseudo@0	@pseudo imm_3 = 62772;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "pseudo@0	@pseudo imm_2 = 10748;"
        "pseudo@0	@pseudo imm_3 = 17429;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 53749;"
        "pseudo@0	@pseudo imm_1 = 10071;"
        "pseudo@0	@pseudo imm_2 = 49371;"
        "pseudo@0	@pseudo imm_3 = 13533;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 9;"
        "pseudo@0	@pseudo imm_2 = 64551;"
        "pseudo@0	@pseudo imm_3 = 5417;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 62772;"
        "pseudo@0	@pseudo imm_1 = 22481;"
        "pseudo@0	@pseudo imm_2 = 56162;"
        "pseudo@0	@pseudo imm_3 = 56768;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 10;"
        "pseudo@0	@pseudo imm_2 = 10071;"
        "pseudo@0	@pseudo imm_3 = 10748;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 13533;"
        "pseudo@0	@pseudo imm_1 = 53749;"
        "pseudo@0	@pseudo imm_2 = 25237;"
        "pseudo@0	@pseudo imm_3 = 49371;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 11;"
        "pseudo@0	@pseudo imm_2 = 22481;"
        "pseudo@0	@pseudo imm_3 = 64551;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 56768;"
        "pseudo@0	@pseudo imm_1 = 62772;"
        "pseudo@0	@pseudo imm_2 = 38297;"
        "pseudo@0	@pseudo imm_3 = 56162;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 12;"
        "pseudo@0	@pseudo imm_2 = 53749;"
        "pseudo@0	@pseudo imm_3 = 10071;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 49371;"
        "pseudo@0	@pseudo imm_1 = 13533;"
        "pseudo@0	@pseudo imm_2 = 39228;"
        "pseudo@0	@pseudo imm_3 = 25237;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 13;"
        "pseudo@0	@pseudo imm_2 = 62772;"
        "pseudo@0	@pseudo imm_3 = 22481;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 56162;"
        "pseudo@0	@pseudo imm_1 = 56768;"
        "pseudo@0	@pseudo imm_2 = 15427;"
        "pseudo@0	@pseudo imm_3 = 38297;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 14;"
        "pseudo@0	@pseudo imm_2 = 13533;"
        "pseudo@0	@pseudo imm_3 = 53749;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 25237;"
        "pseudo@0	@pseudo imm_1 = 49371;"
        "pseudo@0	@pseudo imm_2 = 17296;"
        "pseudo@0	@pseudo imm_3 = 39228;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 23;"
        "V1@(pr0)	vr1 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 7;"
        "V1@(pr0)	vr1 = and.u32 vr1, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 255;"
        "pseudo@0	@pseudo imm_2 = 128;"
        "V0@(pr0)	vr10 = and.u32 vr10, r44;"
        "V1@(pr0)	vr10 = or.u32 vr10, r38;"
        "}"
        "{"
        "V1@(pr0)	vr10 = shl.u32 vr10, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr10;"
        "V1@(pr0)	vr13 = mov.u32 vr28;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr14;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr12, r54;"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr12, r32;"
        "}"
        "{"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr13, r32;"
        "}"
        "{"
        "V1@(pr0)	vr15 = and.u32 vr12, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr16;"
        "V1@(pr0)	vr13 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 vr13;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr10;"
        "V1@(pr0)	vr13 = mov.u32 vr30;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr14;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr12, r54;"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr12, r32;"
        "}"
        "{"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr13, r32;"
        "}"
        "{"
        "V1@(pr0)	vr15 = and.u32 vr12, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr16;"
        "V1@(pr0)	vr13 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr30 = mov.u32 vr12;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr10;"
        "V1@(pr0)	vr13 = mov.u32 vr29;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr14;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr12, r54;"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr12, r32;"
        "}"
        "{"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr13, r32;"
        "}"
        "{"
        "V1@(pr0)	vr15 = and.u32 vr12, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr16;"
        "V1@(pr0)	vr13 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr12;"
        "V1@(pr0)	vr17 = mov.u32 vr13;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 vr28;"
        "V1@(pr0)	vr14 = mov.u32 vr30;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8192;"
        "V0@(pr0)	vr1 = add.s32 vr2, r32;"
        "V1@(pr0)	vr1 = and.u32 vr1, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 14;"
        "V1@(pr0)	vr1 = shr.u32 vr1, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 14;"
        "V1@(pr0)	vr3 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V1@(pr0)	vr2 = sub.s32 vr2, vr3;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr16 = and.u32 vr16, r54;"
        "V1@(pr0)	vr2 = shl.u32 vr2, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr16, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr1;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32768;"
        "V0@(pr0)	vr12 = mov.u32 r44;"
        "V1@(pr0)	vmsk7 = gteq.f32 vr16, r46;"
        "}"
        "{"
        "V1@(pr0)	vr12 = sel vmsk7 vr12, r46;"
        "}"
        "{"
        "V0@(pr0)	vmsk3 = eq.s32 vr17, r46;"
        "V1@(pr0)	vr6 = sub.s32 vr17, r48;"
        "}"
        "{"
        "V1@(pr0)	vr7 = sub.s32 vr16, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr6 = sel vmsk3 vr6, r44;"
        "V1@(pr0)	vr7 = sel vmsk3 vr16, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr6 = xor.u32 vr6, r44;"
        "V1@(pr0)	vr7 = xor.u32 vr7, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr17 = sel vmsk7 vr6, vr17;"
        "V1@(pr0)	vr16 = sel vmsk7 vr7, vr16;"
        "}"
        "{"
        "V0@(pr0)	vr2 = count.u32 vr16;"
        "V1@(pr0)	vr1 = count.u32 vr17;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vmsk6 = ls.s32 vr2, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V0@(pr0)	vr6 = mov.u32 r32;"
        "V1@(pr0)	vr1 = sub.s32 vr6, vr1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 63;"
        "V0@(pr0)	vr6 = mov.u32 r32;"
        "V1@(pr0)	vr2 = sub.s32 vr6, vr2;"
        "}"
        "{"
        "V1@(pr0)	vr13 = sel vmsk6 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr0 = mov.u32 vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1023;"
        "V0@(pr0)	vr13 = add.s32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr13 = shl.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 20;"
        "V0@(pr0)	vmsk5 = lseq.s32 vr0, r32;"
        "V1@(pr0)	vmsk4 = lseq.s32 vr0, r33;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr6 = sub.s32 vr0, r32;"
        "V1@(pr0)	vr7 = mov.u32 r48;"
        "}"
        "{"
        "V1@(pr0)	vr6 = shl.u32 vr7, vr6;"
        "}"
        "{"
        "V1@(pr0)	vr6 = sub.s32 vr16, vr6;"
        "}"
        "{"
        "V1@(pr0)	vr7 = shl.u32 vr7, vr0;"
        "}"
        "{"
        "V1@(pr0)	vr7 = sub.s32 vr17, vr7;"
        "}"
        "{"
        "V1@(pr0)	vr5 = sel vmsk4 vr6, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "V0@(pr0)	vr6 = sub.s32 vr0, r32;"
        "V1@(pr0)	vr3 = shr.u32 vr5, vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 84;"
        "V0@(pr0)	vr6 = mov.u32 r32;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sub.s32 vr6, vr0;"
        "V1@(pr0)	vr4 = shl.u32 vr5, vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "V0@(pr0)	vr6 = sub.s32 vr0, r32;"
        "V1@(pr0)	vr6 = shr.u32 vr17, vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V0@(pr0)	vr4 = add.s32 vr4, vr6;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 vr3;"
        "V1@(pr0)	vr14 = mov.u32 vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "V0@(pr0)	vr6 = mov.u32 r32;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sub.s32 vr6, vr0;"
        "V1@(pr0)	vr3 = shl.u32 vr5, vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V0@(pr0)	vr6 = sub.s32 vr0, r32;"
        "V1@(pr0)	vr6 = shr.u32 vr17, vr6;"
        "}"
        "{"
        "V1@(pr0)	vr3 = add.s32 vr3, vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "V0@(pr0)	vr7 = mov.u32 r32;"
        "}"
        "{"
        "V0@(pr0)	vr7 = sub.s32 vr7, vr0;"
        "V1@(pr0)	vr4 = shl.u32 vr17, vr7;"
        "}"
        "{"
        "V0@(pr0)	vr15 = sel vmsk5 vr15, vr3;"
        "V1@(pr0)	vr14 = sel vmsk5 vr14, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V0@(pr0)	vr6 = mov.u32 r32;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sub.s32 vr6, vr0;"
        "V1@(pr0)	vr3 = shl.u32 vr5, vr6;"
        "}"
        "{"
        "V0@(pr0)	vr15 = sel vmsk4 vr15, vr3;"
        "V1@(pr0)	vr14 = sel vmsk4 vr14, r46;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr12, vr13;"
        "V1@(pr0)	vr10 = or.u32 vr10, vr15;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8699;"
        "pseudo@0	@pseudo imm_1 = 15385;"
        "pseudo@0	@pseudo imm_2 = 11544;"
        "pseudo@0	@pseudo imm_3 = 21572;"
        "V0@(pr0)	vr12 = mov.u32 r44;"
        "V1@(pr0)	vr13 = mov.u32 r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr13;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr1 = and.u32 vr10, r44;"
        "V1@(pr0)	vr2 = and.u32 vr12, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "V0@(pr0)	vmsk1 = ls.f32 vr1, r44;"
        "V1@(pr0)	vmsk2 = ls.f32 vr2, r44;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, r46;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, r46;"
        "}"
        "{"
        "V0@(pr0)	vr12 = sel vmsk2 vr12, r46;"
        "V1@(pr0)	vr13 = sel vmsk2 vr13, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr6 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr7 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 15;"
        "V0@(pr0)	vr10 = and.u32 vr10, r44;"
        "V1@(pr0)	vr12 = and.u32 vr12, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "V0@(pr0)	vr10 = or.u32 vr10, r44;"
        "V1@(pr0)	vr12 = or.u32 vr12, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 11;"
        "V1@(pr0)	vr12 = shl.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 21;"
        "V1@(pr0)	vr1 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 11;"
        "V0@(pr0)	vr12 = or.u32 vr12, vr1;"
        "V1@(pr0)	vr13 = shl.u32 vr13, r32;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr13, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = and.u32 vr12, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr13, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr12, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = and.u32 vr12, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr12, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 r46;"
        "V1@(pr0)	vr10 = mov.u32 vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr10;"
        "V1@(pr0)	vr17 = mov.u32 vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = or.u32 vr1, vr2;"
        "V1@(pr0)	vr14 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr12 = mov.u32 r46;"
        "V1@(pr0)	vr13 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "V1@(pr0)	vr15 = shr.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 vr15;"
        "V1@(pr0)	vr29 = mov.u32 vr14;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr15;"
        "V1@(pr0)	vr15 = mov.u32 vr0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr16;"
        "V1@(pr0)	vr11 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr12;"
        "V1@(pr0)	vr17 = mov.u32 vr13;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr29;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 vr15;"
        "V1@(pr0)	vr29 = mov.u32 vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr16;"
        "V1@(pr0)	vr13 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr10;"
        "V1@(pr0)	vr17 = mov.u32 vr11;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr28;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr16;"
        "V1@(pr0)	vr11 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr29 = shl.u32 vr29, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr12;"
        "V1@(pr0)	vr17 = mov.u32 vr13;"
        "}"
        "{"
        "V0@(pr0)	vr30 = mov.u32 vr14;"
        "V1@(pr0)	vr14 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr31 = mov.u32 vr17;"
        "V1@(pr0)	vr17 = mov.u32 vr16;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr11;"
        "V1@(pr0)	vr15 = mov.u32 vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr16;"
        "V1@(pr0)	vr11 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr13;"
        "V1@(pr0)	vr13 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr28;"
        "V1@(pr0)	vr17 = mov.u32 vr29;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr30;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr13;"
        "V1@(pr0)	vr15 = mov.u32 vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr13 = mov.u32 vr17;"
        "V1@(pr0)	vr12 = mov.u32 vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "V0@(pr0)	vr3 = xor.u32 vr6, vr7;"
        "V1@(pr0)	vr3 = and.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr3 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 2047;"
        "pseudo@0	@pseudo imm_3 = 0;"
        "V0@(pr0)	vr7 = and.u32 vr7, r45;"
        "V1@(pr0)	vr6 = and.u32 vr6, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 1023;"
        "pseudo@0	@pseudo imm_3 = 0;"
        "V0@(pr0)	vr6 = add.s32 vr6, vr7;"
        "V1@(pr0)	vr6 = sub.s32 vr6, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 16;"
        "V0@(pr0)	vr1 = and.u32 vr10, r45;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr6, r48;"
        "V1@(pr0)	vmsk1 = eq.s32 vr1, r46;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr2, vr6;"
        "}"
        "{"
        "V1@(pr0)	vr2 = shl.u32 vr10, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V1@(pr0)	vr5 = shr.u32 vr11, r32;"
        "}"
        "{"
        "V0@(pr0)	vr2 = or.u32 vr2, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V1@(pr0)	vr5 = shl.u32 vr13, r32;"
        "}"
        "{"
        "V1@(pr0)	vr4 = shl.u32 vr11, r48;"
        "}"
        "{"
        "V0@(pr0)	vr5 = or.u32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr2;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr5;"
        "}"
        "{"
        "V1@(pr0)	vr2 = shl.u32 vr12, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V1@(pr0)	vr5 = shr.u32 vr13, r32;"
        "}"
        "{"
        "V0@(pr0)	vr2 = or.u32 vr2, vr5;"
        "}"
        "{"
        "V1@(pr0)	vr5 = shl.u32 vr13, r48;"
        "}"
        "{"
        "V0@(pr0)	vr12 = sel vmsk1 vr12, vr2;"
        "V1@(pr0)	vr13 = sel vmsk1 vr13, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 15;"
        "V0@(pr0)	vr1 = and.u32 vr10, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr2 = shl.u32 vr6, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = gt.s32 vr6, r46;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr10, vr3;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32768;"
        "V0@(pr0)	vr1 = add.s32 vr11, r48;"
        "V1@(pr0)	vmsk1 = gt.s32 vr12, r44;"
        "}"
        "{"
        "V0@(pr0)	vr11 = sel vmsk1 vr11, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr1 = and.u32 vr11, r48;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr11;"
        "V1@(pr0)	vr15 = mov.u32 vr10;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr1;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32768;"
        "V1@(pr0)	vmsk1 = eq.s32 vr12, r45;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr16;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr17;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr28 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr29 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr30 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr31 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr1 = and.u32 vr28, r44;"
        "V1@(pr0)	vr2 = and.u32 vr30, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = lseq.s32 vr1, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 8;"
        "V1@(pr0)	vr1 = or.u32 vr28, r45;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr1, vr10;"
        "V1@(pr0)	vr11 = sel vmsk1 vr29, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = lseq.s32 vr2, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 8;"
        "V1@(pr0)	vr1 = or.u32 vr30, r45;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr1, vr10;"
        "V1@(pr0)	vr11 = sel vmsk1 vr31, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32768;"
        "V0@(pr0)	vr3 = xor.u32 vr28, vr30;"
        "V1@(pr0)	vr3 = and.u32 vr3, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = eq.f32 vr1, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32760;"
        "V0@(pr0)	vr5 = mov.u32 r44;"
        "}"
        "{"
        "V0@(pr0)	vr4 = xor.u32 vr1, vr3;"
        "}"
        "{"
        "V1@(pr0)	vmsk2 = eq.s32 vr2, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr5;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = eq.f32 vr2, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32760;"
        "V0@(pr0)	vr5 = mov.u32 r44;"
        "}"
        "{"
        "V0@(pr0)	vr4 = xor.u32 vr2, vr3;"
        "}"
        "{"
        "V1@(pr0)	vmsk2 = eq.s32 vr1, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr5;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr11;"
        "}"
        "{"
        "V1@(pr0)	vmsk1 = eq.f32 vr1, r46;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr3;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, r46;"
        "}"
        "{"
        "V1@(pr0)	vmsk1 = eq.f32 vr2, r46;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr3;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr12 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr5 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr4 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr1 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr2 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr3 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 5120;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32768;"
        "V0@(pr0)	vmsk1 = eq.s32 vr4, r48;"
        "V1@(pr0)	vr6 = xor.u32 vr10, r36;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 17136;"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 20224;"
        "V0@(pr0)	vr6 = and.u32 vr5, r45;"
        "V1@(pr0)	vmsk4 = ls.f32 vr6, r44;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk4 vr10, vr2;"
        "V1@(pr0)	vr11 = sel vmsk4 vr11, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr13 = sel vmsk4 vr12, vr3;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 49152;"
        "pseudo@0	@pseudo imm_2 = 3;"
        "V0@(pr0)	vr4 = and.u32 vr10, r44;"
        "V1@(pr0)	vr1 = shl.u32 vr10, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 16383;"
        "V0@(pr0)	vr1 = and.u32 vr1, r44;"
        "V1@(pr0)	vr4 = or.u32 vr1, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 29;"
        "V1@(pr0)	vr1 = shr.u32 vr11, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = or.u32 vr1, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr13;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 29;"
        "V1@(pr0)	vr12 = shl.u32 vr4, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 49152;"
        "pseudo@0	@pseudo imm_2 = 3;"
        "V0@(pr0)	vr6 = and.u32 vr4, r44;"
        "V1@(pr0)	vr2 = shr.u32 vr4, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 2047;"
        "V0@(pr0)	vr2 = and.u32 vr2, r44;"
        "V1@(pr0)	vr6 = or.u32 vr2, vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 2048;"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 14336;"
        "V0@(pr0)	vr2 = and.u32 vr4, r44;"
        "V1@(pr0)	vr3 = or.u32 vr6, r45;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = eq.s32 vr2, r46;"
        "V1@(pr0)	vr6 = sel vmsk1 vr3, vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32768;"
        "V1@(pr0)	vr13 = xor.u32 vr6, r36;"
        "}"
        "{"
        "V0@(pr0)	vr2 = mov.u32 vr10;"
        "V1@(pr0)	vr10 = mov.u32 vr11;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 32767;"
        "V0@(pr0)	vr14 = and.u32 vr10, r44;"
        "V1@(pr0)	vr15 = and.u32 vr11, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 32767;"
        "V0@(pr0)	vr16 = and.u32 vr12, r44;"
        "V1@(pr0)	vr17 = and.u32 vr13, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8192;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 5120;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr15;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 6144;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 7168;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = gt.s32 vr17, vr15;"
        "V1@(pr0)	vmsk1 = eq.s32 vr17, vr15;"
        "}"
        "{"
        "V0@(pr0)	vmsk2 = gt.s32 vr16, vr14;"
        "}"
        "{"
        "V0@(pr0)	vr1 = sel vmsk2 vr13, vr11;"
        "V1@(pr0)	vr0 = sel vmsk2 vr12, vr10;"
        "}"
        "{"
        "V0@(pr0)	vr1 = sel vmsk1 vr13, vr1;"
        "V1@(pr0)	vr0 = sel vmsk1 vr12, vr0;"
        "}"
        "{"
        "V0@(pr0)	vr1 = sel vmsk0 vr1, vr11;"
        "V1@(pr0)	vr0 = sel vmsk0 vr0, vr10;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = eq.s32 vr1, vr11;"
        "V1@(pr0)	vmsk1 = eq.s32 vr0, vr10;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr7 = sel vmsk0 vr5, r48;"
        "V1@(pr0)	vr6 = sel vmsk1 vr5, r48;"
        "}"
        "{"
        "V0@(pr0)	vr7 = and.u32 vr6, vr7;"
        "V1@(pr0)	vmsk0 = eq.s32 vr7, r48;"
        "}"
        "{"
        "V0@(pr0)	vr15 = sel vmsk0 vr11, vr13;"
        "V1@(pr0)	vr14 = sel vmsk0 vr10, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr1;"
        "V1@(pr0)	vr16 = mov.u32 vr0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2047;"
        "pseudo@0	@pseudo imm_1 = 20;"
        "V0@(pr0)	vr7 = mov.u32 r32;"
        "V1@(pr0)	vr6 = mov.u32 r33;"
        "}"
        "{"
        "V1@(pr0)	vr0 = shr.u32 vr15, vr6;"
        "}"
        "{"
        "V0@(pr0)	vr0 = and.u32 vr0, vr7;"
        "}"
        "{"
        "V1@(pr0)	vr1 = shr.u32 vr17, vr6;"
        "}"
        "{"
        "V0@(pr0)	vr1 = and.u32 vr1, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 15;"
        "V0@(pr0)	vr2 = and.u32 vr14, r44;"
        "V1@(pr0)	vr3 = and.u32 vr15, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 15;"
        "V0@(pr0)	vr4 = and.u32 vr16, r44;"
        "V1@(pr0)	vr5 = and.u32 vr17, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32768;"
        "V0@(pr0)	vr6 = and.u32 vr15, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32768;"
        "V0@(pr0)	vr7 = xor.u32 vr15, vr17;"
        "V1@(pr0)	vr7 = and.u32 vr7, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "pseudo@0	@pseudo imm_2 = 3;"
        "V0@(pr0)	vr3 = or.u32 vr3, r44;"
        "V1@(pr0)	vr3 = shl.u32 vr3, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 57344;"
        "pseudo@0	@pseudo imm_2 = 29;"
        "V0@(pr0)	vr28 = and.u32 vr2, r44;"
        "V1@(pr0)	vr28 = shr.u32 vr28, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3;"
        "V0@(pr0)	vr3 = or.u32 vr3, vr28;"
        "V1@(pr0)	vr2 = shl.u32 vr2, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "pseudo@0	@pseudo imm_2 = 3;"
        "V0@(pr0)	vr5 = or.u32 vr5, r44;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 57344;"
        "pseudo@0	@pseudo imm_2 = 29;"
        "V0@(pr0)	vr28 = and.u32 vr4, r44;"
        "V1@(pr0)	vr28 = shr.u32 vr28, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3;"
        "V0@(pr0)	vr5 = or.u32 vr5, vr28;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 64;"
        "V0@(pr0)	vr28 = sub.s32 vr0, vr1;"
        "V1@(pr0)	vr29 = mov.u32 r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr29 = sub.s32 vr29, vr28;"
        "V1@(pr0)	vmsk0 = gt.s32 vr29, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr31 = mov.u32 r32;"
        "V1@(pr0)	vr31 = sub.s32 vr31, vr29;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr30 = mov.u32 r44;"
        "}"
        "{"
        "V1@(pr0)	vr30 = shl.u32 vr30, vr31;"
        "}"
        "{"
        "V0@(pr0)	vr30 = and.u32 vr4, vr30;"
        "V1@(pr0)	vr30 = shr.u32 vr30, vr31;"
        "}"
        "{"
        "V1@(pr0)	vr11 = shl.u32 vr5, vr29;"
        "}"
        "{"
        "V0@(pr0)	vr11 = or.u32 vr11, vr30;"
        "V1@(pr0)	vr10 = shl.u32 vr4, vr29;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V1@(pr0)	vr30 = sub.s32 vr29, r32;"
        "}"
        "{"
        "V1@(pr0)	vr12 = shl.u32 vr4, vr30;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk0 vr10, r46;"
        "V1@(pr0)	vr11 = sel vmsk0 vr11, vr12;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = eq.s32 vr10, r46;"
        "V1@(pr0)	vmsk1 = eq.s32 vr11, r46;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 r46;"
        "V1@(pr0)	vr17 = sel vmsk0 vr11, r48;"
        "}"
        "{"
        "V0@(pr0)	vr16 = sel vmsk1 vr11, r48;"
        "V1@(pr0)	vr17 = add.s32 vr16, vr17;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vmsk0 = eq.s32 vr17, r32;"
        "V1@(pr0)	vr17 = sel vmsk0 vr11, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V1@(pr0)	vmsk0 = gt.s32 vr28, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 65535;"
        "V0@(pr0)	vr31 = mov.u32 r45;"
        "V1@(pr0)	vr30 = mov.u32 r32;"
        "}"
        "{"
        "V0@(pr0)	vr30 = sub.s32 vr30, vr28;"
        "V1@(pr0)	vr31 = shr.u32 vr31, vr30;"
        "}"
        "{"
        "V0@(pr0)	vr31 = and.u32 vr31, vr5;"
        "V1@(pr0)	vr31 = shl.u32 vr31, vr30;"
        "}"
        "{"
        "V1@(pr0)	vr10 = shr.u32 vr4, vr28;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr10, vr31;"
        "V1@(pr0)	vr11 = shr.u32 vr5, vr28;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr31 = sub.s32 vr28, r32;"
        "V1@(pr0)	vr12 = shr.u32 vr5, vr31;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk0 vr10, vr12;"
        "V1@(pr0)	vr11 = sel vmsk0 vr11, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 64;"
        "V0@(pr0)	vr10 = or.u32 vr10, vr17;"
        "V1@(pr0)	vmsk0 = ls.s32 vr28, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 r46;"
        "V1@(pr0)	vr11 = sel vmsk0 vr17, vr11;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 r48;"
        "V1@(pr0)	vr10 = sel vmsk0 vr17, vr10;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = eq.s32 vr28, r36;"
        "V1@(pr0)	vr4 = sel vmsk0 vr10, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "V0@(pr0)	vr5 = sel vmsk0 vr11, vr5;"
        "V1@(pr0)	vr10 = and.u32 vr2, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr12 = and.u32 vr4, r33;"
        "V1@(pr0)	vr11 = shr.u32 vr2, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr13 = shr.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = gteq.s32 vr10, vr12;"
        "V1@(pr0)	vr28 = mov.u32 r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr17 = sel vmsk0 vr28, r46;"
        "V1@(pr0)	vr17 = shl.u32 vr17, r32;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr10, vr17;"
        "V1@(pr0)	vr14 = sub.s32 vr10, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr15 = add.s32 vr13, r48;"
        "V1@(pr0)	vr13 = sel vmsk0 vr15, vr13;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = gteq.s32 vr11, vr13;"
        "V1@(pr0)	vr17 = sel vmsk0 vr28, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr17 = shl.u32 vr17, r32;"
        "}"
        "{"
        "V0@(pr0)	vr11 = or.u32 vr11, vr17;"
        "V1@(pr0)	vr15 = sub.s32 vr11, vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr14, vr15;"
        "}"
        "{"
        "V0@(pr0)	vr17 = add.s32 vr5, r48;"
        "V1@(pr0)	vr17 = sel vmsk0 vr17, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr11 = sub.s32 vr3, vr17;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = eq.s32 vr11, r46;"
        "V1@(pr0)	vr31 = count.u32 vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr30 = count.u32 vr10;"
        "V1@(pr0)	vr30 = add.s32 vr30, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr31 = sel vmsk0 vr31, vr30;"
        "V1@(pr0)	vr31 = sub.s32 vr31, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vmsk6 = ls.s32 vr31, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr30 = mov.u32 r32;"
        "V1@(pr0)	vr30 = sub.s32 vr30, vr31;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr29 = mov.u32 r44;"
        "V1@(pr0)	vr29 = shl.u32 vr29, vr30;"
        "}"
        "{"
        "V0@(pr0)	vr29 = and.u32 vr29, vr10;"
        "V1@(pr0)	vr29 = shr.u32 vr29, vr30;"
        "}"
        "{"
        "V1@(pr0)	vr13 = shl.u32 vr11, vr31;"
        "}"
        "{"
        "V0@(pr0)	vr13 = or.u32 vr13, vr29;"
        "V1@(pr0)	vr12 = shl.u32 vr10, vr31;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr30 = mov.u32 r32;"
        "V1@(pr0)	vr30 = sub.s32 vr31, vr30;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr14 = shl.u32 vr10, vr30;"
        "}"
        "{"
        "V0@(pr0)	vr12 = sel vmsk6 vr15, vr12;"
        "V1@(pr0)	vr13 = sel vmsk6 vr14, vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 128;"
        "V0@(pr0)	vmsk0 = ls.s32 vr11, r36;"
        "V1@(pr0)	vr10 = sel vmsk0 vr10, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr15 = sub.s32 vr0, vr31;"
        "}"
        "{"
        "V0@(pr0)	vr11 = sel vmsk0 vr11, vr13;"
        "V1@(pr0)	vr15 = sel vmsk0 vr0, vr15;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr12 = and.u32 vr2, r33;"
        "V1@(pr0)	vr13 = shr.u32 vr2, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr16 = and.u32 vr4, r33;"
        "V1@(pr0)	vr17 = shr.u32 vr4, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr28 = add.s32 vr12, vr16;"
        "V1@(pr0)	vr14 = shr.u32 vr28, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "V0@(pr0)	vr28 = and.u32 vr28, r32;"
        "}"
        "{"
        "V0@(pr0)	vr29 = add.s32 vr13, vr17;"
        "V1@(pr0)	vr29 = add.s32 vr29, vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr29, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "V0@(pr0)	vr29 = and.u32 vr29, r32;"
        "V1@(pr0)	vr29 = shl.u32 vr29, r33;"
        "}"
        "{"
        "V0@(pr0)	vr12 = or.u32 vr29, vr28;"
        "V1@(pr0)	vr13 = add.s32 vr3, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr13 = add.s32 vr13, vr14;"
        "V1@(pr0)	vr28 = and.u32 vr12, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V0@(pr0)	vr14 = and.u32 vr13, r48;"
        "V1@(pr0)	vr14 = shl.u32 vr14, r32;"
        "}"
        "{"
        "V1@(pr0)	vr16 = shr.u32 vr12, r48;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr16, vr14;"
        "V1@(pr0)	vr17 = shr.u32 vr13, r48;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr16, vr28;"
        "V1@(pr0)	vr14 = add.s32 vr0, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 256;"
        "V0@(pr0)	vr28 = and.u32 vr13, r36;"
        "V1@(pr0)	vmsk0 = eq.s32 vr28, r46;"
        "}"
        "{"
        "V0@(pr0)	vr12 = sel vmsk0 vr16, vr12;"
        "V1@(pr0)	vr13 = sel vmsk0 vr17, vr13;"
        "}"
        "{"
        "V0@(pr0)	vr14 = sel vmsk0 vr14, vr0;"
        "V1@(pr0)	vmsk1 = eq.s32 vr7, r46;"
        "}"
        "{"
        "V0@(pr0)	vr2 = sel vmsk1 vr10, vr12;"
        "V1@(pr0)	vr3 = sel vmsk1 vr11, vr13;"
        "}"
        "{"
        "V0@(pr0)	vr0 = sel vmsk1 vr15, vr14;"
        "V1@(pr0)	vr31 = mov.u32 r48;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr2, r48;"
        "V1@(pr0)	vmsk0 = lseq.s32 vr0, r46;"
        "}"
        "{"
        "V0@(pr0)	vr2 = sel vmsk0 vr2, vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 7;"
        "pseudo@0	@pseudo imm_1 = 3;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 65535;"
        "V0@(pr0)	vr17 = and.u32 vr2, r32;"
        "V1@(pr0)	vr31 = mov.u32 r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 29;"
        "V1@(pr0)	vr31 = shr.u32 vr31, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 29;"
        "V0@(pr0)	vr31 = and.u32 vr3, vr31;"
        "V1@(pr0)	vr31 = shl.u32 vr31, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3;"
        "V1@(pr0)	vr10 = shr.u32 vr2, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3;"
        "V0@(pr0)	vr10 = or.u32 vr10, vr31;"
        "V1@(pr0)	vr11 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 15;"
        "V0@(pr0)	vr10 = and.u32 vr10, r44;"
        "V1@(pr0)	vr11 = and.u32 vr11, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr28 = shl.u32 vr0, r32;"
        "}"
        "{"
        "V0@(pr0)	vr11 = or.u32 vr11, vr28;"
        "V1@(pr0)	vr11 = or.u32 vr11, vr6;"
        "}"
        "{"
        "V0@(pr0)	vr12 = add.s32 vr10, r48;"
        "V1@(pr0)	vr13 = and.u32 vr10, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "V0@(pr0)	vmsk0 = gt.s32 vr17, r32;"
        "V1@(pr0)	vmsk1 = eq.s32 vr17, r32;"
        "}"
        "{"
        "V0@(pr0)	vr13 = add.s32 vr10, vr13;"
        "V1@(pr0)	vr13 = sel vmsk1 vr10, vr13;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk0 vr13, vr12;"
        "V1@(pr0)	vr0 = mov.u32 vr10;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mov.u32 vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr10 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr11 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr12 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr13 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 5120;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 6144;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr16 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 7168;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr17 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8192;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = eq.s32 vr17, r46;"
        "V1@(pr0)	vmsk1 = eq.s32 vr16, r46;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 r46;"
        "V1@(pr0)	vr7 = sel vmsk0 vr28, r48;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr28, r48;"
        "V1@(pr0)	vr7 = add.s32 vr6, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vmsk0 = eq.s32 vr7, r32;"
        "V1@(pr0)	vr30 = sel vmsk0 vr0, vr10;"
        "}"
        "{"
        "V0@(pr0)	vr31 = sel vmsk0 vr1, vr11;"
        "V1@(pr0)	vr6 = and.u32 vr10, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr7 = and.u32 vr11, vr13;"
        "V1@(pr0)	vr5 = sel vmsk0 vr13, vr7;"
        "}"
        "{"
        "V0@(pr0)	vr4 = sel vmsk0 vr12, vr6;"
        "V1@(pr0)	vmsk0 = eq.s32 vr14, r46;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = eq.s32 vr15, r46;"
        "V1@(pr0)	vr6 = sel vmsk0 vr28, r48;"
        "}"
        "{"
        "V0@(pr0)	vr7 = sel vmsk1 vr28, r48;"
        "V1@(pr0)	vr7 = add.s32 vr6, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vmsk0 = eq.s32 vr7, r32;"
        "V1@(pr0)	vr31 = sel vmsk0 vr31, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr30 = sel vmsk0 vr30, r34;"
        "V1@(pr0)	vr7 = mov.u32 r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 15;"
        "V0@(pr0)	vr6 = mov.u32 r44;"
        "V1@(pr0)	vr7 = xor.u32 vr7, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr6 = xor.u32 vr6, r44;"
        "V1@(pr0)	vmsk0 = eq.s32 vr16, vr6;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mov.u32 vr7;"
        "V1@(pr0)	vr4 = mov.u32 vr6;"
        "}"
        "{"
        "V0@(pr0)	vmsk6 = eq.s32 vr14, vr6;"
        "V1@(pr0)	vmsk7 = eq.s32 vr15, vr7;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = eq.s32 vr17, vr7;"
        "V1@(pr0)	vr7 = sel vmsk0 vr28, r48;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr28, r48;"
        "V1@(pr0)	vr7 = add.s32 vr6, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vmsk0 = eq.s32 vr7, r32;"
        "V1@(pr0)	vr31 = sel vmsk0 vr31, vr13;"
        "}"
        "{"
        "V0@(pr0)	vr30 = sel vmsk0 vr30, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr7 = sel vmsk7 vr28, r48;"
        "V1@(pr0)	vr6 = sel vmsk6 vr28, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vr7 = add.s32 vr6, vr7;"
        "V1@(pr0)	vmsk7 = eq.s32 vr7, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr3 = or.u32 vr5, r36;"
        "V1@(pr0)	vr6 = xor.u32 vr10, vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32768;"
        "V0@(pr0)	vr7 = xor.u32 vr11, vr13;"
        "V1@(pr0)	vmsk0 = eq.s32 vr7, r36;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = eq.s32 vr6, r46;"
        "V1@(pr0)	vr7 = sel vmsk0 vr28, r48;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr28, r48;"
        "V1@(pr0)	vr7 = add.s32 vr6, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vmsk0 = eq.s32 vr7, r32;"
        "V1@(pr0)	vr7 = sel vmsk0 vr11, vr3;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk0 vr10, vr4;"
        "V1@(pr0)	vr30 = sel vmsk7 vr30, vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr31 = sel vmsk7 vr31, vr7;"
        "V1@(pr0)	vr7 = or.u32 vr13, r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk0 = eq.s32 vr16, r46;"
        "V1@(pr0)	vr6 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "V0@(pr0)	vr28 = and.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32752;"
        "V0@(pr0)	vmsk1 = gt.s32 vr6, r32;"
        "V1@(pr0)	vmsk2 = eq.s32 vr28, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk0 vr28, r48;"
        "V1@(pr0)	vr4 = sel vmsk2 vr28, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vr5 = add.s32 vr4, vr5;"
        "V1@(pr0)	vmsk2 = eq.s32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr7, vr31;"
        "V1@(pr0)	vr4 = sel vmsk2 vr12, vr30;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32752;"
        "V0@(pr0)	vmsk2 = eq.s32 vr6, r32;"
        "V1@(pr0)	vr4 = sel vmsk2 vr30, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr31, vr5;"
        "V1@(pr0)	vr31 = sel vmsk1 vr5, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr30 = sel vmsk1 vr4, vr12;"
        "V1@(pr0)	vr7 = or.u32 vr11, r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk0 = eq.s32 vr14, r46;"
        "V1@(pr0)	vr6 = shr.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "V0@(pr0)	vr28 = and.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32752;"
        "V0@(pr0)	vmsk1 = gt.s32 vr6, r32;"
        "V1@(pr0)	vmsk2 = eq.s32 vr28, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk0 vr28, r48;"
        "V1@(pr0)	vr4 = sel vmsk2 vr28, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vr5 = add.s32 vr4, vr5;"
        "V1@(pr0)	vmsk2 = eq.s32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr7, vr31;"
        "V1@(pr0)	vr4 = sel vmsk2 vr10, vr30;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32752;"
        "V0@(pr0)	vmsk2 = eq.s32 vr6, r32;"
        "V1@(pr0)	vr4 = sel vmsk2 vr30, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr31, vr5;"
        "V1@(pr0)	vr11 = sel vmsk1 vr5, vr7;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr4, vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 49152;"
        "pseudo@0	@pseudo imm_2 = 3;"
        "V0@(pr0)	vr5 = and.u32 vr11, r44;"
        "V1@(pr0)	vr1 = shl.u32 vr11, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 16383;"
        "V0@(pr0)	vr1 = and.u32 vr1, r44;"
        "V1@(pr0)	vr5 = or.u32 vr1, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 29;"
        "V1@(pr0)	vr1 = shr.u32 vr10, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = or.u32 vr1, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr4 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr13 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr4;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr5;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr10 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3;"
        "V0@(pr0)	vr15 = and.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr4 = mul.f32 vr11, vr11;"
        "V1@(pr0)	vr7 = and.u32 vr11, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 39322;"
        "pseudo@0	@pseudo imm_1 = 16025;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vmsk1 = ls.s32 vr7, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 55118;"
        "pseudo@0	@pseudo imm_1 = 44359;"
        "pseudo@0	@pseudo imm_2 = 29942;"
        "pseudo@0	@pseudo imm_3 = 12559;"
        "V0@(pr0)	vr5 = mul.f32 vr4, r44;"
        "V1@(pr0)	vr5 = add.f32 vr5, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 62076;"
        "pseudo@0	@pseudo imm_1 = 46227;"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr5 = add.f32 vr5, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3329;"
        "pseudo@0	@pseudo imm_1 = 14288;"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr5 = add.f32 vr5, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2913;"
        "pseudo@0	@pseudo imm_1 = 47798;"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr5 = add.f32 vr5, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 43691;"
        "pseudo@0	@pseudo imm_1 = 15658;"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr5 = add.f32 vr5, r44;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr11, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr2 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr1 = sub.f32 vr2, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr2 = mul.f32 vr4, r50;"
        "V1@(pr0)	vr1 = sub.f32 vr2, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr2 = mov.u32 r49;"
        "V1@(pr0)	vr1 = sub.f32 vr2, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr0, vr1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16200;"
        "pseudo@0	@pseudo imm_1 = 256;"
        "V0@(pr0)	vmsk2 = gt.s32 vr7, r36;"
        "V1@(pr0)	vr1 = sub.s32 vr7, r37;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16016;"
        "V0@(pr0)	vr2 = mul.f32 vr4, r50;"
        "V1@(pr0)	vr1 = sel vmsk2 vr1, r44;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mov.u32 r49;"
        "V1@(pr0)	vr2 = sub.f32 vr2, vr1;"
        "}"
        "{"
        "V1@(pr0)	vr1 = sub.f32 vr3, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr11, vr12;"
        "V1@(pr0)	vr3 = add.f32 vr3, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr2 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr3 = sub.f32 vr3, vr2;"
        "}"
        "{"
        "V1@(pr0)	vr1 = sub.f32 vr1, vr3;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 12800;"
        "V0@(pr0)	vr6 = sel vmsk1 vr1, vr6;"
        "V1@(pr0)	vmsk1 = ls.s32 vr7, r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr2 = cvtftoint.s32 vr11, r44;"
        "}"
        "{"
        "V0@(pr0)	vr1 = sel vmsk1 vr6, r49;"
        "V1@(pr0)	vmsk1 = eq.s32 vr2, r46;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr6, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 vr6;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mul.f32 vr11, r57;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = eq.s32 vr15, r48;"
        "V1@(pr0)	vr14 = sel vmsk1 vr17, vr11;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = eq.s32 vr15, r46;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vmsk1 = eq.s32 vr15, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, vr17;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr0 = and.u32 vr11, r44;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr11, vr11;"
        "V1@(pr0)	vr7 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr4 = mul.f32 vr3, vr11;"
        "V1@(pr0)	vmsk1 = eq.s32 vr13, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 51667;"
        "pseudo@0	@pseudo imm_1 = 12078;"
        "pseudo@0	@pseudo imm_2 = 12084;"
        "pseudo@0	@pseudo imm_3 = 45783;"
        "V0@(pr0)	vr1 = mul.f32 vr3, r44;"
        "V1@(pr0)	vr1 = add.f32 vr1, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 61211;"
        "pseudo@0	@pseudo imm_1 = 13880;"
        "V0@(pr0)	vr1 = mul.f32 vr3, vr1;"
        "V1@(pr0)	vr1 = add.f32 vr1, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3329;"
        "pseudo@0	@pseudo imm_1 = 47440;"
        "V0@(pr0)	vr1 = mul.f32 vr3, vr1;"
        "V1@(pr0)	vr1 = add.f32 vr1, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 34953;"
        "pseudo@0	@pseudo imm_1 = 15368;"
        "V0@(pr0)	vr1 = mul.f32 vr3, vr1;"
        "V1@(pr0)	vr5 = add.f32 vr1, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 43691;"
        "pseudo@0	@pseudo imm_1 = 48682;"
        "V0@(pr0)	vr1 = mul.f32 vr3, vr5;"
        "V1@(pr0)	vr1 = add.f32 vr1, r44;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.f32 vr11, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr2 = mul.f32 vr12, r50;"
        "V1@(pr0)	vr6 = sel vmsk1 vr6, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr1 = sub.f32 vr2, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr3, vr1;"
        "V1@(pr0)	vr1 = sub.f32 vr1, vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 43691;"
        "pseudo@0	@pseudo imm_1 = 48682;"
        "V0@(pr0)	vr2 = mul.f32 vr4, r44;"
        "V1@(pr0)	vr1 = sub.f32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr2 = cvtftoint.s32 vr11, r44;"
        "V1@(pr0)	vr1 = sub.f32 vr11, vr1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 12800;"
        "V0@(pr0)	vr6 = sel vmsk1 vr1, vr6;"
        "V1@(pr0)	vmsk1 = ls.s32 vr0, r36;"
        "}"
        "{"
        "V0@(pr0)	vr1 = sel vmsk1 vr6, vr11;"
        "V1@(pr0)	vmsk1 = eq.s32 vr2, r46;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr6, vr1;"
        "V1@(pr0)	vr11 = mov.u32 vr6;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mul.f32 vr11, r57;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = eq.s32 vr15, r46;"
        "V1@(pr0)	vr14 = sel vmsk1 vr14, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vmsk1 = eq.s32 vr15, r32;"
        "V1@(pr0)	vr14 = sel vmsk1 vr14, vr17;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = eq.s32 vr15, r48;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, vr17;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3;"
        "V0@(pr0)	vmsk1 = eq.s32 vr15, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, vr11;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mul.f32 vr14, r57;"
        "V1@(pr0)	vmsk1 = ls.f32 vr10, r46;"
        "}"
        "{"
        "V0@(pr0)	vr14 = sel vmsk1 vr14, vr17;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 vr10;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 r46;"
        "V1@(pr0)	vr13 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr0 = and.u32 vr11, r44;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr11, vr11;"
        "V1@(pr0)	vr7 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr4 = mul.f32 vr3, vr11;"
        "V1@(pr0)	vmsk1 = eq.s32 vr13, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 51667;"
        "pseudo@0	@pseudo imm_1 = 12078;"
        "pseudo@0	@pseudo imm_2 = 12084;"
        "pseudo@0	@pseudo imm_3 = 45783;"
        "V0@(pr0)	vr1 = mul.f32 vr3, r44;"
        "V1@(pr0)	vr1 = add.f32 vr1, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 61211;"
        "pseudo@0	@pseudo imm_1 = 13880;"
        "V0@(pr0)	vr1 = mul.f32 vr3, vr1;"
        "V1@(pr0)	vr1 = add.f32 vr1, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3329;"
        "pseudo@0	@pseudo imm_1 = 47440;"
        "V0@(pr0)	vr1 = mul.f32 vr3, vr1;"
        "V1@(pr0)	vr1 = add.f32 vr1, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 34953;"
        "pseudo@0	@pseudo imm_1 = 15368;"
        "V0@(pr0)	vr1 = mul.f32 vr3, vr1;"
        "V1@(pr0)	vr5 = add.f32 vr1, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 43691;"
        "pseudo@0	@pseudo imm_1 = 48682;"
        "V0@(pr0)	vr1 = mul.f32 vr3, vr5;"
        "V1@(pr0)	vr1 = add.f32 vr1, r44;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.f32 vr11, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr2 = mul.f32 vr12, r50;"
        "V1@(pr0)	vr6 = sel vmsk1 vr6, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr1 = sub.f32 vr2, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr3, vr1;"
        "V1@(pr0)	vr1 = sub.f32 vr1, vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 43691;"
        "pseudo@0	@pseudo imm_1 = 48682;"
        "V0@(pr0)	vr2 = mul.f32 vr4, r44;"
        "V1@(pr0)	vr1 = sub.f32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr2 = cvtftoint.s32 vr11, r44;"
        "V1@(pr0)	vr1 = sub.f32 vr11, vr1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 12800;"
        "V0@(pr0)	vr6 = sel vmsk1 vr1, vr6;"
        "V1@(pr0)	vmsk1 = ls.s32 vr0, r36;"
        "}"
        "{"
        "V0@(pr0)	vr1 = sel vmsk1 vr6, vr11;"
        "V1@(pr0)	vmsk1 = eq.s32 vr2, r46;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr6, vr1;"
        "V1@(pr0)	vr11 = mov.u32 vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr1 = and.u32 vr10, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4056;"
        "pseudo@0	@pseudo imm_1 = 16201;"
        "V0@(pr0)	vmsk1 = ls.f32 vr1, r44;"
        "}"
        "{"
        "V1@(pr0)	vr14 = sel vmsk1 vr14, vr11;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 vr10;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 r46;"
        "V1@(pr0)	vr13 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr4 = mul.f32 vr11, vr11;"
        "V1@(pr0)	vr7 = and.u32 vr11, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 39322;"
        "pseudo@0	@pseudo imm_1 = 16025;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vmsk1 = ls.s32 vr7, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 55118;"
        "pseudo@0	@pseudo imm_1 = 44359;"
        "pseudo@0	@pseudo imm_2 = 29942;"
        "pseudo@0	@pseudo imm_3 = 12559;"
        "V0@(pr0)	vr5 = mul.f32 vr4, r44;"
        "V1@(pr0)	vr5 = add.f32 vr5, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 62076;"
        "pseudo@0	@pseudo imm_1 = 46227;"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr5 = add.f32 vr5, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3329;"
        "pseudo@0	@pseudo imm_1 = 14288;"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr5 = add.f32 vr5, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2913;"
        "pseudo@0	@pseudo imm_1 = 47798;"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr5 = add.f32 vr5, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 43691;"
        "pseudo@0	@pseudo imm_1 = 15658;"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr5 = add.f32 vr5, r44;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr11, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr2 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr1 = sub.f32 vr2, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr2 = mul.f32 vr4, r50;"
        "V1@(pr0)	vr1 = sub.f32 vr2, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr2 = mov.u32 r49;"
        "V1@(pr0)	vr1 = sub.f32 vr2, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr0, vr1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16200;"
        "pseudo@0	@pseudo imm_1 = 256;"
        "V0@(pr0)	vmsk2 = gt.s32 vr7, r36;"
        "V1@(pr0)	vr1 = sub.s32 vr7, r37;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16016;"
        "V0@(pr0)	vr2 = mul.f32 vr4, r50;"
        "V1@(pr0)	vr1 = sel vmsk2 vr1, r44;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mov.u32 r49;"
        "V1@(pr0)	vr2 = sub.f32 vr2, vr1;"
        "}"
        "{"
        "V1@(pr0)	vr1 = sub.f32 vr3, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr11, vr12;"
        "V1@(pr0)	vr3 = add.f32 vr3, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr2 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr3 = sub.f32 vr3, vr2;"
        "}"
        "{"
        "V1@(pr0)	vr1 = sub.f32 vr1, vr3;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 12800;"
        "V0@(pr0)	vr6 = sel vmsk1 vr1, vr6;"
        "V1@(pr0)	vmsk1 = ls.s32 vr7, r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr2 = cvtftoint.s32 vr11, r44;"
        "}"
        "{"
        "V0@(pr0)	vr1 = sel vmsk1 vr6, r49;"
        "V1@(pr0)	vmsk1 = eq.s32 vr2, r46;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr6, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr1 = and.u32 vr10, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4056;"
        "pseudo@0	@pseudo imm_1 = 16201;"
        "V0@(pr0)	vmsk1 = ls.f32 vr1, r44;"
        "}"
        "{"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr1 = and.u32 vr10, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32640;"
        "V0@(pr0)	vmsk1 = ls.f32 vr1, r44;"
        "V1@(pr0)	vr15 = sub.f32 vr10, vr10;"
        "}"
        "{"
        "V0@(pr0)	vr14 = sel vmsk1 vr15, vr14;"
        "V1@(pr0)	vr28 = sel vmsk1 vr15, vr28;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr14;"
        "V1@(pr0)	vr11 = mov.u32 vr28;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "V0@(pr0)	vr28 = mov.u32 vr10;"
        "V1@(pr0)	vr29 = mov.u32 vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r1 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VL@(pr0)	vr10 = ld [vmem:r0,$1,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r1 = add.s32 r1, r44;"
        "S1@(pr0)	[smem:r3] = st r1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr14 = and.u32 vr10, r44;"
        "V1@(pr0)	vr15 = add.f32 vr14, vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r1 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r1 = sub.s32 r1, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VS@(pr0)	 [vmem:r0,$1,$255] = st vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VS@(pr0)	 [vmem:r0,$1,$255] = st vr28;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VS@(pr0)	 [vmem:r0,$1,$255] = st vr29;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r1;"
        "V0@(pr0)	vr10 = mov.u32 vr15;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr10;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr1 = and.u32 vr10, r44;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 17136;"
        "V1@(pr0)	vmsk4 = ls.f32 vr2, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr10;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 29;"
        "V1@(pr0)	vr11 = shl.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 49152;"
        "pseudo@0	@pseudo imm_2 = 3;"
        "V0@(pr0)	vr1 = and.u32 vr10, r44;"
        "V1@(pr0)	vr2 = shr.u32 vr10, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 2047;"
        "V0@(pr0)	vr2 = and.u32 vr2, r44;"
        "V1@(pr0)	vr1 = or.u32 vr2, vr1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 2048;"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 14336;"
        "V0@(pr0)	vr2 = and.u32 vr10, r44;"
        "V1@(pr0)	vr3 = or.u32 vr1, r45;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = eq.s32 vr2, r46;"
        "V1@(pr0)	vr1 = sel vmsk1 vr3, vr1;"
        "}"
        "{"
        "V1@(pr0)	vr10 = mov.u32 vr1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr11;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 24368;"
        "pseudo@0	@pseudo imm_1 = 16740;"
        "pseudo@0	@pseudo imm_2 = 51331;"
        "pseudo@0	@pseudo imm_3 = 28105;"
        "V0@(pr0)	vr12 = mov.u32 r44;"
        "V1@(pr0)	vr13 = mov.u32 r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr13;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr1 = and.u32 vr10, r44;"
        "V1@(pr0)	vr2 = and.u32 vr12, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "V0@(pr0)	vmsk1 = ls.f32 vr1, r44;"
        "V1@(pr0)	vmsk2 = ls.f32 vr2, r44;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, r46;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, r46;"
        "}"
        "{"
        "V0@(pr0)	vr12 = sel vmsk2 vr12, r46;"
        "V1@(pr0)	vr13 = sel vmsk2 vr13, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr6 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr7 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 15;"
        "V0@(pr0)	vr10 = and.u32 vr10, r44;"
        "V1@(pr0)	vr12 = and.u32 vr12, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "V0@(pr0)	vr10 = or.u32 vr10, r44;"
        "V1@(pr0)	vr12 = or.u32 vr12, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 11;"
        "V1@(pr0)	vr12 = shl.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 21;"
        "V1@(pr0)	vr1 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 11;"
        "V0@(pr0)	vr12 = or.u32 vr12, vr1;"
        "V1@(pr0)	vr13 = shl.u32 vr13, r32;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr13, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = and.u32 vr12, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr13, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr12, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = and.u32 vr12, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr12, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 r46;"
        "V1@(pr0)	vr10 = mov.u32 vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr10;"
        "V1@(pr0)	vr17 = mov.u32 vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = or.u32 vr1, vr2;"
        "V1@(pr0)	vr14 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr12 = mov.u32 r46;"
        "V1@(pr0)	vr13 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "V1@(pr0)	vr15 = shr.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 vr15;"
        "V1@(pr0)	vr29 = mov.u32 vr14;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr15;"
        "V1@(pr0)	vr15 = mov.u32 vr0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr16;"
        "V1@(pr0)	vr11 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr12;"
        "V1@(pr0)	vr17 = mov.u32 vr13;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr29;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 vr15;"
        "V1@(pr0)	vr29 = mov.u32 vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr16;"
        "V1@(pr0)	vr13 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr10;"
        "V1@(pr0)	vr17 = mov.u32 vr11;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr28;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr16;"
        "V1@(pr0)	vr11 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr29 = shl.u32 vr29, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr12;"
        "V1@(pr0)	vr17 = mov.u32 vr13;"
        "}"
        "{"
        "V0@(pr0)	vr30 = mov.u32 vr14;"
        "V1@(pr0)	vr14 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr31 = mov.u32 vr17;"
        "V1@(pr0)	vr17 = mov.u32 vr16;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr11;"
        "V1@(pr0)	vr15 = mov.u32 vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr16;"
        "V1@(pr0)	vr11 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr13;"
        "V1@(pr0)	vr13 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr28;"
        "V1@(pr0)	vr17 = mov.u32 vr29;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr30;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr13;"
        "V1@(pr0)	vr15 = mov.u32 vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr13 = mov.u32 vr17;"
        "V1@(pr0)	vr12 = mov.u32 vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "V0@(pr0)	vr3 = xor.u32 vr6, vr7;"
        "V1@(pr0)	vr3 = and.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr3 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 2047;"
        "pseudo@0	@pseudo imm_3 = 0;"
        "V0@(pr0)	vr7 = and.u32 vr7, r45;"
        "V1@(pr0)	vr6 = and.u32 vr6, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 1023;"
        "pseudo@0	@pseudo imm_3 = 0;"
        "V0@(pr0)	vr6 = add.s32 vr6, vr7;"
        "V1@(pr0)	vr6 = sub.s32 vr6, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 16;"
        "V0@(pr0)	vr1 = and.u32 vr10, r45;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr6, r48;"
        "V1@(pr0)	vmsk1 = eq.s32 vr1, r46;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr2, vr6;"
        "}"
        "{"
        "V1@(pr0)	vr2 = shl.u32 vr10, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V1@(pr0)	vr5 = shr.u32 vr11, r32;"
        "}"
        "{"
        "V0@(pr0)	vr2 = or.u32 vr2, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V1@(pr0)	vr5 = shl.u32 vr13, r32;"
        "}"
        "{"
        "V1@(pr0)	vr4 = shl.u32 vr11, r48;"
        "}"
        "{"
        "V0@(pr0)	vr5 = or.u32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr2;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr5;"
        "}"
        "{"
        "V1@(pr0)	vr2 = shl.u32 vr12, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V1@(pr0)	vr5 = shr.u32 vr13, r32;"
        "}"
        "{"
        "V0@(pr0)	vr2 = or.u32 vr2, vr5;"
        "}"
        "{"
        "V1@(pr0)	vr5 = shl.u32 vr13, r48;"
        "}"
        "{"
        "V0@(pr0)	vr12 = sel vmsk1 vr12, vr2;"
        "V1@(pr0)	vr13 = sel vmsk1 vr13, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 15;"
        "V0@(pr0)	vr1 = and.u32 vr10, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr2 = shl.u32 vr6, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = gt.s32 vr6, r46;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr10, vr3;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32768;"
        "V0@(pr0)	vr1 = add.s32 vr11, r48;"
        "V1@(pr0)	vmsk1 = gt.s32 vr12, r44;"
        "}"
        "{"
        "V0@(pr0)	vr11 = sel vmsk1 vr11, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr1 = and.u32 vr11, r48;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr11;"
        "V1@(pr0)	vr15 = mov.u32 vr10;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr1;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32768;"
        "V1@(pr0)	vmsk1 = eq.s32 vr12, r45;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr16;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr17;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr28 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr29 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr30 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr31 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr1 = and.u32 vr28, r44;"
        "V1@(pr0)	vr2 = and.u32 vr30, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = lseq.s32 vr1, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 8;"
        "V1@(pr0)	vr1 = or.u32 vr28, r45;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr1, vr10;"
        "V1@(pr0)	vr11 = sel vmsk1 vr29, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = lseq.s32 vr2, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 8;"
        "V1@(pr0)	vr1 = or.u32 vr30, r45;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr1, vr10;"
        "V1@(pr0)	vr11 = sel vmsk1 vr31, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32768;"
        "V0@(pr0)	vr3 = xor.u32 vr28, vr30;"
        "V1@(pr0)	vr3 = and.u32 vr3, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = eq.f32 vr1, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32760;"
        "V0@(pr0)	vr5 = mov.u32 r44;"
        "}"
        "{"
        "V0@(pr0)	vr4 = xor.u32 vr1, vr3;"
        "}"
        "{"
        "V1@(pr0)	vmsk2 = eq.s32 vr2, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr5;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = eq.f32 vr2, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32760;"
        "V0@(pr0)	vr5 = mov.u32 r44;"
        "}"
        "{"
        "V0@(pr0)	vr4 = xor.u32 vr2, vr3;"
        "}"
        "{"
        "V1@(pr0)	vmsk2 = eq.s32 vr1, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr5;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr11;"
        "}"
        "{"
        "V1@(pr0)	vmsk1 = eq.f32 vr1, r46;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr3;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, r46;"
        "}"
        "{"
        "V1@(pr0)	vmsk1 = eq.f32 vr2, r46;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr3;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 49152;"
        "pseudo@0	@pseudo imm_2 = 3;"
        "V0@(pr0)	vr13 = and.u32 vr10, r44;"
        "V1@(pr0)	vr1 = shl.u32 vr10, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 16383;"
        "V0@(pr0)	vr1 = and.u32 vr1, r44;"
        "V1@(pr0)	vr13 = or.u32 vr1, vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 29;"
        "V1@(pr0)	vr1 = shr.u32 vr11, r32;"
        "}"
        "{"
        "V0@(pr0)	vr13 = or.u32 vr1, vr13;"
        "}"
        "{"
        "V0@(pr0)	vr13 = cvtftoint.s32 vr13, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 128;"
        "V0@(pr0)	vr13 = add.s32 vr13, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 24;"
        "V1@(pr0)	vr13 = shra.s32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr13;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr1 = cvtinttof.f32 vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 29;"
        "V1@(pr0)	vr11 = shl.u32 vr1, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 49152;"
        "pseudo@0	@pseudo imm_2 = 3;"
        "V0@(pr0)	vr10 = and.u32 vr1, r44;"
        "V1@(pr0)	vr2 = shr.u32 vr1, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 2047;"
        "V0@(pr0)	vr2 = and.u32 vr2, r44;"
        "V1@(pr0)	vr10 = or.u32 vr2, vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 2048;"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 14336;"
        "V0@(pr0)	vr2 = and.u32 vr1, r44;"
        "V1@(pr0)	vr3 = or.u32 vr10, r45;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = eq.s32 vr2, r46;"
        "V1@(pr0)	vr10 = sel vmsk1 vr3, vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8699;"
        "pseudo@0	@pseudo imm_1 = 16377;"
        "pseudo@0	@pseudo imm_2 = 11544;"
        "pseudo@0	@pseudo imm_3 = 21572;"
        "V0@(pr0)	vr12 = mov.u32 r44;"
        "V1@(pr0)	vr13 = mov.u32 r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr13;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr1 = and.u32 vr10, r44;"
        "V1@(pr0)	vr2 = and.u32 vr12, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "V0@(pr0)	vmsk1 = ls.f32 vr1, r44;"
        "V1@(pr0)	vmsk2 = ls.f32 vr2, r44;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, r46;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, r46;"
        "}"
        "{"
        "V0@(pr0)	vr12 = sel vmsk2 vr12, r46;"
        "V1@(pr0)	vr13 = sel vmsk2 vr13, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr6 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr7 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 15;"
        "V0@(pr0)	vr10 = and.u32 vr10, r44;"
        "V1@(pr0)	vr12 = and.u32 vr12, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "V0@(pr0)	vr10 = or.u32 vr10, r44;"
        "V1@(pr0)	vr12 = or.u32 vr12, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 11;"
        "V1@(pr0)	vr12 = shl.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 21;"
        "V1@(pr0)	vr1 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 11;"
        "V0@(pr0)	vr12 = or.u32 vr12, vr1;"
        "V1@(pr0)	vr13 = shl.u32 vr13, r32;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr13, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = and.u32 vr12, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr13, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr12, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = and.u32 vr12, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr12, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 r46;"
        "V1@(pr0)	vr10 = mov.u32 vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr10;"
        "V1@(pr0)	vr17 = mov.u32 vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = or.u32 vr1, vr2;"
        "V1@(pr0)	vr14 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr12 = mov.u32 r46;"
        "V1@(pr0)	vr13 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "V1@(pr0)	vr15 = shr.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 vr15;"
        "V1@(pr0)	vr29 = mov.u32 vr14;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr15;"
        "V1@(pr0)	vr15 = mov.u32 vr0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr16;"
        "V1@(pr0)	vr11 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr12;"
        "V1@(pr0)	vr17 = mov.u32 vr13;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr29;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 vr15;"
        "V1@(pr0)	vr29 = mov.u32 vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr16;"
        "V1@(pr0)	vr13 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr10;"
        "V1@(pr0)	vr17 = mov.u32 vr11;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr28;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr16;"
        "V1@(pr0)	vr11 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr29 = shl.u32 vr29, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr12;"
        "V1@(pr0)	vr17 = mov.u32 vr13;"
        "}"
        "{"
        "V0@(pr0)	vr30 = mov.u32 vr14;"
        "V1@(pr0)	vr14 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr31 = mov.u32 vr17;"
        "V1@(pr0)	vr17 = mov.u32 vr16;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr11;"
        "V1@(pr0)	vr15 = mov.u32 vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr16;"
        "V1@(pr0)	vr11 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr13;"
        "V1@(pr0)	vr13 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr28;"
        "V1@(pr0)	vr17 = mov.u32 vr29;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr30;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr13;"
        "V1@(pr0)	vr15 = mov.u32 vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr13 = mov.u32 vr17;"
        "V1@(pr0)	vr12 = mov.u32 vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "V0@(pr0)	vr3 = xor.u32 vr6, vr7;"
        "V1@(pr0)	vr3 = and.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr3 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 2047;"
        "pseudo@0	@pseudo imm_3 = 0;"
        "V0@(pr0)	vr7 = and.u32 vr7, r45;"
        "V1@(pr0)	vr6 = and.u32 vr6, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 1023;"
        "pseudo@0	@pseudo imm_3 = 0;"
        "V0@(pr0)	vr6 = add.s32 vr6, vr7;"
        "V1@(pr0)	vr6 = sub.s32 vr6, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 16;"
        "V0@(pr0)	vr1 = and.u32 vr10, r45;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr6, r48;"
        "V1@(pr0)	vmsk1 = eq.s32 vr1, r46;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr2, vr6;"
        "}"
        "{"
        "V1@(pr0)	vr2 = shl.u32 vr10, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V1@(pr0)	vr5 = shr.u32 vr11, r32;"
        "}"
        "{"
        "V0@(pr0)	vr2 = or.u32 vr2, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V1@(pr0)	vr5 = shl.u32 vr13, r32;"
        "}"
        "{"
        "V1@(pr0)	vr4 = shl.u32 vr11, r48;"
        "}"
        "{"
        "V0@(pr0)	vr5 = or.u32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr2;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr5;"
        "}"
        "{"
        "V1@(pr0)	vr2 = shl.u32 vr12, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V1@(pr0)	vr5 = shr.u32 vr13, r32;"
        "}"
        "{"
        "V0@(pr0)	vr2 = or.u32 vr2, vr5;"
        "}"
        "{"
        "V1@(pr0)	vr5 = shl.u32 vr13, r48;"
        "}"
        "{"
        "V0@(pr0)	vr12 = sel vmsk1 vr12, vr2;"
        "V1@(pr0)	vr13 = sel vmsk1 vr13, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 15;"
        "V0@(pr0)	vr1 = and.u32 vr10, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr2 = shl.u32 vr6, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = gt.s32 vr6, r46;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr10, vr3;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32768;"
        "V0@(pr0)	vr1 = add.s32 vr11, r48;"
        "V1@(pr0)	vmsk1 = gt.s32 vr12, r44;"
        "}"
        "{"
        "V0@(pr0)	vr11 = sel vmsk1 vr11, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr1 = and.u32 vr11, r48;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr11;"
        "V1@(pr0)	vr15 = mov.u32 vr10;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr1;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32768;"
        "V1@(pr0)	vmsk1 = eq.s32 vr12, r45;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr16;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr17;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr28 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr29 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr30 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr31 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr1 = and.u32 vr28, r44;"
        "V1@(pr0)	vr2 = and.u32 vr30, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = lseq.s32 vr1, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 8;"
        "V1@(pr0)	vr1 = or.u32 vr28, r45;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr1, vr10;"
        "V1@(pr0)	vr11 = sel vmsk1 vr29, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = lseq.s32 vr2, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 8;"
        "V1@(pr0)	vr1 = or.u32 vr30, r45;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr1, vr10;"
        "V1@(pr0)	vr11 = sel vmsk1 vr31, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32768;"
        "V0@(pr0)	vr3 = xor.u32 vr28, vr30;"
        "V1@(pr0)	vr3 = and.u32 vr3, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = eq.f32 vr1, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32760;"
        "V0@(pr0)	vr5 = mov.u32 r44;"
        "}"
        "{"
        "V0@(pr0)	vr4 = xor.u32 vr1, vr3;"
        "}"
        "{"
        "V1@(pr0)	vmsk2 = eq.s32 vr2, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr5;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = eq.f32 vr2, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32760;"
        "V0@(pr0)	vr5 = mov.u32 r44;"
        "}"
        "{"
        "V0@(pr0)	vr4 = xor.u32 vr2, vr3;"
        "}"
        "{"
        "V1@(pr0)	vmsk2 = eq.s32 vr1, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr5;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr11;"
        "}"
        "{"
        "V1@(pr0)	vmsk1 = eq.f32 vr1, r46;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr3;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, r46;"
        "}"
        "{"
        "V1@(pr0)	vmsk1 = eq.f32 vr2, r46;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr3;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32768;"
        "V0@(pr0)	vr12 = mov.u32 vr11;"
        "V1@(pr0)	vr13 = xor.u32 vr10, r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr1 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr11 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr10 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr1;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 32767;"
        "V0@(pr0)	vr14 = and.u32 vr10, r44;"
        "V1@(pr0)	vr15 = and.u32 vr11, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 32767;"
        "V0@(pr0)	vr16 = and.u32 vr12, r44;"
        "V1@(pr0)	vr17 = and.u32 vr13, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8192;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 5120;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr15;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 6144;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 7168;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = gt.s32 vr17, vr15;"
        "V1@(pr0)	vmsk1 = eq.s32 vr17, vr15;"
        "}"
        "{"
        "V0@(pr0)	vmsk2 = gt.s32 vr16, vr14;"
        "}"
        "{"
        "V0@(pr0)	vr1 = sel vmsk2 vr13, vr11;"
        "V1@(pr0)	vr0 = sel vmsk2 vr12, vr10;"
        "}"
        "{"
        "V0@(pr0)	vr1 = sel vmsk1 vr13, vr1;"
        "V1@(pr0)	vr0 = sel vmsk1 vr12, vr0;"
        "}"
        "{"
        "V0@(pr0)	vr1 = sel vmsk0 vr1, vr11;"
        "V1@(pr0)	vr0 = sel vmsk0 vr0, vr10;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = eq.s32 vr1, vr11;"
        "V1@(pr0)	vmsk1 = eq.s32 vr0, vr10;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr7 = sel vmsk0 vr5, r48;"
        "V1@(pr0)	vr6 = sel vmsk1 vr5, r48;"
        "}"
        "{"
        "V0@(pr0)	vr7 = and.u32 vr6, vr7;"
        "V1@(pr0)	vmsk0 = eq.s32 vr7, r48;"
        "}"
        "{"
        "V0@(pr0)	vr15 = sel vmsk0 vr11, vr13;"
        "V1@(pr0)	vr14 = sel vmsk0 vr10, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr1;"
        "V1@(pr0)	vr16 = mov.u32 vr0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2047;"
        "pseudo@0	@pseudo imm_1 = 20;"
        "V0@(pr0)	vr7 = mov.u32 r32;"
        "V1@(pr0)	vr6 = mov.u32 r33;"
        "}"
        "{"
        "V1@(pr0)	vr0 = shr.u32 vr15, vr6;"
        "}"
        "{"
        "V0@(pr0)	vr0 = and.u32 vr0, vr7;"
        "}"
        "{"
        "V1@(pr0)	vr1 = shr.u32 vr17, vr6;"
        "}"
        "{"
        "V0@(pr0)	vr1 = and.u32 vr1, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 15;"
        "V0@(pr0)	vr2 = and.u32 vr14, r44;"
        "V1@(pr0)	vr3 = and.u32 vr15, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 15;"
        "V0@(pr0)	vr4 = and.u32 vr16, r44;"
        "V1@(pr0)	vr5 = and.u32 vr17, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32768;"
        "V0@(pr0)	vr6 = and.u32 vr15, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32768;"
        "V0@(pr0)	vr7 = xor.u32 vr15, vr17;"
        "V1@(pr0)	vr7 = and.u32 vr7, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "pseudo@0	@pseudo imm_2 = 3;"
        "V0@(pr0)	vr3 = or.u32 vr3, r44;"
        "V1@(pr0)	vr3 = shl.u32 vr3, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 57344;"
        "pseudo@0	@pseudo imm_2 = 29;"
        "V0@(pr0)	vr28 = and.u32 vr2, r44;"
        "V1@(pr0)	vr28 = shr.u32 vr28, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3;"
        "V0@(pr0)	vr3 = or.u32 vr3, vr28;"
        "V1@(pr0)	vr2 = shl.u32 vr2, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "pseudo@0	@pseudo imm_2 = 3;"
        "V0@(pr0)	vr5 = or.u32 vr5, r44;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 57344;"
        "pseudo@0	@pseudo imm_2 = 29;"
        "V0@(pr0)	vr28 = and.u32 vr4, r44;"
        "V1@(pr0)	vr28 = shr.u32 vr28, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3;"
        "V0@(pr0)	vr5 = or.u32 vr5, vr28;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 64;"
        "V0@(pr0)	vr28 = sub.s32 vr0, vr1;"
        "V1@(pr0)	vr29 = mov.u32 r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr29 = sub.s32 vr29, vr28;"
        "V1@(pr0)	vmsk0 = gt.s32 vr29, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr31 = mov.u32 r32;"
        "V1@(pr0)	vr31 = sub.s32 vr31, vr29;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr30 = mov.u32 r44;"
        "}"
        "{"
        "V1@(pr0)	vr30 = shl.u32 vr30, vr31;"
        "}"
        "{"
        "V0@(pr0)	vr30 = and.u32 vr4, vr30;"
        "V1@(pr0)	vr30 = shr.u32 vr30, vr31;"
        "}"
        "{"
        "V1@(pr0)	vr11 = shl.u32 vr5, vr29;"
        "}"
        "{"
        "V0@(pr0)	vr11 = or.u32 vr11, vr30;"
        "V1@(pr0)	vr10 = shl.u32 vr4, vr29;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V1@(pr0)	vr30 = sub.s32 vr29, r32;"
        "}"
        "{"
        "V1@(pr0)	vr12 = shl.u32 vr4, vr30;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk0 vr10, r46;"
        "V1@(pr0)	vr11 = sel vmsk0 vr11, vr12;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = eq.s32 vr10, r46;"
        "V1@(pr0)	vmsk1 = eq.s32 vr11, r46;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 r46;"
        "V1@(pr0)	vr17 = sel vmsk0 vr11, r48;"
        "}"
        "{"
        "V0@(pr0)	vr16 = sel vmsk1 vr11, r48;"
        "V1@(pr0)	vr17 = add.s32 vr16, vr17;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vmsk0 = eq.s32 vr17, r32;"
        "V1@(pr0)	vr17 = sel vmsk0 vr11, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V1@(pr0)	vmsk0 = gt.s32 vr28, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 65535;"
        "V0@(pr0)	vr31 = mov.u32 r45;"
        "V1@(pr0)	vr30 = mov.u32 r32;"
        "}"
        "{"
        "V0@(pr0)	vr30 = sub.s32 vr30, vr28;"
        "V1@(pr0)	vr31 = shr.u32 vr31, vr30;"
        "}"
        "{"
        "V0@(pr0)	vr31 = and.u32 vr31, vr5;"
        "V1@(pr0)	vr31 = shl.u32 vr31, vr30;"
        "}"
        "{"
        "V1@(pr0)	vr10 = shr.u32 vr4, vr28;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr10, vr31;"
        "V1@(pr0)	vr11 = shr.u32 vr5, vr28;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr31 = sub.s32 vr28, r32;"
        "V1@(pr0)	vr12 = shr.u32 vr5, vr31;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk0 vr10, vr12;"
        "V1@(pr0)	vr11 = sel vmsk0 vr11, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 64;"
        "V0@(pr0)	vr10 = or.u32 vr10, vr17;"
        "V1@(pr0)	vmsk0 = ls.s32 vr28, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 r46;"
        "V1@(pr0)	vr11 = sel vmsk0 vr17, vr11;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 r48;"
        "V1@(pr0)	vr10 = sel vmsk0 vr17, vr10;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = eq.s32 vr28, r36;"
        "V1@(pr0)	vr4 = sel vmsk0 vr10, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "V0@(pr0)	vr5 = sel vmsk0 vr11, vr5;"
        "V1@(pr0)	vr10 = and.u32 vr2, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr12 = and.u32 vr4, r33;"
        "V1@(pr0)	vr11 = shr.u32 vr2, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr13 = shr.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = gteq.s32 vr10, vr12;"
        "V1@(pr0)	vr28 = mov.u32 r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr17 = sel vmsk0 vr28, r46;"
        "V1@(pr0)	vr17 = shl.u32 vr17, r32;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr10, vr17;"
        "V1@(pr0)	vr14 = sub.s32 vr10, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr15 = add.s32 vr13, r48;"
        "V1@(pr0)	vr13 = sel vmsk0 vr15, vr13;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = gteq.s32 vr11, vr13;"
        "V1@(pr0)	vr17 = sel vmsk0 vr28, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr17 = shl.u32 vr17, r32;"
        "}"
        "{"
        "V0@(pr0)	vr11 = or.u32 vr11, vr17;"
        "V1@(pr0)	vr15 = sub.s32 vr11, vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr14, vr15;"
        "}"
        "{"
        "V0@(pr0)	vr17 = add.s32 vr5, r48;"
        "V1@(pr0)	vr17 = sel vmsk0 vr17, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr11 = sub.s32 vr3, vr17;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = eq.s32 vr11, r46;"
        "V1@(pr0)	vr31 = count.u32 vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr30 = count.u32 vr10;"
        "V1@(pr0)	vr30 = add.s32 vr30, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr31 = sel vmsk0 vr31, vr30;"
        "V1@(pr0)	vr31 = sub.s32 vr31, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vmsk6 = ls.s32 vr31, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr30 = mov.u32 r32;"
        "V1@(pr0)	vr30 = sub.s32 vr30, vr31;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr29 = mov.u32 r44;"
        "V1@(pr0)	vr29 = shl.u32 vr29, vr30;"
        "}"
        "{"
        "V0@(pr0)	vr29 = and.u32 vr29, vr10;"
        "V1@(pr0)	vr29 = shr.u32 vr29, vr30;"
        "}"
        "{"
        "V1@(pr0)	vr13 = shl.u32 vr11, vr31;"
        "}"
        "{"
        "V0@(pr0)	vr13 = or.u32 vr13, vr29;"
        "V1@(pr0)	vr12 = shl.u32 vr10, vr31;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr30 = mov.u32 r32;"
        "V1@(pr0)	vr30 = sub.s32 vr31, vr30;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr14 = shl.u32 vr10, vr30;"
        "}"
        "{"
        "V0@(pr0)	vr12 = sel vmsk6 vr15, vr12;"
        "V1@(pr0)	vr13 = sel vmsk6 vr14, vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 128;"
        "V0@(pr0)	vmsk0 = ls.s32 vr11, r36;"
        "V1@(pr0)	vr10 = sel vmsk0 vr10, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr15 = sub.s32 vr0, vr31;"
        "}"
        "{"
        "V0@(pr0)	vr11 = sel vmsk0 vr11, vr13;"
        "V1@(pr0)	vr15 = sel vmsk0 vr0, vr15;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr12 = and.u32 vr2, r33;"
        "V1@(pr0)	vr13 = shr.u32 vr2, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr16 = and.u32 vr4, r33;"
        "V1@(pr0)	vr17 = shr.u32 vr4, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr28 = add.s32 vr12, vr16;"
        "V1@(pr0)	vr14 = shr.u32 vr28, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "V0@(pr0)	vr28 = and.u32 vr28, r32;"
        "}"
        "{"
        "V0@(pr0)	vr29 = add.s32 vr13, vr17;"
        "V1@(pr0)	vr29 = add.s32 vr29, vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr29, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "V0@(pr0)	vr29 = and.u32 vr29, r32;"
        "V1@(pr0)	vr29 = shl.u32 vr29, r33;"
        "}"
        "{"
        "V0@(pr0)	vr12 = or.u32 vr29, vr28;"
        "V1@(pr0)	vr13 = add.s32 vr3, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr13 = add.s32 vr13, vr14;"
        "V1@(pr0)	vr28 = and.u32 vr12, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V0@(pr0)	vr14 = and.u32 vr13, r48;"
        "V1@(pr0)	vr14 = shl.u32 vr14, r32;"
        "}"
        "{"
        "V1@(pr0)	vr16 = shr.u32 vr12, r48;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr16, vr14;"
        "V1@(pr0)	vr17 = shr.u32 vr13, r48;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr16, vr28;"
        "V1@(pr0)	vr14 = add.s32 vr0, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 256;"
        "V0@(pr0)	vr28 = and.u32 vr13, r36;"
        "V1@(pr0)	vmsk0 = eq.s32 vr28, r46;"
        "}"
        "{"
        "V0@(pr0)	vr12 = sel vmsk0 vr16, vr12;"
        "V1@(pr0)	vr13 = sel vmsk0 vr17, vr13;"
        "}"
        "{"
        "V0@(pr0)	vr14 = sel vmsk0 vr14, vr0;"
        "V1@(pr0)	vmsk1 = eq.s32 vr7, r46;"
        "}"
        "{"
        "V0@(pr0)	vr2 = sel vmsk1 vr10, vr12;"
        "V1@(pr0)	vr3 = sel vmsk1 vr11, vr13;"
        "}"
        "{"
        "V0@(pr0)	vr0 = sel vmsk1 vr15, vr14;"
        "V1@(pr0)	vr31 = mov.u32 r48;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr2, r48;"
        "V1@(pr0)	vmsk0 = lseq.s32 vr0, r46;"
        "}"
        "{"
        "V0@(pr0)	vr2 = sel vmsk0 vr2, vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 7;"
        "pseudo@0	@pseudo imm_1 = 3;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 65535;"
        "V0@(pr0)	vr17 = and.u32 vr2, r32;"
        "V1@(pr0)	vr31 = mov.u32 r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 29;"
        "V1@(pr0)	vr31 = shr.u32 vr31, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 29;"
        "V0@(pr0)	vr31 = and.u32 vr3, vr31;"
        "V1@(pr0)	vr31 = shl.u32 vr31, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3;"
        "V1@(pr0)	vr10 = shr.u32 vr2, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3;"
        "V0@(pr0)	vr10 = or.u32 vr10, vr31;"
        "V1@(pr0)	vr11 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 15;"
        "V0@(pr0)	vr10 = and.u32 vr10, r44;"
        "V1@(pr0)	vr11 = and.u32 vr11, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr28 = shl.u32 vr0, r32;"
        "}"
        "{"
        "V0@(pr0)	vr11 = or.u32 vr11, vr28;"
        "V1@(pr0)	vr11 = or.u32 vr11, vr6;"
        "}"
        "{"
        "V0@(pr0)	vr12 = add.s32 vr10, r48;"
        "V1@(pr0)	vr13 = and.u32 vr10, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "V0@(pr0)	vmsk0 = gt.s32 vr17, r32;"
        "V1@(pr0)	vmsk1 = eq.s32 vr17, r32;"
        "}"
        "{"
        "V0@(pr0)	vr13 = add.s32 vr10, vr13;"
        "V1@(pr0)	vr13 = sel vmsk1 vr10, vr13;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk0 vr13, vr12;"
        "V1@(pr0)	vr0 = mov.u32 vr10;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mov.u32 vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr10 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr11 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr12 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr13 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 5120;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 6144;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr16 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 7168;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr17 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8192;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = eq.s32 vr17, r46;"
        "V1@(pr0)	vmsk1 = eq.s32 vr16, r46;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 r46;"
        "V1@(pr0)	vr7 = sel vmsk0 vr28, r48;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr28, r48;"
        "V1@(pr0)	vr7 = add.s32 vr6, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vmsk0 = eq.s32 vr7, r32;"
        "V1@(pr0)	vr30 = sel vmsk0 vr0, vr10;"
        "}"
        "{"
        "V0@(pr0)	vr31 = sel vmsk0 vr1, vr11;"
        "V1@(pr0)	vr6 = and.u32 vr10, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr7 = and.u32 vr11, vr13;"
        "V1@(pr0)	vr5 = sel vmsk0 vr13, vr7;"
        "}"
        "{"
        "V0@(pr0)	vr4 = sel vmsk0 vr12, vr6;"
        "V1@(pr0)	vmsk0 = eq.s32 vr14, r46;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = eq.s32 vr15, r46;"
        "V1@(pr0)	vr6 = sel vmsk0 vr28, r48;"
        "}"
        "{"
        "V0@(pr0)	vr7 = sel vmsk1 vr28, r48;"
        "V1@(pr0)	vr7 = add.s32 vr6, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vmsk0 = eq.s32 vr7, r32;"
        "V1@(pr0)	vr31 = sel vmsk0 vr31, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr30 = sel vmsk0 vr30, r34;"
        "V1@(pr0)	vr7 = mov.u32 r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 15;"
        "V0@(pr0)	vr6 = mov.u32 r44;"
        "V1@(pr0)	vr7 = xor.u32 vr7, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr6 = xor.u32 vr6, r44;"
        "V1@(pr0)	vmsk0 = eq.s32 vr16, vr6;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mov.u32 vr7;"
        "V1@(pr0)	vr4 = mov.u32 vr6;"
        "}"
        "{"
        "V0@(pr0)	vmsk6 = eq.s32 vr14, vr6;"
        "V1@(pr0)	vmsk7 = eq.s32 vr15, vr7;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = eq.s32 vr17, vr7;"
        "V1@(pr0)	vr7 = sel vmsk0 vr28, r48;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr28, r48;"
        "V1@(pr0)	vr7 = add.s32 vr6, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vmsk0 = eq.s32 vr7, r32;"
        "V1@(pr0)	vr31 = sel vmsk0 vr31, vr13;"
        "}"
        "{"
        "V0@(pr0)	vr30 = sel vmsk0 vr30, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr7 = sel vmsk7 vr28, r48;"
        "V1@(pr0)	vr6 = sel vmsk6 vr28, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vr7 = add.s32 vr6, vr7;"
        "V1@(pr0)	vmsk7 = eq.s32 vr7, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr3 = or.u32 vr5, r36;"
        "V1@(pr0)	vr6 = xor.u32 vr10, vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32768;"
        "V0@(pr0)	vr7 = xor.u32 vr11, vr13;"
        "V1@(pr0)	vmsk0 = eq.s32 vr7, r36;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = eq.s32 vr6, r46;"
        "V1@(pr0)	vr7 = sel vmsk0 vr28, r48;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr28, r48;"
        "V1@(pr0)	vr7 = add.s32 vr6, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vmsk0 = eq.s32 vr7, r32;"
        "V1@(pr0)	vr7 = sel vmsk0 vr11, vr3;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk0 vr10, vr4;"
        "V1@(pr0)	vr30 = sel vmsk7 vr30, vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr31 = sel vmsk7 vr31, vr7;"
        "V1@(pr0)	vr7 = or.u32 vr13, r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk0 = eq.s32 vr16, r46;"
        "V1@(pr0)	vr6 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "V0@(pr0)	vr28 = and.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32752;"
        "V0@(pr0)	vmsk1 = gt.s32 vr6, r32;"
        "V1@(pr0)	vmsk2 = eq.s32 vr28, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk0 vr28, r48;"
        "V1@(pr0)	vr4 = sel vmsk2 vr28, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vr5 = add.s32 vr4, vr5;"
        "V1@(pr0)	vmsk2 = eq.s32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr7, vr31;"
        "V1@(pr0)	vr4 = sel vmsk2 vr12, vr30;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32752;"
        "V0@(pr0)	vmsk2 = eq.s32 vr6, r32;"
        "V1@(pr0)	vr4 = sel vmsk2 vr30, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr31, vr5;"
        "V1@(pr0)	vr31 = sel vmsk1 vr5, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr30 = sel vmsk1 vr4, vr12;"
        "V1@(pr0)	vr7 = or.u32 vr11, r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk0 = eq.s32 vr14, r46;"
        "V1@(pr0)	vr6 = shr.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "V0@(pr0)	vr28 = and.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32752;"
        "V0@(pr0)	vmsk1 = gt.s32 vr6, r32;"
        "V1@(pr0)	vmsk2 = eq.s32 vr28, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk0 vr28, r48;"
        "V1@(pr0)	vr4 = sel vmsk2 vr28, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vr5 = add.s32 vr4, vr5;"
        "V1@(pr0)	vmsk2 = eq.s32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr7, vr31;"
        "V1@(pr0)	vr4 = sel vmsk2 vr10, vr30;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32752;"
        "V0@(pr0)	vmsk2 = eq.s32 vr6, r32;"
        "V1@(pr0)	vr4 = sel vmsk2 vr30, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr31, vr5;"
        "V1@(pr0)	vr11 = sel vmsk1 vr5, vr7;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr4, vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr12 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr1 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V1@(pr0)	vr2 = shr.u32 vr1, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 5120;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr12;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 26;"
        "V1@(pr0)	vr1 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 15;"
        "V1@(pr0)	vr1 = and.u32 vr1, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_2 = 162;"
        "pseudo@0	@pseudo imm_3 = 0;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 28238;"
        "pseudo@0	@pseudo imm_1 = 63875;"
        "pseudo@0	@pseudo imm_2 = 10748;"
        "pseudo@0	@pseudo imm_3 = 17429;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "pseudo@0	@pseudo imm_2 = 41721;"
        "pseudo@0	@pseudo imm_3 = 0;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20036;"
        "pseudo@0	@pseudo imm_1 = 33646;"
        "pseudo@0	@pseudo imm_2 = 64551;"
        "pseudo@0	@pseudo imm_3 = 5417;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "pseudo@0	@pseudo imm_2 = 63875;"
        "pseudo@0	@pseudo imm_3 = 162;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 17429;"
        "pseudo@0	@pseudo imm_1 = 28238;"
        "pseudo@0	@pseudo imm_2 = 10071;"
        "pseudo@0	@pseudo imm_3 = 10748;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3;"
        "pseudo@0	@pseudo imm_2 = 33646;"
        "pseudo@0	@pseudo imm_3 = 41721;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 5417;"
        "pseudo@0	@pseudo imm_1 = 20036;"
        "pseudo@0	@pseudo imm_2 = 22481;"
        "pseudo@0	@pseudo imm_3 = 64551;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "pseudo@0	@pseudo imm_2 = 28238;"
        "pseudo@0	@pseudo imm_3 = 63875;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 10748;"
        "pseudo@0	@pseudo imm_1 = 17429;"
        "pseudo@0	@pseudo imm_2 = 53749;"
        "pseudo@0	@pseudo imm_3 = 10071;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 5;"
        "pseudo@0	@pseudo imm_2 = 20036;"
        "pseudo@0	@pseudo imm_3 = 33646;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 64551;"
        "pseudo@0	@pseudo imm_1 = 5417;"
        "pseudo@0	@pseudo imm_2 = 62772;"
        "pseudo@0	@pseudo imm_3 = 22481;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 6;"
        "pseudo@0	@pseudo imm_2 = 17429;"
        "pseudo@0	@pseudo imm_3 = 28238;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 10071;"
        "pseudo@0	@pseudo imm_1 = 10748;"
        "pseudo@0	@pseudo imm_2 = 13533;"
        "pseudo@0	@pseudo imm_3 = 53749;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 7;"
        "pseudo@0	@pseudo imm_2 = 5417;"
        "pseudo@0	@pseudo imm_3 = 20036;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 22481;"
        "pseudo@0	@pseudo imm_1 = 64551;"
        "pseudo@0	@pseudo imm_2 = 56768;"
        "pseudo@0	@pseudo imm_3 = 62772;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "pseudo@0	@pseudo imm_2 = 10748;"
        "pseudo@0	@pseudo imm_3 = 17429;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 53749;"
        "pseudo@0	@pseudo imm_1 = 10071;"
        "pseudo@0	@pseudo imm_2 = 49371;"
        "pseudo@0	@pseudo imm_3 = 13533;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 9;"
        "pseudo@0	@pseudo imm_2 = 64551;"
        "pseudo@0	@pseudo imm_3 = 5417;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 62772;"
        "pseudo@0	@pseudo imm_1 = 22481;"
        "pseudo@0	@pseudo imm_2 = 56162;"
        "pseudo@0	@pseudo imm_3 = 56768;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 10;"
        "pseudo@0	@pseudo imm_2 = 10071;"
        "pseudo@0	@pseudo imm_3 = 10748;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 13533;"
        "pseudo@0	@pseudo imm_1 = 53749;"
        "pseudo@0	@pseudo imm_2 = 25237;"
        "pseudo@0	@pseudo imm_3 = 49371;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 11;"
        "pseudo@0	@pseudo imm_2 = 22481;"
        "pseudo@0	@pseudo imm_3 = 64551;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 56768;"
        "pseudo@0	@pseudo imm_1 = 62772;"
        "pseudo@0	@pseudo imm_2 = 38297;"
        "pseudo@0	@pseudo imm_3 = 56162;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 12;"
        "pseudo@0	@pseudo imm_2 = 53749;"
        "pseudo@0	@pseudo imm_3 = 10071;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 49371;"
        "pseudo@0	@pseudo imm_1 = 13533;"
        "pseudo@0	@pseudo imm_2 = 39228;"
        "pseudo@0	@pseudo imm_3 = 25237;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 13;"
        "pseudo@0	@pseudo imm_2 = 62772;"
        "pseudo@0	@pseudo imm_3 = 22481;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 56162;"
        "pseudo@0	@pseudo imm_1 = 56768;"
        "pseudo@0	@pseudo imm_2 = 15427;"
        "pseudo@0	@pseudo imm_3 = 38297;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 14;"
        "pseudo@0	@pseudo imm_2 = 13533;"
        "pseudo@0	@pseudo imm_3 = 53749;"
        "V0@(pr0)	vmsk1 = eq.s32 vr1, r32;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 25237;"
        "pseudo@0	@pseudo imm_1 = 49371;"
        "pseudo@0	@pseudo imm_2 = 17296;"
        "pseudo@0	@pseudo imm_3 = 39228;"
        "V0@(pr0)	vr29 = sel vmsk1 vr29, r44;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 23;"
        "V1@(pr0)	vr1 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 7;"
        "V1@(pr0)	vr1 = and.u32 vr1, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 255;"
        "pseudo@0	@pseudo imm_2 = 128;"
        "V0@(pr0)	vr10 = and.u32 vr10, r44;"
        "V1@(pr0)	vr10 = or.u32 vr10, r38;"
        "}"
        "{"
        "V1@(pr0)	vr10 = shl.u32 vr10, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr10;"
        "V1@(pr0)	vr13 = mov.u32 vr28;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr14;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr12, r54;"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr12, r32;"
        "}"
        "{"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr13, r32;"
        "}"
        "{"
        "V1@(pr0)	vr15 = and.u32 vr12, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr16;"
        "V1@(pr0)	vr13 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 vr13;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr10;"
        "V1@(pr0)	vr13 = mov.u32 vr30;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr14;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr12, r54;"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr12, r32;"
        "}"
        "{"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr13, r32;"
        "}"
        "{"
        "V1@(pr0)	vr15 = and.u32 vr12, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr16;"
        "V1@(pr0)	vr13 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr30 = mov.u32 vr12;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr10;"
        "V1@(pr0)	vr13 = mov.u32 vr29;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr14;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr12, r54;"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr12, r32;"
        "}"
        "{"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr13, r32;"
        "}"
        "{"
        "V1@(pr0)	vr15 = and.u32 vr12, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr16;"
        "V1@(pr0)	vr13 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr12;"
        "V1@(pr0)	vr17 = mov.u32 vr13;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 vr28;"
        "V1@(pr0)	vr14 = mov.u32 vr30;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8192;"
        "V0@(pr0)	vr1 = add.s32 vr2, r32;"
        "V1@(pr0)	vr1 = and.u32 vr1, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 14;"
        "V1@(pr0)	vr1 = shr.u32 vr1, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 14;"
        "V1@(pr0)	vr3 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V1@(pr0)	vr2 = sub.s32 vr2, vr3;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr16 = and.u32 vr16, r54;"
        "V1@(pr0)	vr2 = shl.u32 vr2, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr16, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr1;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32768;"
        "V0@(pr0)	vr12 = mov.u32 r44;"
        "V1@(pr0)	vmsk7 = gteq.f32 vr16, r46;"
        "}"
        "{"
        "V1@(pr0)	vr12 = sel vmsk7 vr12, r46;"
        "}"
        "{"
        "V0@(pr0)	vmsk3 = eq.s32 vr17, r46;"
        "V1@(pr0)	vr6 = sub.s32 vr17, r48;"
        "}"
        "{"
        "V1@(pr0)	vr7 = sub.s32 vr16, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr6 = sel vmsk3 vr6, r44;"
        "V1@(pr0)	vr7 = sel vmsk3 vr16, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr6 = xor.u32 vr6, r44;"
        "V1@(pr0)	vr7 = xor.u32 vr7, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr17 = sel vmsk7 vr6, vr17;"
        "V1@(pr0)	vr16 = sel vmsk7 vr7, vr16;"
        "}"
        "{"
        "V0@(pr0)	vr2 = count.u32 vr16;"
        "V1@(pr0)	vr1 = count.u32 vr17;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vmsk6 = ls.s32 vr2, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V0@(pr0)	vr6 = mov.u32 r32;"
        "V1@(pr0)	vr1 = sub.s32 vr6, vr1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 63;"
        "V0@(pr0)	vr6 = mov.u32 r32;"
        "V1@(pr0)	vr2 = sub.s32 vr6, vr2;"
        "}"
        "{"
        "V1@(pr0)	vr13 = sel vmsk6 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr0 = mov.u32 vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1023;"
        "V0@(pr0)	vr13 = add.s32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr13 = shl.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 20;"
        "V0@(pr0)	vmsk5 = lseq.s32 vr0, r32;"
        "V1@(pr0)	vmsk4 = lseq.s32 vr0, r33;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr6 = sub.s32 vr0, r32;"
        "V1@(pr0)	vr7 = mov.u32 r48;"
        "}"
        "{"
        "V1@(pr0)	vr6 = shl.u32 vr7, vr6;"
        "}"
        "{"
        "V1@(pr0)	vr6 = sub.s32 vr16, vr6;"
        "}"
        "{"
        "V1@(pr0)	vr7 = shl.u32 vr7, vr0;"
        "}"
        "{"
        "V1@(pr0)	vr7 = sub.s32 vr17, vr7;"
        "}"
        "{"
        "V1@(pr0)	vr5 = sel vmsk4 vr6, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "V0@(pr0)	vr6 = sub.s32 vr0, r32;"
        "V1@(pr0)	vr3 = shr.u32 vr5, vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 84;"
        "V0@(pr0)	vr6 = mov.u32 r32;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sub.s32 vr6, vr0;"
        "V1@(pr0)	vr4 = shl.u32 vr5, vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "V0@(pr0)	vr6 = sub.s32 vr0, r32;"
        "V1@(pr0)	vr6 = shr.u32 vr17, vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V0@(pr0)	vr4 = add.s32 vr4, vr6;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 vr3;"
        "V1@(pr0)	vr14 = mov.u32 vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "V0@(pr0)	vr6 = mov.u32 r32;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sub.s32 vr6, vr0;"
        "V1@(pr0)	vr3 = shl.u32 vr5, vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V0@(pr0)	vr6 = sub.s32 vr0, r32;"
        "V1@(pr0)	vr6 = shr.u32 vr17, vr6;"
        "}"
        "{"
        "V1@(pr0)	vr3 = add.s32 vr3, vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "V0@(pr0)	vr7 = mov.u32 r32;"
        "}"
        "{"
        "V0@(pr0)	vr7 = sub.s32 vr7, vr0;"
        "V1@(pr0)	vr4 = shl.u32 vr17, vr7;"
        "}"
        "{"
        "V0@(pr0)	vr15 = sel vmsk5 vr15, vr3;"
        "V1@(pr0)	vr14 = sel vmsk5 vr14, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V0@(pr0)	vr6 = mov.u32 r32;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sub.s32 vr6, vr0;"
        "V1@(pr0)	vr3 = shl.u32 vr5, vr6;"
        "}"
        "{"
        "V0@(pr0)	vr15 = sel vmsk4 vr15, vr3;"
        "V1@(pr0)	vr14 = sel vmsk4 vr14, r46;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr12, vr13;"
        "V1@(pr0)	vr10 = or.u32 vr10, vr15;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8699;"
        "pseudo@0	@pseudo imm_1 = 15385;"
        "pseudo@0	@pseudo imm_2 = 11544;"
        "pseudo@0	@pseudo imm_3 = 21572;"
        "V0@(pr0)	vr12 = mov.u32 r44;"
        "V1@(pr0)	vr13 = mov.u32 r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr13;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr1 = and.u32 vr10, r44;"
        "V1@(pr0)	vr2 = and.u32 vr12, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "V0@(pr0)	vmsk1 = ls.f32 vr1, r44;"
        "V1@(pr0)	vmsk2 = ls.f32 vr2, r44;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, r46;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, r46;"
        "}"
        "{"
        "V0@(pr0)	vr12 = sel vmsk2 vr12, r46;"
        "V1@(pr0)	vr13 = sel vmsk2 vr13, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr6 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr7 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 15;"
        "V0@(pr0)	vr10 = and.u32 vr10, r44;"
        "V1@(pr0)	vr12 = and.u32 vr12, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "V0@(pr0)	vr10 = or.u32 vr10, r44;"
        "V1@(pr0)	vr12 = or.u32 vr12, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 11;"
        "V1@(pr0)	vr12 = shl.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 21;"
        "V1@(pr0)	vr1 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 11;"
        "V0@(pr0)	vr12 = or.u32 vr12, vr1;"
        "V1@(pr0)	vr13 = shl.u32 vr13, r32;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr13, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = and.u32 vr13, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = and.u32 vr12, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr13, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr12, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr11, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr13, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = and.u32 vr12, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr11, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = and.u32 vr12, r54;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr14;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = and.u32 vr10, r54;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr10, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shr.u32 vr12, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 255;"
        "V0@(pr0)	vr2 = and.u32 vr15, r32;"
        "V1@(pr0)	vr3 = cvtinttof.f32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr4 = cvtinttof.f32 vr14;"
        "V1@(pr0)	vr1 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr3, vr4;"
        "V1@(pr0)	vr5 = cvtinttof.f32 vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = cvtftoint.s32 vr3, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = cvtftoint.s32 vr5, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr5, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr1 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = sel vmsk1 vr15, r48;"
        "V1@(pr0)	vr2 = shr.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.s32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr1, r32;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr3, r54;"
        "V1@(pr0)	vr5 = and.u32 vr5, r54;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr3, vr5;"
        "V1@(pr0)	vr2 = and.u32 vr2, r54;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 r46;"
        "V1@(pr0)	vr10 = mov.u32 vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr10;"
        "V1@(pr0)	vr17 = mov.u32 vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr15 = or.u32 vr1, vr2;"
        "V1@(pr0)	vr14 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr12 = mov.u32 r46;"
        "V1@(pr0)	vr13 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "V1@(pr0)	vr15 = shr.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 vr15;"
        "V1@(pr0)	vr29 = mov.u32 vr14;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr15;"
        "V1@(pr0)	vr15 = mov.u32 vr0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr16;"
        "V1@(pr0)	vr11 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr12;"
        "V1@(pr0)	vr17 = mov.u32 vr13;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr29;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 vr15;"
        "V1@(pr0)	vr29 = mov.u32 vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr1 = shl.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr14 = or.u32 vr1, vr2;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr16;"
        "V1@(pr0)	vr13 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr10;"
        "V1@(pr0)	vr17 = mov.u32 vr11;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr28;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr16;"
        "V1@(pr0)	vr11 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr29 = shl.u32 vr29, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr12;"
        "V1@(pr0)	vr17 = mov.u32 vr13;"
        "}"
        "{"
        "V0@(pr0)	vr30 = mov.u32 vr14;"
        "V1@(pr0)	vr14 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr31 = mov.u32 vr17;"
        "V1@(pr0)	vr17 = mov.u32 vr16;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr11;"
        "V1@(pr0)	vr15 = mov.u32 vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr16;"
        "V1@(pr0)	vr11 = mov.u32 vr17;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr13;"
        "V1@(pr0)	vr13 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr28;"
        "V1@(pr0)	vr17 = mov.u32 vr29;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr30;"
        "V1@(pr0)	vr15 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr13;"
        "V1@(pr0)	vr15 = mov.u32 vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr13 = mov.u32 vr17;"
        "V1@(pr0)	vr12 = mov.u32 vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "V0@(pr0)	vr3 = xor.u32 vr6, vr7;"
        "V1@(pr0)	vr3 = and.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr3 = shl.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 2047;"
        "pseudo@0	@pseudo imm_3 = 0;"
        "V0@(pr0)	vr7 = and.u32 vr7, r45;"
        "V1@(pr0)	vr6 = and.u32 vr6, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 1023;"
        "pseudo@0	@pseudo imm_3 = 0;"
        "V0@(pr0)	vr6 = add.s32 vr6, vr7;"
        "V1@(pr0)	vr6 = sub.s32 vr6, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 16;"
        "V0@(pr0)	vr1 = and.u32 vr10, r45;"
        "}"
        "{"
        "V0@(pr0)	vr2 = add.s32 vr6, r48;"
        "V1@(pr0)	vmsk1 = eq.s32 vr1, r46;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr2, vr6;"
        "}"
        "{"
        "V1@(pr0)	vr2 = shl.u32 vr10, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V1@(pr0)	vr5 = shr.u32 vr11, r32;"
        "}"
        "{"
        "V0@(pr0)	vr2 = or.u32 vr2, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V1@(pr0)	vr5 = shl.u32 vr13, r32;"
        "}"
        "{"
        "V1@(pr0)	vr4 = shl.u32 vr11, r48;"
        "}"
        "{"
        "V0@(pr0)	vr5 = or.u32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr2;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr5;"
        "}"
        "{"
        "V1@(pr0)	vr2 = shl.u32 vr12, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V1@(pr0)	vr5 = shr.u32 vr13, r32;"
        "}"
        "{"
        "V0@(pr0)	vr2 = or.u32 vr2, vr5;"
        "}"
        "{"
        "V1@(pr0)	vr5 = shl.u32 vr13, r48;"
        "}"
        "{"
        "V0@(pr0)	vr12 = sel vmsk1 vr12, vr2;"
        "V1@(pr0)	vr13 = sel vmsk1 vr13, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 15;"
        "V0@(pr0)	vr1 = and.u32 vr10, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr2 = shl.u32 vr6, r32;"
        "}"
        "{"
        "V0@(pr0)	vr1 = or.u32 vr1, vr2;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = gt.s32 vr6, r46;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr10, vr3;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32768;"
        "V0@(pr0)	vr1 = add.s32 vr11, r48;"
        "V1@(pr0)	vmsk1 = gt.s32 vr12, r44;"
        "}"
        "{"
        "V0@(pr0)	vr11 = sel vmsk1 vr11, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr1 = and.u32 vr11, r48;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr11;"
        "V1@(pr0)	vr15 = mov.u32 vr10;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr1;"
        "V1@(pr0)	vr16 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vr1 = shl.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr14, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk1 = carry.u32 vr1, vr2;"
        "V1@(pr0)	vr4 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = shr.u32 vr14, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr3;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr4, r36;"
        "V1@(pr0)	vr1 = sel vmsk1 vr0, r48;"
        "}"
        "{"
        "V0@(pr0)	vr3 = and.u32 vr17, r54;"
        "V1@(pr0)	vr5 = and.u32 vr14, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = add.s32 vr5, vr3;"
        "V1@(pr0)	vr3 = and.u32 vr3, r54;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = or.u32 vr3, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shl.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr3 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = carry.u32 vr2, vr3;"
        "V1@(pr0)	vr4 = and.u32 vr16, r54;"
        "}"
        "{"
        "V0@(pr0)	vr3 = sel vmsk1 vr0, r48;"
        "V1@(pr0)	vr5 = and.u32 vr15, r54;"
        "}"
        "{"
        "V0@(pr0)	vr4 = add.s32 vr4, vr1;"
        "V1@(pr0)	vr4 = add.s32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr4 = and.u32 vr4, r54;"
        "V1@(pr0)	vr1 = shr.u32 vr16, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr2 = shr.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = add.s32 vr1, vr2;"
        "V1@(pr0)	vr5 = add.s32 vr3, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr5, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32768;"
        "V1@(pr0)	vmsk1 = eq.s32 vr12, r45;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr16;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr17;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr28 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr29 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr30 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr31 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr1 = and.u32 vr28, r44;"
        "V1@(pr0)	vr2 = and.u32 vr30, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = lseq.s32 vr1, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 8;"
        "V1@(pr0)	vr1 = or.u32 vr28, r45;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr1, vr10;"
        "V1@(pr0)	vr11 = sel vmsk1 vr29, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = lseq.s32 vr2, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 8;"
        "V1@(pr0)	vr1 = or.u32 vr30, r45;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr1, vr10;"
        "V1@(pr0)	vr11 = sel vmsk1 vr31, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32768;"
        "V0@(pr0)	vr3 = xor.u32 vr28, vr30;"
        "V1@(pr0)	vr3 = and.u32 vr3, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = eq.f32 vr1, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32760;"
        "V0@(pr0)	vr5 = mov.u32 r44;"
        "}"
        "{"
        "V0@(pr0)	vr4 = xor.u32 vr1, vr3;"
        "}"
        "{"
        "V1@(pr0)	vmsk2 = eq.s32 vr2, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr5;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32752;"
        "V1@(pr0)	vmsk1 = eq.f32 vr2, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32760;"
        "V0@(pr0)	vr5 = mov.u32 r44;"
        "}"
        "{"
        "V0@(pr0)	vr4 = xor.u32 vr2, vr3;"
        "}"
        "{"
        "V1@(pr0)	vmsk2 = eq.s32 vr1, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr5, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr5;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, vr11;"
        "}"
        "{"
        "V1@(pr0)	vmsk1 = eq.f32 vr1, r46;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr3;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, r46;"
        "}"
        "{"
        "V1@(pr0)	vmsk1 = eq.f32 vr2, r46;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr3;"
        "V1@(pr0)	vr11 = sel vmsk1 vr11, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr12 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr5 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr4 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr1 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr2 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr3 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 5120;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32768;"
        "V0@(pr0)	vmsk1 = eq.s32 vr4, r48;"
        "V1@(pr0)	vr6 = xor.u32 vr10, r36;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr10, vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 17136;"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 20224;"
        "V0@(pr0)	vr6 = and.u32 vr5, r45;"
        "V1@(pr0)	vmsk4 = ls.f32 vr6, r44;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk4 vr10, vr2;"
        "V1@(pr0)	vr11 = sel vmsk4 vr11, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr13 = sel vmsk4 vr12, vr3;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 49152;"
        "pseudo@0	@pseudo imm_2 = 3;"
        "V0@(pr0)	vr4 = and.u32 vr10, r44;"
        "V1@(pr0)	vr1 = shl.u32 vr10, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 16383;"
        "V0@(pr0)	vr1 = and.u32 vr1, r44;"
        "V1@(pr0)	vr4 = or.u32 vr1, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 29;"
        "V1@(pr0)	vr1 = shr.u32 vr11, r32;"
        "}"
        "{"
        "V0@(pr0)	vr4 = or.u32 vr1, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr13;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 29;"
        "V1@(pr0)	vr12 = shl.u32 vr4, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 49152;"
        "pseudo@0	@pseudo imm_2 = 3;"
        "V0@(pr0)	vr6 = and.u32 vr4, r44;"
        "V1@(pr0)	vr2 = shr.u32 vr4, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 2047;"
        "V0@(pr0)	vr2 = and.u32 vr2, r44;"
        "V1@(pr0)	vr6 = or.u32 vr2, vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 2048;"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 14336;"
        "V0@(pr0)	vr2 = and.u32 vr4, r44;"
        "V1@(pr0)	vr3 = or.u32 vr6, r45;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = eq.s32 vr2, r46;"
        "V1@(pr0)	vr6 = sel vmsk1 vr3, vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32768;"
        "V1@(pr0)	vr13 = xor.u32 vr6, r36;"
        "}"
        "{"
        "V0@(pr0)	vr2 = mov.u32 vr10;"
        "V1@(pr0)	vr10 = mov.u32 vr11;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 32767;"
        "V0@(pr0)	vr14 = and.u32 vr10, r44;"
        "V1@(pr0)	vr15 = and.u32 vr11, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 32767;"
        "V0@(pr0)	vr16 = and.u32 vr12, r44;"
        "V1@(pr0)	vr17 = and.u32 vr13, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8192;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S1@(pr0)	r0 = sub.s32 r0, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 5120;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr15;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 6144;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 7168;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VS@(pr0)	 [vmem:r1+r64,r34,$255] = st vr17;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = gt.s32 vr17, vr15;"
        "V1@(pr0)	vmsk1 = eq.s32 vr17, vr15;"
        "}"
        "{"
        "V0@(pr0)	vmsk2 = gt.s32 vr16, vr14;"
        "}"
        "{"
        "V0@(pr0)	vr1 = sel vmsk2 vr13, vr11;"
        "V1@(pr0)	vr0 = sel vmsk2 vr12, vr10;"
        "}"
        "{"
        "V0@(pr0)	vr1 = sel vmsk1 vr13, vr1;"
        "V1@(pr0)	vr0 = sel vmsk1 vr12, vr0;"
        "}"
        "{"
        "V0@(pr0)	vr1 = sel vmsk0 vr1, vr11;"
        "V1@(pr0)	vr0 = sel vmsk0 vr0, vr10;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = eq.s32 vr1, vr11;"
        "V1@(pr0)	vmsk1 = eq.s32 vr0, vr10;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr7 = sel vmsk0 vr5, r48;"
        "V1@(pr0)	vr6 = sel vmsk1 vr5, r48;"
        "}"
        "{"
        "V0@(pr0)	vr7 = and.u32 vr6, vr7;"
        "V1@(pr0)	vmsk0 = eq.s32 vr7, r48;"
        "}"
        "{"
        "V0@(pr0)	vr15 = sel vmsk0 vr11, vr13;"
        "V1@(pr0)	vr14 = sel vmsk0 vr10, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 vr1;"
        "V1@(pr0)	vr16 = mov.u32 vr0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2047;"
        "pseudo@0	@pseudo imm_1 = 20;"
        "V0@(pr0)	vr7 = mov.u32 r32;"
        "V1@(pr0)	vr6 = mov.u32 r33;"
        "}"
        "{"
        "V1@(pr0)	vr0 = shr.u32 vr15, vr6;"
        "}"
        "{"
        "V0@(pr0)	vr0 = and.u32 vr0, vr7;"
        "}"
        "{"
        "V1@(pr0)	vr1 = shr.u32 vr17, vr6;"
        "}"
        "{"
        "V0@(pr0)	vr1 = and.u32 vr1, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 15;"
        "V0@(pr0)	vr2 = and.u32 vr14, r44;"
        "V1@(pr0)	vr3 = and.u32 vr15, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 15;"
        "V0@(pr0)	vr4 = and.u32 vr16, r44;"
        "V1@(pr0)	vr5 = and.u32 vr17, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32768;"
        "V0@(pr0)	vr6 = and.u32 vr15, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32768;"
        "V0@(pr0)	vr7 = xor.u32 vr15, vr17;"
        "V1@(pr0)	vr7 = and.u32 vr7, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "pseudo@0	@pseudo imm_2 = 3;"
        "V0@(pr0)	vr3 = or.u32 vr3, r44;"
        "V1@(pr0)	vr3 = shl.u32 vr3, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 57344;"
        "pseudo@0	@pseudo imm_2 = 29;"
        "V0@(pr0)	vr28 = and.u32 vr2, r44;"
        "V1@(pr0)	vr28 = shr.u32 vr28, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3;"
        "V0@(pr0)	vr3 = or.u32 vr3, vr28;"
        "V1@(pr0)	vr2 = shl.u32 vr2, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "pseudo@0	@pseudo imm_2 = 3;"
        "V0@(pr0)	vr5 = or.u32 vr5, r44;"
        "V1@(pr0)	vr5 = shl.u32 vr5, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 57344;"
        "pseudo@0	@pseudo imm_2 = 29;"
        "V0@(pr0)	vr28 = and.u32 vr4, r44;"
        "V1@(pr0)	vr28 = shr.u32 vr28, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3;"
        "V0@(pr0)	vr5 = or.u32 vr5, vr28;"
        "V1@(pr0)	vr4 = shl.u32 vr4, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 64;"
        "V0@(pr0)	vr28 = sub.s32 vr0, vr1;"
        "V1@(pr0)	vr29 = mov.u32 r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr29 = sub.s32 vr29, vr28;"
        "V1@(pr0)	vmsk0 = gt.s32 vr29, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr31 = mov.u32 r32;"
        "V1@(pr0)	vr31 = sub.s32 vr31, vr29;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr30 = mov.u32 r44;"
        "}"
        "{"
        "V1@(pr0)	vr30 = shl.u32 vr30, vr31;"
        "}"
        "{"
        "V0@(pr0)	vr30 = and.u32 vr4, vr30;"
        "V1@(pr0)	vr30 = shr.u32 vr30, vr31;"
        "}"
        "{"
        "V1@(pr0)	vr11 = shl.u32 vr5, vr29;"
        "}"
        "{"
        "V0@(pr0)	vr11 = or.u32 vr11, vr30;"
        "V1@(pr0)	vr10 = shl.u32 vr4, vr29;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V1@(pr0)	vr30 = sub.s32 vr29, r32;"
        "}"
        "{"
        "V1@(pr0)	vr12 = shl.u32 vr4, vr30;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk0 vr10, r46;"
        "V1@(pr0)	vr11 = sel vmsk0 vr11, vr12;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = eq.s32 vr10, r46;"
        "V1@(pr0)	vmsk1 = eq.s32 vr11, r46;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 r46;"
        "V1@(pr0)	vr17 = sel vmsk0 vr11, r48;"
        "}"
        "{"
        "V0@(pr0)	vr16 = sel vmsk1 vr11, r48;"
        "V1@(pr0)	vr17 = add.s32 vr16, vr17;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vmsk0 = eq.s32 vr17, r32;"
        "V1@(pr0)	vr17 = sel vmsk0 vr11, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V1@(pr0)	vmsk0 = gt.s32 vr28, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 65535;"
        "V0@(pr0)	vr31 = mov.u32 r45;"
        "V1@(pr0)	vr30 = mov.u32 r32;"
        "}"
        "{"
        "V0@(pr0)	vr30 = sub.s32 vr30, vr28;"
        "V1@(pr0)	vr31 = shr.u32 vr31, vr30;"
        "}"
        "{"
        "V0@(pr0)	vr31 = and.u32 vr31, vr5;"
        "V1@(pr0)	vr31 = shl.u32 vr31, vr30;"
        "}"
        "{"
        "V1@(pr0)	vr10 = shr.u32 vr4, vr28;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr10, vr31;"
        "V1@(pr0)	vr11 = shr.u32 vr5, vr28;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr31 = sub.s32 vr28, r32;"
        "V1@(pr0)	vr12 = shr.u32 vr5, vr31;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk0 vr10, vr12;"
        "V1@(pr0)	vr11 = sel vmsk0 vr11, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 64;"
        "V0@(pr0)	vr10 = or.u32 vr10, vr17;"
        "V1@(pr0)	vmsk0 = ls.s32 vr28, r32;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 r46;"
        "V1@(pr0)	vr11 = sel vmsk0 vr17, vr11;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 r48;"
        "V1@(pr0)	vr10 = sel vmsk0 vr17, vr10;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = eq.s32 vr28, r36;"
        "V1@(pr0)	vr4 = sel vmsk0 vr10, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "V0@(pr0)	vr5 = sel vmsk0 vr11, vr5;"
        "V1@(pr0)	vr10 = and.u32 vr2, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr12 = and.u32 vr4, r33;"
        "V1@(pr0)	vr11 = shr.u32 vr2, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr13 = shr.u32 vr4, r32;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = gteq.s32 vr10, vr12;"
        "V1@(pr0)	vr28 = mov.u32 r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr17 = sel vmsk0 vr28, r46;"
        "V1@(pr0)	vr17 = shl.u32 vr17, r32;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr10, vr17;"
        "V1@(pr0)	vr14 = sub.s32 vr10, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr15 = add.s32 vr13, r48;"
        "V1@(pr0)	vr13 = sel vmsk0 vr15, vr13;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = gteq.s32 vr11, vr13;"
        "V1@(pr0)	vr17 = sel vmsk0 vr28, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr17 = shl.u32 vr17, r32;"
        "}"
        "{"
        "V0@(pr0)	vr11 = or.u32 vr11, vr17;"
        "V1@(pr0)	vr15 = sub.s32 vr11, vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr15 = shl.u32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr14, vr15;"
        "}"
        "{"
        "V0@(pr0)	vr17 = add.s32 vr5, r48;"
        "V1@(pr0)	vr17 = sel vmsk0 vr17, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr11 = sub.s32 vr3, vr17;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = eq.s32 vr11, r46;"
        "V1@(pr0)	vr31 = count.u32 vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr30 = count.u32 vr10;"
        "V1@(pr0)	vr30 = add.s32 vr30, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr31 = sel vmsk0 vr31, vr30;"
        "V1@(pr0)	vr31 = sub.s32 vr31, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vmsk6 = ls.s32 vr31, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr30 = mov.u32 r32;"
        "V1@(pr0)	vr30 = sub.s32 vr30, vr31;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr29 = mov.u32 r44;"
        "V1@(pr0)	vr29 = shl.u32 vr29, vr30;"
        "}"
        "{"
        "V0@(pr0)	vr29 = and.u32 vr29, vr10;"
        "V1@(pr0)	vr29 = shr.u32 vr29, vr30;"
        "}"
        "{"
        "V1@(pr0)	vr13 = shl.u32 vr11, vr31;"
        "}"
        "{"
        "V0@(pr0)	vr13 = or.u32 vr13, vr29;"
        "V1@(pr0)	vr12 = shl.u32 vr10, vr31;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32;"
        "V0@(pr0)	vr30 = mov.u32 r32;"
        "V1@(pr0)	vr30 = sub.s32 vr31, vr30;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 r46;"
        "V1@(pr0)	vr14 = shl.u32 vr10, vr30;"
        "}"
        "{"
        "V0@(pr0)	vr12 = sel vmsk6 vr15, vr12;"
        "V1@(pr0)	vr13 = sel vmsk6 vr14, vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 128;"
        "V0@(pr0)	vmsk0 = ls.s32 vr11, r36;"
        "V1@(pr0)	vr10 = sel vmsk0 vr10, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr15 = sub.s32 vr0, vr31;"
        "}"
        "{"
        "V0@(pr0)	vr11 = sel vmsk0 vr11, vr13;"
        "V1@(pr0)	vr15 = sel vmsk0 vr0, vr15;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr12 = and.u32 vr2, r33;"
        "V1@(pr0)	vr13 = shr.u32 vr2, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr16 = and.u32 vr4, r33;"
        "V1@(pr0)	vr17 = shr.u32 vr4, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vr28 = add.s32 vr12, vr16;"
        "V1@(pr0)	vr14 = shr.u32 vr28, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "V0@(pr0)	vr28 = and.u32 vr28, r32;"
        "}"
        "{"
        "V0@(pr0)	vr29 = add.s32 vr13, vr17;"
        "V1@(pr0)	vr29 = add.s32 vr29, vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V1@(pr0)	vr14 = shr.u32 vr29, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 16;"
        "V0@(pr0)	vr29 = and.u32 vr29, r32;"
        "V1@(pr0)	vr29 = shl.u32 vr29, r33;"
        "}"
        "{"
        "V0@(pr0)	vr12 = or.u32 vr29, vr28;"
        "V1@(pr0)	vr13 = add.s32 vr3, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr13 = add.s32 vr13, vr14;"
        "V1@(pr0)	vr28 = and.u32 vr12, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31;"
        "V0@(pr0)	vr14 = and.u32 vr13, r48;"
        "V1@(pr0)	vr14 = shl.u32 vr14, r32;"
        "}"
        "{"
        "V1@(pr0)	vr16 = shr.u32 vr12, r48;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr16, vr14;"
        "V1@(pr0)	vr17 = shr.u32 vr13, r48;"
        "}"
        "{"
        "V0@(pr0)	vr16 = or.u32 vr16, vr28;"
        "V1@(pr0)	vr14 = add.s32 vr0, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 256;"
        "V0@(pr0)	vr28 = and.u32 vr13, r36;"
        "V1@(pr0)	vmsk0 = eq.s32 vr28, r46;"
        "}"
        "{"
        "V0@(pr0)	vr12 = sel vmsk0 vr16, vr12;"
        "V1@(pr0)	vr13 = sel vmsk0 vr17, vr13;"
        "}"
        "{"
        "V0@(pr0)	vr14 = sel vmsk0 vr14, vr0;"
        "V1@(pr0)	vmsk1 = eq.s32 vr7, r46;"
        "}"
        "{"
        "V0@(pr0)	vr2 = sel vmsk1 vr10, vr12;"
        "V1@(pr0)	vr3 = sel vmsk1 vr11, vr13;"
        "}"
        "{"
        "V0@(pr0)	vr0 = sel vmsk1 vr15, vr14;"
        "V1@(pr0)	vr31 = mov.u32 r48;"
        "}"
        "{"
        "V0@(pr0)	vr10 = or.u32 vr2, r48;"
        "V1@(pr0)	vmsk0 = lseq.s32 vr0, r46;"
        "}"
        "{"
        "V0@(pr0)	vr2 = sel vmsk0 vr2, vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 7;"
        "pseudo@0	@pseudo imm_1 = 3;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 65535;"
        "V0@(pr0)	vr17 = and.u32 vr2, r32;"
        "V1@(pr0)	vr31 = mov.u32 r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 29;"
        "V1@(pr0)	vr31 = shr.u32 vr31, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 29;"
        "V0@(pr0)	vr31 = and.u32 vr3, vr31;"
        "V1@(pr0)	vr31 = shl.u32 vr31, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3;"
        "V1@(pr0)	vr10 = shr.u32 vr2, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3;"
        "V0@(pr0)	vr10 = or.u32 vr10, vr31;"
        "V1@(pr0)	vr11 = shr.u32 vr3, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 15;"
        "V0@(pr0)	vr10 = and.u32 vr10, r44;"
        "V1@(pr0)	vr11 = and.u32 vr11, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20;"
        "V1@(pr0)	vr28 = shl.u32 vr0, r32;"
        "}"
        "{"
        "V0@(pr0)	vr11 = or.u32 vr11, vr28;"
        "V1@(pr0)	vr11 = or.u32 vr11, vr6;"
        "}"
        "{"
        "V0@(pr0)	vr12 = add.s32 vr10, r48;"
        "V1@(pr0)	vr13 = and.u32 vr10, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4;"
        "V0@(pr0)	vmsk0 = gt.s32 vr17, r32;"
        "V1@(pr0)	vmsk1 = eq.s32 vr17, r32;"
        "}"
        "{"
        "V0@(pr0)	vr13 = add.s32 vr10, vr13;"
        "V1@(pr0)	vr13 = sel vmsk1 vr10, vr13;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk0 vr13, vr12;"
        "V1@(pr0)	vr0 = mov.u32 vr10;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mov.u32 vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr10 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr11 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr12 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr13 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr14 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 5120;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr15 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 6144;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr16 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 7168;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr17 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8192;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = eq.s32 vr17, r46;"
        "V1@(pr0)	vmsk1 = eq.s32 vr16, r46;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mov.u32 r46;"
        "V1@(pr0)	vr7 = sel vmsk0 vr28, r48;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr28, r48;"
        "V1@(pr0)	vr7 = add.s32 vr6, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vmsk0 = eq.s32 vr7, r32;"
        "V1@(pr0)	vr30 = sel vmsk0 vr0, vr10;"
        "}"
        "{"
        "V0@(pr0)	vr31 = sel vmsk0 vr1, vr11;"
        "V1@(pr0)	vr6 = and.u32 vr10, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr7 = and.u32 vr11, vr13;"
        "V1@(pr0)	vr5 = sel vmsk0 vr13, vr7;"
        "}"
        "{"
        "V0@(pr0)	vr4 = sel vmsk0 vr12, vr6;"
        "V1@(pr0)	vmsk0 = eq.s32 vr14, r46;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = eq.s32 vr15, r46;"
        "V1@(pr0)	vr6 = sel vmsk0 vr28, r48;"
        "}"
        "{"
        "V0@(pr0)	vr7 = sel vmsk1 vr28, r48;"
        "V1@(pr0)	vr7 = add.s32 vr6, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vmsk0 = eq.s32 vr7, r32;"
        "V1@(pr0)	vr31 = sel vmsk0 vr31, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr30 = sel vmsk0 vr30, r34;"
        "V1@(pr0)	vr7 = mov.u32 r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 15;"
        "V0@(pr0)	vr6 = mov.u32 r44;"
        "V1@(pr0)	vr7 = xor.u32 vr7, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 65535;"
        "V0@(pr0)	vr6 = xor.u32 vr6, r44;"
        "V1@(pr0)	vmsk0 = eq.s32 vr16, vr6;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mov.u32 vr7;"
        "V1@(pr0)	vr4 = mov.u32 vr6;"
        "}"
        "{"
        "V0@(pr0)	vmsk6 = eq.s32 vr14, vr6;"
        "V1@(pr0)	vmsk7 = eq.s32 vr15, vr7;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = eq.s32 vr17, vr7;"
        "V1@(pr0)	vr7 = sel vmsk0 vr28, r48;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr28, r48;"
        "V1@(pr0)	vr7 = add.s32 vr6, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vmsk0 = eq.s32 vr7, r32;"
        "V1@(pr0)	vr31 = sel vmsk0 vr31, vr13;"
        "}"
        "{"
        "V0@(pr0)	vr30 = sel vmsk0 vr30, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr7 = sel vmsk7 vr28, r48;"
        "V1@(pr0)	vr6 = sel vmsk6 vr28, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vr7 = add.s32 vr6, vr7;"
        "V1@(pr0)	vmsk7 = eq.s32 vr7, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr3 = or.u32 vr5, r36;"
        "V1@(pr0)	vr6 = xor.u32 vr10, vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32768;"
        "V0@(pr0)	vr7 = xor.u32 vr11, vr13;"
        "V1@(pr0)	vmsk0 = eq.s32 vr7, r36;"
        "}"
        "{"
        "V0@(pr0)	vmsk1 = eq.s32 vr6, r46;"
        "V1@(pr0)	vr7 = sel vmsk0 vr28, r48;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr28, r48;"
        "V1@(pr0)	vr7 = add.s32 vr6, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vmsk0 = eq.s32 vr7, r32;"
        "V1@(pr0)	vr7 = sel vmsk0 vr11, vr3;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk0 vr10, vr4;"
        "V1@(pr0)	vr30 = sel vmsk7 vr30, vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr31 = sel vmsk7 vr31, vr7;"
        "V1@(pr0)	vr7 = or.u32 vr13, r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk0 = eq.s32 vr16, r46;"
        "V1@(pr0)	vr6 = shr.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "V0@(pr0)	vr28 = and.u32 vr17, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32752;"
        "V0@(pr0)	vmsk1 = gt.s32 vr6, r32;"
        "V1@(pr0)	vmsk2 = eq.s32 vr28, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk0 vr28, r48;"
        "V1@(pr0)	vr4 = sel vmsk2 vr28, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vr5 = add.s32 vr4, vr5;"
        "V1@(pr0)	vmsk2 = eq.s32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr7, vr31;"
        "V1@(pr0)	vr4 = sel vmsk2 vr12, vr30;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32752;"
        "V0@(pr0)	vmsk2 = eq.s32 vr6, r32;"
        "V1@(pr0)	vr4 = sel vmsk2 vr30, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr31, vr5;"
        "V1@(pr0)	vr31 = sel vmsk1 vr5, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 8;"
        "V0@(pr0)	vr30 = sel vmsk1 vr4, vr12;"
        "V1@(pr0)	vr7 = or.u32 vr11, r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16;"
        "V0@(pr0)	vmsk0 = eq.s32 vr14, r46;"
        "V1@(pr0)	vr6 = shr.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "V0@(pr0)	vr28 = and.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32752;"
        "V0@(pr0)	vmsk1 = gt.s32 vr6, r32;"
        "V1@(pr0)	vmsk2 = eq.s32 vr28, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk0 vr28, r48;"
        "V1@(pr0)	vr4 = sel vmsk2 vr28, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V0@(pr0)	vr5 = add.s32 vr4, vr5;"
        "V1@(pr0)	vmsk2 = eq.s32 vr5, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr7, vr31;"
        "V1@(pr0)	vr4 = sel vmsk2 vr10, vr30;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32752;"
        "V0@(pr0)	vmsk2 = eq.s32 vr6, r32;"
        "V1@(pr0)	vr4 = sel vmsk2 vr30, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk2 vr31, vr5;"
        "V1@(pr0)	vr11 = sel vmsk1 vr5, vr7;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk1 vr4, vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 49152;"
        "pseudo@0	@pseudo imm_2 = 3;"
        "V0@(pr0)	vr5 = and.u32 vr11, r44;"
        "V1@(pr0)	vr1 = shl.u32 vr11, r34;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 16383;"
        "V0@(pr0)	vr1 = and.u32 vr1, r44;"
        "V1@(pr0)	vr5 = or.u32 vr1, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 29;"
        "V1@(pr0)	vr1 = shr.u32 vr10, r32;"
        "}"
        "{"
        "V0@(pr0)	vr5 = or.u32 vr1, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr4 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr13 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr4;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr5;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mov.u32 vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r0 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "pseudo@0	@pseudo imm_2 = 1;"
        "pseudo@0	@pseudo imm_4 = 0;"
        "pseudo@0	@pseudo vs_imm0 = 1;"
        "S0@(pr0)	r1 = add.s32 r0, r32;"
        "S1@(pr0)	r1 = shr.u32 r1, r33;"
        "VL@(pr0)	vr10 = ld [vmem:r1+r64,r34,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r0 = add.s32 r0, r44;"
        "S1@(pr0)	[smem:r3] = st r0;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3;"
        "V0@(pr0)	vr15 = and.u32 vr15, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr0 = and.u32 vr11, r44;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr11, vr11;"
        "V1@(pr0)	vr7 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr4 = mul.f32 vr3, vr11;"
        "V1@(pr0)	vmsk1 = eq.s32 vr13, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 51667;"
        "pseudo@0	@pseudo imm_1 = 12078;"
        "pseudo@0	@pseudo imm_2 = 12084;"
        "pseudo@0	@pseudo imm_3 = 45783;"
        "V0@(pr0)	vr1 = mul.f32 vr3, r44;"
        "V1@(pr0)	vr1 = add.f32 vr1, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 61211;"
        "pseudo@0	@pseudo imm_1 = 13880;"
        "V0@(pr0)	vr1 = mul.f32 vr3, vr1;"
        "V1@(pr0)	vr1 = add.f32 vr1, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3329;"
        "pseudo@0	@pseudo imm_1 = 47440;"
        "V0@(pr0)	vr1 = mul.f32 vr3, vr1;"
        "V1@(pr0)	vr1 = add.f32 vr1, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 34953;"
        "pseudo@0	@pseudo imm_1 = 15368;"
        "V0@(pr0)	vr1 = mul.f32 vr3, vr1;"
        "V1@(pr0)	vr5 = add.f32 vr1, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 43691;"
        "pseudo@0	@pseudo imm_1 = 48682;"
        "V0@(pr0)	vr1 = mul.f32 vr3, vr5;"
        "V1@(pr0)	vr1 = add.f32 vr1, r44;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr4, vr1;"
        "V1@(pr0)	vr1 = add.f32 vr11, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr2 = mul.f32 vr12, r50;"
        "V1@(pr0)	vr6 = sel vmsk1 vr6, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr1 = sub.f32 vr2, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr3, vr1;"
        "V1@(pr0)	vr1 = sub.f32 vr1, vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 43691;"
        "pseudo@0	@pseudo imm_1 = 48682;"
        "V0@(pr0)	vr2 = mul.f32 vr4, r44;"
        "V1@(pr0)	vr1 = sub.f32 vr1, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr2 = cvtftoint.s32 vr11, r44;"
        "V1@(pr0)	vr1 = sub.f32 vr11, vr1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 12800;"
        "V0@(pr0)	vr6 = sel vmsk1 vr1, vr6;"
        "V1@(pr0)	vmsk1 = ls.s32 vr0, r36;"
        "}"
        "{"
        "V0@(pr0)	vr1 = sel vmsk1 vr6, vr11;"
        "V1@(pr0)	vmsk1 = eq.s32 vr2, r46;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr6, vr1;"
        "V1@(pr0)	vr11 = mov.u32 vr6;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mul.f32 vr11, r57;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V1@(pr0)	vmsk1 = eq.s32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr14 = sel vmsk1 vr11, vr17;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr4 = mul.f32 vr11, vr11;"
        "V1@(pr0)	vr7 = and.u32 vr11, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 39322;"
        "pseudo@0	@pseudo imm_1 = 16025;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vmsk1 = ls.s32 vr7, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 55118;"
        "pseudo@0	@pseudo imm_1 = 44359;"
        "pseudo@0	@pseudo imm_2 = 29942;"
        "pseudo@0	@pseudo imm_3 = 12559;"
        "V0@(pr0)	vr5 = mul.f32 vr4, r44;"
        "V1@(pr0)	vr5 = add.f32 vr5, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 62076;"
        "pseudo@0	@pseudo imm_1 = 46227;"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr5 = add.f32 vr5, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3329;"
        "pseudo@0	@pseudo imm_1 = 14288;"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr5 = add.f32 vr5, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2913;"
        "pseudo@0	@pseudo imm_1 = 47798;"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr5 = add.f32 vr5, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 43691;"
        "pseudo@0	@pseudo imm_1 = 15658;"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr5 = add.f32 vr5, r44;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr11, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr2 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr1 = sub.f32 vr2, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr2 = mul.f32 vr4, r50;"
        "V1@(pr0)	vr1 = sub.f32 vr2, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr2 = mov.u32 r49;"
        "V1@(pr0)	vr1 = sub.f32 vr2, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr0, vr1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16200;"
        "pseudo@0	@pseudo imm_1 = 256;"
        "V0@(pr0)	vmsk2 = gt.s32 vr7, r36;"
        "V1@(pr0)	vr1 = sub.s32 vr7, r37;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16016;"
        "V0@(pr0)	vr2 = mul.f32 vr4, r50;"
        "V1@(pr0)	vr1 = sel vmsk2 vr1, r44;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mov.u32 r49;"
        "V1@(pr0)	vr2 = sub.f32 vr2, vr1;"
        "}"
        "{"
        "V1@(pr0)	vr1 = sub.f32 vr3, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr11, vr12;"
        "V1@(pr0)	vr3 = add.f32 vr3, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr2 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr3 = sub.f32 vr3, vr2;"
        "}"
        "{"
        "V1@(pr0)	vr1 = sub.f32 vr1, vr3;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 12800;"
        "V0@(pr0)	vr6 = sel vmsk1 vr1, vr6;"
        "V1@(pr0)	vmsk1 = ls.s32 vr7, r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr2 = cvtftoint.s32 vr11, r44;"
        "}"
        "{"
        "V0@(pr0)	vr1 = sel vmsk1 vr6, r49;"
        "V1@(pr0)	vmsk1 = eq.s32 vr2, r46;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr6, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 vr6;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mul.f32 vr11, r57;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "V1@(pr0)	vmsk1 = eq.s32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr14 = sel vmsk1 vr14, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "V1@(pr0)	vmsk1 = eq.s32 vr15, r32;"
        "}"
        "{"
        "V0@(pr0)	vr14 = sel vmsk1 vr14, vr17;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 vr10;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 r46;"
        "V1@(pr0)	vr13 = mov.u32 r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr4 = mul.f32 vr11, vr11;"
        "V1@(pr0)	vr7 = and.u32 vr11, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 39322;"
        "pseudo@0	@pseudo imm_1 = 16025;"
        "V0@(pr0)	vr0 = mov.u32 r46;"
        "V1@(pr0)	vmsk1 = ls.s32 vr7, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 55118;"
        "pseudo@0	@pseudo imm_1 = 44359;"
        "pseudo@0	@pseudo imm_2 = 29942;"
        "pseudo@0	@pseudo imm_3 = 12559;"
        "V0@(pr0)	vr5 = mul.f32 vr4, r44;"
        "V1@(pr0)	vr5 = add.f32 vr5, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 62076;"
        "pseudo@0	@pseudo imm_1 = 46227;"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr5 = add.f32 vr5, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3329;"
        "pseudo@0	@pseudo imm_1 = 14288;"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr5 = add.f32 vr5, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2913;"
        "pseudo@0	@pseudo imm_1 = 47798;"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr5 = add.f32 vr5, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 43691;"
        "pseudo@0	@pseudo imm_1 = 15658;"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr5 = add.f32 vr5, r44;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr11, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr2 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr1 = sub.f32 vr2, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr2 = mul.f32 vr4, r50;"
        "V1@(pr0)	vr1 = sub.f32 vr2, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr2 = mov.u32 r49;"
        "V1@(pr0)	vr1 = sub.f32 vr2, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr0, vr1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16200;"
        "pseudo@0	@pseudo imm_1 = 256;"
        "V0@(pr0)	vmsk2 = gt.s32 vr7, r36;"
        "V1@(pr0)	vr1 = sub.s32 vr7, r37;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 16016;"
        "V0@(pr0)	vr2 = mul.f32 vr4, r50;"
        "V1@(pr0)	vr1 = sel vmsk2 vr1, r44;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mov.u32 r49;"
        "V1@(pr0)	vr2 = sub.f32 vr2, vr1;"
        "}"
        "{"
        "V1@(pr0)	vr1 = sub.f32 vr3, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr11, vr12;"
        "V1@(pr0)	vr3 = add.f32 vr3, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr2 = mul.f32 vr4, vr5;"
        "V1@(pr0)	vr3 = sub.f32 vr3, vr2;"
        "}"
        "{"
        "V1@(pr0)	vr1 = sub.f32 vr1, vr3;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 12800;"
        "V0@(pr0)	vr6 = sel vmsk1 vr1, vr6;"
        "V1@(pr0)	vmsk1 = ls.s32 vr7, r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr2 = cvtftoint.s32 vr11, r44;"
        "}"
        "{"
        "V0@(pr0)	vr1 = sel vmsk1 vr6, r49;"
        "V1@(pr0)	vmsk1 = eq.s32 vr2, r46;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr6, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mov.u32 vr6;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr1 = and.u32 vr10, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4056;"
        "pseudo@0	@pseudo imm_1 = 16201;"
        "V0@(pr0)	vmsk1 = ls.f32 vr1, r44;"
        "}"
        "{"
        "V1@(pr0)	vr14 = sel vmsk1 vr14, vr11;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr1 = and.u32 vr10, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32640;"
        "V0@(pr0)	vmsk1 = ls.f32 vr1, r44;"
        "V1@(pr0)	vr15 = sub.f32 vr10, vr10;"
        "}"
        "{"
        "V1@(pr0)	vr14 = sel vmsk1 vr15, vr14;"
        "}"
        "{"
        "V0@(pr0)	vr10 = mov.u32 vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "V0@(pr0)	vr15 = mov.u32 vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r1 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VL@(pr0)	vr10 = ld [vmem:r0,$1,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VL@(pr0)	vr28 = ld [vmem:r0,$1,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VL@(pr0)	vr29 = ld [vmem:r0,$1,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r1 = add.s32 r1, r44;"
        "S1@(pr0)	[smem:r3] = st r1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 65535;"
        "pseudo@0	@pseudo imm_3 = 32767;"
        "V1@(pr0)	vr14 = and.u32 vr10, r45;"
        "}"
        "{"
        "V0@(pr0)	vmsk2 = lseq.f32 vr14, r36;"
        "V1@(pr0)	vr31 = sub.f32 vr28, vr29;"
        "}"
        "{"
        "V0@(pr0)	vr30 = mul.f32 vr28, r57;"
        "V1@(pr0)	vr30 = sub.f32 vr30, vr29;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mul.f32 vr28, vr29;"
        "V1@(pr0)	vmsk3 = gt.f32 vr28, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32640;"
        "V0@(pr0)	vr0 = and.u32 vr30, r44;"
        "V1@(pr0)	vmsk1 = eq.s32 vr0, r45;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr0, r50;"
        "V1@(pr0)	vr2 = mov.u32 vr0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 23007;"
        "pseudo@0	@pseudo imm_1 = 24375;"
        "V0@(pr0)	vr3 = mov.u32 r44;"
        "V1@(pr0)	vr2 = shr.u32 vr2, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16320;"
        "V0@(pr0)	vr2 = sub.s32 vr3, vr2;"
        "V1@(pr0)	vr4 = mov.u32 r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32768;"
        "V0@(pr0)	vr3 = mul.f32 vr2, vr2;"
        "V1@(pr0)	vr6 = and.u32 vr30, r36;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr1, vr3;"
        "V1@(pr0)	vr5 = sub.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr5, vr2;"
        "V1@(pr0)	vr2 = mov.u32 vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr2, vr2;"
        "V1@(pr0)	vmsk0 = eq.f32 vr30, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr1, vr3;"
        "V1@(pr0)	vr5 = sub.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr5, vr2;"
        "V1@(pr0)	vr2 = mov.u32 vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32640;"
        "V0@(pr0)	vr3 = mul.f32 vr2, vr2;"
        "V1@(pr0)	vr7 = or.u32 vr6, r44;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr1, vr3;"
        "V1@(pr0)	vr5 = sub.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr5, vr2;"
        "V1@(pr0)	vr2 = or.u32 vr6, r46;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr5, vr5;"
        "V1@(pr0)	vr1 = or.u32 vr6, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr28 = sel vmsk0 vr1, vr7;"
        "V1@(pr0)	vr28 = sel vmsk1 vr28, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mul.f32 vr28, vr15;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32640;"
        "V0@(pr0)	vr0 = and.u32 vr31, r44;"
        "V1@(pr0)	vmsk1 = eq.s32 vr0, r45;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr0, r50;"
        "V1@(pr0)	vr2 = mov.u32 vr0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 23007;"
        "pseudo@0	@pseudo imm_1 = 24375;"
        "V0@(pr0)	vr3 = mov.u32 r44;"
        "V1@(pr0)	vr2 = shr.u32 vr2, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16320;"
        "V0@(pr0)	vr2 = sub.s32 vr3, vr2;"
        "V1@(pr0)	vr4 = mov.u32 r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32768;"
        "V0@(pr0)	vr3 = mul.f32 vr2, vr2;"
        "V1@(pr0)	vr6 = and.u32 vr31, r36;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr1, vr3;"
        "V1@(pr0)	vr5 = sub.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr5, vr2;"
        "V1@(pr0)	vr2 = mov.u32 vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr2, vr2;"
        "V1@(pr0)	vmsk0 = eq.f32 vr31, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr1, vr3;"
        "V1@(pr0)	vr5 = sub.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr5, vr2;"
        "V1@(pr0)	vr2 = mov.u32 vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32640;"
        "V0@(pr0)	vr3 = mul.f32 vr2, vr2;"
        "V1@(pr0)	vr7 = or.u32 vr6, r44;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr1, vr3;"
        "V1@(pr0)	vr5 = sub.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr5, vr2;"
        "V1@(pr0)	vr2 = or.u32 vr6, r46;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr5, vr5;"
        "V1@(pr0)	vr1 = or.u32 vr6, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr29 = sel vmsk0 vr1, vr7;"
        "V1@(pr0)	vr29 = sel vmsk1 vr29, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr29 = mul.f32 vr29, vr15;"
        "}"
        "{"
        "V0@(pr0)	vr17 = sel vmsk2 vr31, vr28;"
        "V1@(pr0)	vr17 = sel vmsk3 vr31, vr17;"
        "}"
        "{"
        "V0@(pr0)	vr31 = sel vmsk2 vr30, vr29;"
        "V1@(pr0)	vr31 = sel vmsk3 vr31, vr30;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr14, r50;"
        "V1@(pr0)	vr2 = mov.u32 vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 23007;"
        "pseudo@0	@pseudo imm_1 = 24375;"
        "V0@(pr0)	vr3 = mov.u32 r44;"
        "V1@(pr0)	vr2 = shr.u32 vr2, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16320;"
        "V0@(pr0)	vr2 = sub.s32 vr3, vr2;"
        "V1@(pr0)	vr4 = mov.u32 r36;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr2, vr2;"
        "V1@(pr0)	vmsk0 = lseq.f32 vr14, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr1, vr3;"
        "V1@(pr0)	vr5 = sub.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr5, vr2;"
        "V1@(pr0)	vr2 = mov.u32 vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32640;"
        "V0@(pr0)	vr3 = mul.f32 vr2, vr2;"
        "V1@(pr0)	vmsk1 = eq.s32 vr14, r45;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr1, vr3;"
        "V1@(pr0)	vr5 = sub.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr5, vr2;"
        "V1@(pr0)	vr2 = mov.u32 vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr2, vr2;"
        "V1@(pr0)	vmsk2 = eq.f32 vr14, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr1, vr3;"
        "V1@(pr0)	vr5 = sub.f32 vr4, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32768;"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32640;"
        "V0@(pr0)	vr1 = and.u32 vr14, r44;"
        "V1@(pr0)	vr3 = or.u32 vr1, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32704;"
        "V0@(pr0)	vr12 = mul.f32 vr5, vr2;"
        "V1@(pr0)	vr12 = sel vmsk0 vr12, r44;"
        "}"
        "{"
        "V0@(pr0)	vr12 = sel vmsk1 vr12, r46;"
        "V1@(pr0)	vr12 = sel vmsk2 vr12, vr3;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 28347;"
        "pseudo@0	@pseudo imm_3 = 16144;"
        "V0@(pr0)	vr11 = mul.f32 vr17, r45;"
        "}"
        "{"
        "V0@(pr0)	vr11 = mul.f32 vr11, vr12;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "pseudo@0	@pseudo imm_2 = 56168;"
        "pseudo@0	@pseudo imm_3 = 16438;"
        "V0@(pr0)	vr30 = and.u32 vr14, r44;"
        "V1@(pr0)	vmsk0 = gteq.f32 vr28, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20136;"
        "pseudo@0	@pseudo imm_1 = 13287;"
        "pseudo@0	@pseudo imm_2 = 57613;"
        "pseudo@0	@pseudo imm_3 = 12623;"
        "V0@(pr0)	vr0 = mov.u32 r44;"
        "V1@(pr0)	vr0 = sel vmsk0 vr0, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 64022;"
        "pseudo@0	@pseudo imm_1 = 15855;"
        "pseudo@0	@pseudo imm_2 = 65451;"
        "pseudo@0	@pseudo imm_3 = 15855;"
        "V0@(pr0)	vr1 = mov.u32 r44;"
        "V1@(pr0)	vr1 = sel vmsk0 vr1, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 38336;"
        "pseudo@0	@pseudo imm_1 = 16407;"
        "pseudo@0	@pseudo imm_2 = 46567;"
        "pseudo@0	@pseudo imm_3 = 16507;"
        "V0@(pr0)	vr2 = mov.u32 r44;"
        "V1@(pr0)	vr2 = sel vmsk0 vr2, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 57788;"
        "pseudo@0	@pseudo imm_1 = 16707;"
        "pseudo@0	@pseudo imm_2 = 31301;"
        "pseudo@0	@pseudo imm_3 = 16908;"
        "V0@(pr0)	vr3 = mov.u32 r44;"
        "V1@(pr0)	vr3 = sel vmsk0 vr3, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 36161;"
        "pseudo@0	@pseudo imm_1 = 16781;"
        "pseudo@0	@pseudo imm_2 = 7210;"
        "pseudo@0	@pseudo imm_3 = 17078;"
        "V0@(pr0)	vr4 = mov.u32 r44;"
        "V1@(pr0)	vr4 = sel vmsk0 vr4, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 23117;"
        "pseudo@0	@pseudo imm_1 = 16546;"
        "pseudo@0	@pseudo imm_2 = 15484;"
        "pseudo@0	@pseudo imm_3 = 16962;"
        "V0@(pr0)	vr5 = mov.u32 r44;"
        "V1@(pr0)	vr5 = sel vmsk0 vr5, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 7256;"
        "pseudo@0	@pseudo imm_1 = 16631;"
        "pseudo@0	@pseudo imm_2 = 13119;"
        "pseudo@0	@pseudo imm_3 = 11624;"
        "V0@(pr0)	vmsk0 = gteq.f32 vr30, r44;"
        "V1@(pr0)	vr0 = sel vmsk0 vr0, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 15855;"
        "pseudo@0	@pseudo imm_2 = 45091;"
        "pseudo@0	@pseudo imm_3 = 16601;"
        "V0@(pr0)	vr1 = sel vmsk0 vr1, r44;"
        "V1@(pr0)	vr2 = sel vmsk0 vr2, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 40394;"
        "pseudo@0	@pseudo imm_1 = 17112;"
        "pseudo@0	@pseudo imm_2 = 26807;"
        "pseudo@0	@pseudo imm_3 = 17409;"
        "V0@(pr0)	vr3 = sel vmsk0 vr3, r44;"
        "V1@(pr0)	vr4 = sel vmsk0 vr4, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 11718;"
        "pseudo@0	@pseudo imm_1 = 17412;"
        "V0@(pr0)	vr5 = sel vmsk0 vr5, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16416;"
        "V0@(pr0)	vmsk0 = gteq.f32 vr30, r36;"
        "V1@(pr0)	vr0 = sel vmsk0 vr0, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 15856;"
        "pseudo@0	@pseudo imm_2 = 54506;"
        "pseudo@0	@pseudo imm_3 = 16723;"
        "V0@(pr0)	vr1 = sel vmsk0 vr1, r44;"
        "V1@(pr0)	vr2 = sel vmsk0 vr2, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1699;"
        "pseudo@0	@pseudo imm_1 = 17358;"
        "pseudo@0	@pseudo imm_2 = 11245;"
        "pseudo@0	@pseudo imm_3 = 17778;"
        "V0@(pr0)	vr3 = sel vmsk0 vr3, r44;"
        "V1@(pr0)	vr4 = sel vmsk0 vr4, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 21462;"
        "pseudo@0	@pseudo imm_1 = 17911;"
        "V0@(pr0)	vr7 = mul.f32 vr14, vr14;"
        "V1@(pr0)	vr5 = sel vmsk0 vr5, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r1 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 6144;"
        "S1@(pr0)	r1 = sub.s32 r1, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VS@(pr0)	 [vmem:r0,$1,$255] = st vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VS@(pr0)	 [vmem:r0,$1,$255] = st vr1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VS@(pr0)	 [vmem:r0,$1,$255] = st vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VS@(pr0)	 [vmem:r0,$1,$255] = st vr3;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VS@(pr0)	 [vmem:r0,$1,$255] = st vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 5120;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VS@(pr0)	 [vmem:r0,$1,$255] = st vr0;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32640;"
        "V0@(pr0)	vr0 = and.u32 vr7, r44;"
        "V1@(pr0)	vmsk1 = eq.s32 vr0, r45;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr0, r50;"
        "V1@(pr0)	vr2 = mov.u32 vr0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 23007;"
        "pseudo@0	@pseudo imm_1 = 24375;"
        "V0@(pr0)	vr3 = mov.u32 r44;"
        "V1@(pr0)	vr2 = shr.u32 vr2, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16320;"
        "V0@(pr0)	vr2 = sub.s32 vr3, vr2;"
        "V1@(pr0)	vr4 = mov.u32 r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32768;"
        "V0@(pr0)	vr3 = mul.f32 vr2, vr2;"
        "V1@(pr0)	vr6 = and.u32 vr7, r36;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr1, vr3;"
        "V1@(pr0)	vr5 = sub.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr5, vr2;"
        "V1@(pr0)	vr2 = mov.u32 vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr2, vr2;"
        "V1@(pr0)	vmsk0 = eq.f32 vr7, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr1, vr3;"
        "V1@(pr0)	vr5 = sub.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr5, vr2;"
        "V1@(pr0)	vr2 = mov.u32 vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32640;"
        "V0@(pr0)	vr3 = mul.f32 vr2, vr2;"
        "V1@(pr0)	vr7 = or.u32 vr6, r44;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr1, vr3;"
        "V1@(pr0)	vr5 = sub.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr5, vr2;"
        "V1@(pr0)	vr2 = or.u32 vr6, r46;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr5, vr5;"
        "V1@(pr0)	vr1 = or.u32 vr6, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr7 = sel vmsk0 vr1, vr7;"
        "V1@(pr0)	vr7 = sel vmsk1 vr7, vr2;"
        "}"
        "{"
        "S1@(pr0)	r1 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VL@(pr0)	vr5 = ld [vmem:r0,$1,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VL@(pr0)	vr1 = ld [vmem:r0,$1,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VL@(pr0)	vr2 = ld [vmem:r0,$1,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VL@(pr0)	vr3 = ld [vmem:r0,$1,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VL@(pr0)	vr4 = ld [vmem:r0,$1,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 5120;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VL@(pr0)	vr0 = ld [vmem:r0,$1,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 6144;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r1 = add.s32 r1, r44;"
        "S1@(pr0)	[smem:r3] = st r1;"
        "}"
        "{"
        "V0@(pr0)	vr6 = mul.f32 vr5, vr7;"
        "V1@(pr0)	vr6 = add.f32 vr6, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr6 = mul.f32 vr6, vr7;"
        "V1@(pr0)	vr6 = add.f32 vr6, vr3;"
        "}"
        "{"
        "V0@(pr0)	vr6 = mul.f32 vr6, vr7;"
        "V1@(pr0)	vr6 = add.f32 vr6, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr6 = mul.f32 vr6, vr7;"
        "V1@(pr0)	vr6 = add.f32 vr6, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr6 = mul.f32 vr6, vr7;"
        "V1@(pr0)	vr6 = add.f32 vr6, vr0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 56168;"
        "pseudo@0	@pseudo imm_3 = 16438;"
        "V0@(pr0)	vmsk0 = gteq.f32 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32236;"
        "pseudo@0	@pseudo imm_1 = 16811;"
        "pseudo@0	@pseudo imm_2 = 10829;"
        "pseudo@0	@pseudo imm_3 = 16907;"
        "V0@(pr0)	vr0 = mov.u32 r44;"
        "V1@(pr0)	vr0 = sel vmsk0 vr0, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 38041;"
        "pseudo@0	@pseudo imm_1 = 17146;"
        "pseudo@0	@pseudo imm_2 = 24984;"
        "pseudo@0	@pseudo imm_3 = 17320;"
        "V0@(pr0)	vr1 = mov.u32 r44;"
        "V1@(pr0)	vr1 = sel vmsk0 vr1, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 18119;"
        "pseudo@0	@pseudo imm_1 = 17256;"
        "pseudo@0	@pseudo imm_2 = 56291;"
        "pseudo@0	@pseudo imm_3 = 17538;"
        "V0@(pr0)	vr2 = mov.u32 r44;"
        "V1@(pr0)	vr2 = sel vmsk0 vr2, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 23511;"
        "pseudo@0	@pseudo imm_1 = 17131;"
        "pseudo@0	@pseudo imm_2 = 46061;"
        "pseudo@0	@pseudo imm_3 = 17502;"
        "V0@(pr0)	vr3 = mov.u32 r44;"
        "V1@(pr0)	vr3 = sel vmsk0 vr3, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 54672;"
        "pseudo@0	@pseudo imm_1 = 16645;"
        "pseudo@0	@pseudo imm_2 = 37740;"
        "pseudo@0	@pseudo imm_3 = 17103;"
        "V0@(pr0)	vr4 = mov.u32 r44;"
        "V1@(pr0)	vr4 = sel vmsk0 vr4, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 7256;"
        "pseudo@0	@pseudo imm_1 = 16631;"
        "pseudo@0	@pseudo imm_2 = 8021;"
        "pseudo@0	@pseudo imm_3 = 17005;"
        "V0@(pr0)	vmsk0 = gteq.f32 vr30, r44;"
        "V1@(pr0)	vr0 = sel vmsk0 vr0, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 55729;"
        "pseudo@0	@pseudo imm_1 = 17527;"
        "pseudo@0	@pseudo imm_2 = 18979;"
        "pseudo@0	@pseudo imm_3 = 17831;"
        "V0@(pr0)	vr1 = sel vmsk0 vr1, r44;"
        "V1@(pr0)	vr2 = sel vmsk0 vr2, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 9606;"
        "pseudo@0	@pseudo imm_1 = 17909;"
        "pseudo@0	@pseudo imm_2 = 384;"
        "pseudo@0	@pseudo imm_3 = 17596;"
        "V0@(pr0)	vr3 = sel vmsk0 vr3, r44;"
        "V1@(pr0)	vr4 = sel vmsk0 vr4, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16416;"
        "pseudo@0	@pseudo imm_2 = 27180;"
        "pseudo@0	@pseudo imm_3 = 17124;"
        "V0@(pr0)	vmsk0 = gteq.f32 vr30, r36;"
        "V1@(pr0)	vr0 = sel vmsk0 vr0, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 12005;"
        "pseudo@0	@pseudo imm_1 = 17764;"
        "pseudo@0	@pseudo imm_2 = 23605;"
        "pseudo@0	@pseudo imm_3 = 18192;"
        "V0@(pr0)	vr1 = sel vmsk0 vr1, r44;"
        "V1@(pr0)	vr2 = sel vmsk0 vr2, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 41318;"
        "pseudo@0	@pseudo imm_1 = 18366;"
        "pseudo@0	@pseudo imm_2 = 43147;"
        "pseudo@0	@pseudo imm_3 = 18160;"
        "V0@(pr0)	vr3 = sel vmsk0 vr3, r44;"
        "V1@(pr0)	vr4 = sel vmsk0 vr4, r45;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr7;"
        "V1@(pr0)	vr5 = add.f32 vr5, vr3;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr5, vr7;"
        "V1@(pr0)	vr5 = add.f32 vr5, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr5, vr7;"
        "V1@(pr0)	vr5 = add.f32 vr5, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr5, vr7;"
        "V1@(pr0)	vr5 = add.f32 vr5, vr0;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr5, vr7;"
        "V1@(pr0)	vr5 = add.f32 vr5, r49;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32640;"
        "V0@(pr0)	vr0 = and.u32 vr5, r44;"
        "V1@(pr0)	vmsk1 = eq.s32 vr0, r45;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr0, r50;"
        "V1@(pr0)	vr2 = mov.u32 vr0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 23007;"
        "pseudo@0	@pseudo imm_1 = 24375;"
        "V0@(pr0)	vr3 = mov.u32 r44;"
        "V1@(pr0)	vr2 = shr.u32 vr2, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16320;"
        "V0@(pr0)	vr2 = sub.s32 vr3, vr2;"
        "V1@(pr0)	vr4 = mov.u32 r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32768;"
        "V0@(pr0)	vr3 = mul.f32 vr2, vr2;"
        "V1@(pr0)	vr16 = and.u32 vr5, r36;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mul.f32 vr1, vr3;"
        "V1@(pr0)	vr15 = sub.f32 vr4, vr15;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mul.f32 vr15, vr2;"
        "V1@(pr0)	vr2 = mov.u32 vr15;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr2, vr2;"
        "V1@(pr0)	vmsk0 = eq.f32 vr5, r46;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mul.f32 vr1, vr3;"
        "V1@(pr0)	vr15 = sub.f32 vr4, vr15;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mul.f32 vr15, vr2;"
        "V1@(pr0)	vr2 = mov.u32 vr15;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr3 = mul.f32 vr2, vr2;"
        "V1@(pr0)	vr7 = or.u32 vr16, r44;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mul.f32 vr1, vr3;"
        "V1@(pr0)	vr15 = sub.f32 vr4, vr15;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mul.f32 vr15, vr2;"
        "V1@(pr0)	vr2 = or.u32 vr16, r46;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr15, vr15;"
        "V1@(pr0)	vr1 = or.u32 vr16, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk0 vr1, vr7;"
        "V1@(pr0)	vr5 = sel vmsk1 vr5, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr5, vr6;"
        "V1@(pr0)	vr28 = add.f32 vr5, r49;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "pseudo@0	@pseudo imm_2 = 56168;"
        "pseudo@0	@pseudo imm_3 = 16438;"
        "V0@(pr0)	vr30 = and.u32 vr14, r44;"
        "V1@(pr0)	vmsk0 = gteq.f32 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 35122;"
        "pseudo@0	@pseudo imm_1 = 46143;"
        "pseudo@0	@pseudo imm_2 = 32079;"
        "pseudo@0	@pseudo imm_3 = 45486;"
        "V0@(pr0)	vr0 = mov.u32 r44;"
        "V1@(pr0)	vr0 = sel vmsk0 vr0, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 62581;"
        "pseudo@0	@pseudo imm_1 = 48593;"
        "pseudo@0	@pseudo imm_2 = 65371;"
        "pseudo@0	@pseudo imm_3 = 48593;"
        "V0@(pr0)	vr1 = mov.u32 r44;"
        "V1@(pr0)	vr1 = sel vmsk0 vr1, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 9251;"
        "pseudo@0	@pseudo imm_1 = 49200;"
        "pseudo@0	@pseudo imm_2 = 34322;"
        "pseudo@0	@pseudo imm_3 = 49299;"
        "V0@(pr0)	vr2 = mov.u32 r44;"
        "V1@(pr0)	vr2 = sel vmsk0 vr2, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 20246;"
        "pseudo@0	@pseudo imm_1 = 49565;"
        "pseudo@0	@pseudo imm_2 = 25486;"
        "pseudo@0	@pseudo imm_3 = 49767;"
        "V0@(pr0)	vr3 = mov.u32 r44;"
        "V1@(pr0)	vr3 = sel vmsk0 vr3, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 19743;"
        "pseudo@0	@pseudo imm_1 = 49705;"
        "pseudo@0	@pseudo imm_2 = 16026;"
        "pseudo@0	@pseudo imm_3 = 50020;"
        "V0@(pr0)	vr4 = mov.u32 r44;"
        "V1@(pr0)	vr4 = sel vmsk0 vr4, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 63922;"
        "pseudo@0	@pseudo imm_1 = 49578;"
        "pseudo@0	@pseudo imm_2 = 13771;"
        "pseudo@0	@pseudo imm_3 = 50011;"
        "V0@(pr0)	vr5 = mov.u32 r44;"
        "V1@(pr0)	vr5 = sel vmsk0 vr5, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 7256;"
        "pseudo@0	@pseudo imm_1 = 16631;"
        "pseudo@0	@pseudo imm_2 = 53785;"
        "pseudo@0	@pseudo imm_3 = 44471;"
        "V0@(pr0)	vmsk0 = gteq.f32 vr30, r44;"
        "V1@(pr0)	vr0 = sel vmsk0 vr0, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65534;"
        "pseudo@0	@pseudo imm_1 = 48593;"
        "pseudo@0	@pseudo imm_2 = 59190;"
        "pseudo@0	@pseudo imm_3 = 49408;"
        "V0@(pr0)	vr1 = sel vmsk0 vr1, r44;"
        "V1@(pr0)	vr2 = sel vmsk0 vr2, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 43883;"
        "pseudo@0	@pseudo imm_1 = 49975;"
        "pseudo@0	@pseudo imm_2 = 42547;"
        "pseudo@0	@pseudo imm_3 = 50347;"
        "V0@(pr0)	vr3 = sel vmsk0 vr3, r44;"
        "V1@(pr0)	vr4 = sel vmsk0 vr4, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 18204;"
        "pseudo@0	@pseudo imm_1 = 50467;"
        "V0@(pr0)	vr5 = sel vmsk0 vr5, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16416;"
        "V0@(pr0)	vmsk0 = gteq.f32 vr30, r36;"
        "V1@(pr0)	vr0 = sel vmsk0 vr0, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 48594;"
        "pseudo@0	@pseudo imm_2 = 11405;"
        "pseudo@0	@pseudo imm_3 = 49538;"
        "V0@(pr0)	vr1 = sel vmsk0 vr1, r44;"
        "V1@(pr0)	vr2 = sel vmsk0 vr2, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 59011;"
        "pseudo@0	@pseudo imm_1 = 50237;"
        "pseudo@0	@pseudo imm_2 = 10042;"
        "pseudo@0	@pseudo imm_3 = 50745;"
        "V0@(pr0)	vr3 = sel vmsk0 vr3, r44;"
        "V1@(pr0)	vr4 = sel vmsk0 vr4, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 13955;"
        "pseudo@0	@pseudo imm_1 = 51005;"
        "V0@(pr0)	vr16 = mul.f32 vr14, vr14;"
        "V1@(pr0)	vr5 = sel vmsk0 vr5, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r1 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 6144;"
        "S1@(pr0)	r1 = sub.s32 r1, r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VS@(pr0)	 [vmem:r0,$1,$255] = st vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VS@(pr0)	 [vmem:r0,$1,$255] = st vr1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VS@(pr0)	 [vmem:r0,$1,$255] = st vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VS@(pr0)	 [vmem:r0,$1,$255] = st vr3;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VS@(pr0)	 [vmem:r0,$1,$255] = st vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 5120;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VS@(pr0)	 [vmem:r0,$1,$255] = st vr0;"
        "}"
        "{"
        "S1@(pr0)	[smem:r3] = st r1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr0 = and.u32 vr16, r44;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr0, r50;"
        "V1@(pr0)	vr2 = mov.u32 vr0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 23007;"
        "pseudo@0	@pseudo imm_1 = 24375;"
        "V0@(pr0)	vr3 = mov.u32 r44;"
        "V1@(pr0)	vr2 = shr.u32 vr2, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16320;"
        "V0@(pr0)	vr2 = sub.s32 vr3, vr2;"
        "V1@(pr0)	vr4 = mov.u32 r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32768;"
        "V0@(pr0)	vr3 = mul.f32 vr2, vr2;"
        "V1@(pr0)	vr6 = and.u32 vr16, r36;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr1, vr3;"
        "V1@(pr0)	vr5 = sub.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr5, vr2;"
        "V1@(pr0)	vr2 = mov.u32 vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr2, vr2;"
        "V1@(pr0)	vmsk0 = eq.f32 vr16, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr1, vr3;"
        "V1@(pr0)	vr5 = sub.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr5, vr2;"
        "V1@(pr0)	vr2 = mov.u32 vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr3 = mul.f32 vr2, vr2;"
        "V1@(pr0)	vr7 = or.u32 vr6, r44;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr1, vr3;"
        "V1@(pr0)	vr5 = sub.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr5, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr5, vr5;"
        "V1@(pr0)	vr1 = or.u32 vr6, vr1;"
        "}"
        "{"
        "V1@(pr0)	vr16 = sel vmsk0 vr1, vr7;"
        "}"
        "{"
        "S1@(pr0)	r1 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VL@(pr0)	vr5 = ld [vmem:r0,$1,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VL@(pr0)	vr1 = ld [vmem:r0,$1,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VL@(pr0)	vr2 = ld [vmem:r0,$1,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VL@(pr0)	vr3 = ld [vmem:r0,$1,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 4096;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VL@(pr0)	vr4 = ld [vmem:r0,$1,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 5120;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VL@(pr0)	vr0 = ld [vmem:r0,$1,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 6144;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r1 = add.s32 r1, r44;"
        "S1@(pr0)	[smem:r3] = st r1;"
        "}"
        "{"
        "V0@(pr0)	vr6 = mul.f32 vr5, vr16;"
        "V1@(pr0)	vr6 = add.f32 vr6, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr6 = mul.f32 vr6, vr16;"
        "V1@(pr0)	vr6 = add.f32 vr6, vr3;"
        "}"
        "{"
        "V0@(pr0)	vr6 = mul.f32 vr6, vr16;"
        "V1@(pr0)	vr6 = add.f32 vr6, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr6 = mul.f32 vr6, vr16;"
        "V1@(pr0)	vr6 = add.f32 vr6, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr6 = mul.f32 vr6, vr16;"
        "V1@(pr0)	vr6 = add.f32 vr6, vr0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 56168;"
        "pseudo@0	@pseudo imm_3 = 16438;"
        "V0@(pr0)	vmsk0 = gteq.f32 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 17492;"
        "pseudo@0	@pseudo imm_1 = 16876;"
        "pseudo@0	@pseudo imm_2 = 43294;"
        "pseudo@0	@pseudo imm_3 = 16958;"
        "V0@(pr0)	vr0 = mov.u32 r44;"
        "V1@(pr0)	vr0 = sel vmsk0 vr0, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 64327;"
        "pseudo@0	@pseudo imm_1 = 17276;"
        "pseudo@0	@pseudo imm_2 = 30558;"
        "pseudo@0	@pseudo imm_3 = 17448;"
        "V0@(pr0)	vr1 = mov.u32 r44;"
        "V1@(pr0)	vr1 = sel vmsk0 vr1, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 24622;"
        "pseudo@0	@pseudo imm_1 = 17469;"
        "pseudo@0	@pseudo imm_2 = 17010;"
        "pseudo@0	@pseudo imm_3 = 17747;"
        "V0@(pr0)	vr2 = mov.u32 r44;"
        "V1@(pr0)	vr2 = sel vmsk0 vr2, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 55594;"
        "pseudo@0	@pseudo imm_1 = 17464;"
        "pseudo@0	@pseudo imm_2 = 24021;"
        "pseudo@0	@pseudo imm_3 = 17837;"
        "V0@(pr0)	vr3 = mov.u32 r44;"
        "V1@(pr0)	vr3 = sel vmsk0 vr3, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 62194;"
        "pseudo@0	@pseudo imm_1 = 17179;"
        "pseudo@0	@pseudo imm_2 = 58320;"
        "pseudo@0	@pseudo imm_3 = 17645;"
        "V0@(pr0)	vr4 = mov.u32 r44;"
        "V1@(pr0)	vr4 = sel vmsk0 vr4, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 46135;"
        "pseudo@0	@pseudo imm_1 = 49310;"
        "pseudo@0	@pseudo imm_2 = 13185;"
        "pseudo@0	@pseudo imm_3 = 49927;"
        "V0@(pr0)	vr5 = mov.u32 r44;"
        "V1@(pr0)	vr5 = sel vmsk0 vr5, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 7256;"
        "pseudo@0	@pseudo imm_1 = 16631;"
        "pseudo@0	@pseudo imm_2 = 36248;"
        "pseudo@0	@pseudo imm_3 = 17058;"
        "V0@(pr0)	vmsk0 = gteq.f32 vr30, r44;"
        "V1@(pr0)	vr0 = sel vmsk0 vr0, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 63887;"
        "pseudo@0	@pseudo imm_1 = 17656;"
        "pseudo@0	@pseudo imm_2 = 30968;"
        "pseudo@0	@pseudo imm_3 = 18056;"
        "V0@(pr0)	vr1 = sel vmsk0 vr1, r44;"
        "V1@(pr0)	vr2 = sel vmsk0 vr2, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 47981;"
        "pseudo@0	@pseudo imm_1 = 18242;"
        "pseudo@0	@pseudo imm_2 = 22566;"
        "pseudo@0	@pseudo imm_3 = 18138;"
        "V0@(pr0)	vr3 = sel vmsk0 vr3, r44;"
        "V1@(pr0)	vr4 = sel vmsk0 vr4, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 31096;"
        "pseudo@0	@pseudo imm_1 = 50579;"
        "V0@(pr0)	vr5 = sel vmsk0 vr5, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16416;"
        "pseudo@0	@pseudo imm_2 = 25911;"
        "pseudo@0	@pseudo imm_3 = 17185;"
        "V0@(pr0)	vmsk0 = gteq.f32 vr30, r36;"
        "V1@(pr0)	vr0 = sel vmsk0 vr0, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 35607;"
        "pseudo@0	@pseudo imm_1 = 17908;"
        "pseudo@0	@pseudo imm_2 = 48342;"
        "pseudo@0	@pseudo imm_3 = 18434;"
        "V0@(pr0)	vr1 = sel vmsk0 vr1, r44;"
        "V1@(pr0)	vr2 = sel vmsk0 vr2, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 45724;"
        "pseudo@0	@pseudo imm_1 = 18735;"
        "pseudo@0	@pseudo imm_2 = 48788;"
        "pseudo@0	@pseudo imm_3 = 18722;"
        "V0@(pr0)	vr3 = sel vmsk0 vr3, r44;"
        "V1@(pr0)	vr4 = sel vmsk0 vr4, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52040;"
        "pseudo@0	@pseudo imm_1 = 51343;"
        "V1@(pr0)	vr5 = sel vmsk0 vr5, r44;"
        "}"
        "{"
        "V0@(pr0)	vr7 = mul.f32 vr5, vr16;"
        "V1@(pr0)	vr7 = add.f32 vr7, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr7 = mul.f32 vr7, vr16;"
        "V1@(pr0)	vr7 = add.f32 vr7, vr3;"
        "}"
        "{"
        "V0@(pr0)	vr7 = mul.f32 vr7, vr16;"
        "V1@(pr0)	vr7 = add.f32 vr7, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr7 = mul.f32 vr7, vr16;"
        "V1@(pr0)	vr7 = add.f32 vr7, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr7 = mul.f32 vr7, vr16;"
        "V1@(pr0)	vr7 = add.f32 vr7, vr0;"
        "}"
        "{"
        "V0@(pr0)	vr7 = mul.f32 vr7, vr16;"
        "V1@(pr0)	vr7 = add.f32 vr7, r49;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr0 = and.u32 vr7, r44;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr0, r50;"
        "V1@(pr0)	vr2 = mov.u32 vr0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 23007;"
        "pseudo@0	@pseudo imm_1 = 24375;"
        "V0@(pr0)	vr3 = mov.u32 r44;"
        "V1@(pr0)	vr2 = shr.u32 vr2, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16320;"
        "V0@(pr0)	vr2 = sub.s32 vr3, vr2;"
        "V1@(pr0)	vr4 = mov.u32 r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32768;"
        "V0@(pr0)	vr3 = mul.f32 vr2, vr2;"
        "V1@(pr0)	vr16 = and.u32 vr7, r36;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr1, vr3;"
        "V1@(pr0)	vr5 = sub.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr5, vr2;"
        "V1@(pr0)	vr2 = mov.u32 vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr2, vr2;"
        "V1@(pr0)	vmsk0 = eq.f32 vr7, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr1, vr3;"
        "V1@(pr0)	vr5 = sub.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr5, vr2;"
        "V1@(pr0)	vr2 = mov.u32 vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr3 = mul.f32 vr2, vr2;"
        "V1@(pr0)	vr7 = or.u32 vr16, r44;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr1, vr3;"
        "V1@(pr0)	vr5 = sub.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr5, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr5, vr5;"
        "V1@(pr0)	vr1 = or.u32 vr16, vr1;"
        "}"
        "{"
        "V1@(pr0)	vr15 = sel vmsk0 vr1, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr16 = and.u32 vr14, r44;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr16, r50;"
        "V1@(pr0)	vr2 = mov.u32 vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 23007;"
        "pseudo@0	@pseudo imm_1 = 24375;"
        "V0@(pr0)	vr3 = mov.u32 r44;"
        "V1@(pr0)	vr2 = shr.u32 vr2, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16320;"
        "V0@(pr0)	vr2 = sub.s32 vr3, vr2;"
        "V1@(pr0)	vr4 = mov.u32 r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32768;"
        "V0@(pr0)	vr3 = mul.f32 vr2, vr2;"
        "V1@(pr0)	vr0 = and.u32 vr14, r36;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr1, vr3;"
        "V1@(pr0)	vr5 = sub.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr5, vr2;"
        "V1@(pr0)	vr2 = mov.u32 vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr2, vr2;"
        "V1@(pr0)	vmsk0 = eq.f32 vr14, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr1, vr3;"
        "V1@(pr0)	vr5 = sub.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr5, vr2;"
        "V1@(pr0)	vr2 = mov.u32 vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr3 = mul.f32 vr2, vr2;"
        "V1@(pr0)	vr7 = or.u32 vr0, r44;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr1, vr3;"
        "V1@(pr0)	vr5 = sub.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr5, vr2;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr5, vr5;"
        "V1@(pr0)	vr1 = or.u32 vr0, vr1;"
        "}"
        "{"
        "V1@(pr0)	vr16 = sel vmsk0 vr1, vr7;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16064;"
        "V0@(pr0)	vr15 = mul.f32 vr15, vr6;"
        "V1@(pr0)	vr15 = add.f32 vr15, r36;"
        "}"
        "{"
        "V0@(pr0)	vr29 = mul.f32 vr16, vr15;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mul.f32 vr28, vr17;"
        "}"
        "{"
        "V0@(pr0)	vr29 = mul.f32 vr29, vr31;"
        "V1@(pr0)	vr28 = sub.f32 vr28, vr29;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 28347;"
        "pseudo@0	@pseudo imm_1 = 16144;"
        "V0@(pr0)	vr28 = mul.f32 vr28, r44;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mul.f32 vr28, vr12;"
        "V1@(pr0)	vmsk0 = ls.s32 vr10, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 23552;"
        "V0@(pr0)	vmsk1 = gt.f32 vr14, r36;"
        "V1@(pr0)	vr11 = sel vmsk1 vr12, vr11;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mul.f32 vr11, r57;"
        "V1@(pr0)	vr11 = sel vmsk0 vr11, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr28 = mul.f32 vr10, vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 22482;"
        "pseudo@0	@pseudo imm_1 = 13141;"
        "pseudo@0	@pseudo imm_2 = 11830;"
        "pseudo@0	@pseudo imm_3 = 46982;"
        "V0@(pr0)	vr29 = mul.f32 vr28, r44;"
        "V1@(pr0)	vr29 = add.f32 vr29, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 27901;"
        "pseudo@0	@pseudo imm_1 = 15032;"
        "V0@(pr0)	vr29 = mul.f32 vr29, vr28;"
        "V1@(pr0)	vr29 = add.f32 vr29, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 48512;"
        "V0@(pr0)	vr29 = mul.f32 vr29, vr28;"
        "V1@(pr0)	vr29 = add.f32 vr29, r44;"
        "}"
        "{"
        "V0@(pr0)	vr29 = mul.f32 vr29, vr28;"
        "}"
        "{"
        "V0@(pr0)	vr29 = mul.f32 vr29, vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 22142;"
        "pseudo@0	@pseudo imm_1 = 11609;"
        "pseudo@0	@pseudo imm_2 = 25670;"
        "pseudo@0	@pseudo imm_3 = 12717;"
        "V0@(pr0)	vr30 = mul.f32 vr28, r44;"
        "V1@(pr0)	vr30 = add.f32 vr30, r45;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65474;"
        "pseudo@0	@pseudo imm_1 = 13725;"
        "V0@(pr0)	vr30 = mul.f32 vr28, vr30;"
        "V1@(pr0)	vr30 = add.f32 vr30, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 64182;"
        "pseudo@0	@pseudo imm_1 = 14658;"
        "V0@(pr0)	vr30 = mul.f32 vr28, vr30;"
        "V1@(pr0)	vr30 = add.f32 vr30, r44;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 59481;"
        "pseudo@0	@pseudo imm_1 = 15516;"
        "V0@(pr0)	vr30 = mul.f32 vr28, vr30;"
        "V1@(pr0)	vr30 = add.f32 vr30, r44;"
        "}"
        "{"
        "V0@(pr0)	vr30 = mul.f32 vr28, vr30;"
        "V1@(pr0)	vr30 = add.f32 vr30, r49;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32640;"
        "V0@(pr0)	vr0 = and.u32 vr30, r44;"
        "V1@(pr0)	vmsk1 = eq.s32 vr0, r45;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr0, r50;"
        "V1@(pr0)	vr2 = mov.u32 vr0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 23007;"
        "pseudo@0	@pseudo imm_1 = 24375;"
        "V0@(pr0)	vr3 = mov.u32 r44;"
        "V1@(pr0)	vr2 = shr.u32 vr2, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16320;"
        "V0@(pr0)	vr2 = sub.s32 vr3, vr2;"
        "V1@(pr0)	vr4 = mov.u32 r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32768;"
        "V0@(pr0)	vr3 = mul.f32 vr2, vr2;"
        "V1@(pr0)	vr6 = and.u32 vr30, r36;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr1, vr3;"
        "V1@(pr0)	vr5 = sub.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr5, vr2;"
        "V1@(pr0)	vr2 = mov.u32 vr5;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mul.f32 vr2, vr2;"
        "V1@(pr0)	vmsk0 = eq.f32 vr30, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr1, vr3;"
        "V1@(pr0)	vr5 = sub.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr5, vr2;"
        "V1@(pr0)	vr2 = mov.u32 vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32640;"
        "V0@(pr0)	vr3 = mul.f32 vr2, vr2;"
        "V1@(pr0)	vr7 = or.u32 vr6, r44;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr1, vr3;"
        "V1@(pr0)	vr5 = sub.f32 vr4, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr5, vr2;"
        "V1@(pr0)	vr2 = or.u32 vr6, r46;"
        "}"
        "{"
        "V0@(pr0)	vr1 = mul.f32 vr5, vr5;"
        "V1@(pr0)	vr1 = or.u32 vr6, vr1;"
        "}"
        "{"
        "V0@(pr0)	vr30 = sel vmsk0 vr1, vr7;"
        "V1@(pr0)	vr30 = sel vmsk1 vr30, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 12800;"
        "V0@(pr0)	vr30 = mul.f32 vr30, vr29;"
        "V1@(pr0)	vmsk0 = ls.f32 vr14, r44;"
        "}"
        "{"
        "V0@(pr0)	vr13 = mul.f32 vr10, r50;"
        "V1@(pr0)	vr12 = add.f32 vr13, vr30;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16384;"
        "V0@(pr0)	vr12 = sel vmsk0 vr12, vr13;"
        "V1@(pr0)	vmsk0 = gteq.f32 vr14, r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32640;"
        "V0@(pr0)	vr12 = sel vmsk0 vr12, vr11;"
        "V1@(pr0)	vmsk0 = eq.s32 vr14, r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32768;"
        "V0@(pr0)	vr13 = and.u32 vr10, r36;"
        "V1@(pr0)	vr10 = sel vmsk0 vr12, vr13;"
        "}"
        "{"
        "V0@(pr0)	vr3 = mov.u32 vr10;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 52;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r6, r44;"
        "S1@(pr0)	r3 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r3 = add.s32 r3, r44;"
        "S1@(pr0)	r3 = add.s32 r3, r6;"
        "}"
        "{"
        "S1@(pr0)	r1 = ld [smem:r3];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VL@(pr0)	vr11 = ld [vmem:r0,$1,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VL@(pr0)	vr10 = ld [vmem:r0,$1,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2048;"
        "pseudo@0	@pseudo imm_1 = 5;"
        "S0@(pr0)	r0 = add.s32 r1, r32;"
        "S1@(pr0)	r0 = shr.u32 r0, r33;"
        "VL@(pr0)	vr2 = ld [vmem:r0,$1,$255];"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 3072;"
        "pseudo@0	@pseudo imm_1 = 0;"
        "S0@(pr0)	r1 = add.s32 r1, r44;"
        "S1@(pr0)	[smem:r3] = st r1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr0 = and.u32 vr11, r44;"
        "V1@(pr0)	vmsk0 = ls.s32 vr10, r46;"
        "}"
        "{"
        "V0@(pr0)	vr4 = mov.u32 r46;"
        "V1@(pr0)	vr4 = sub.s32 vr4, vr10;"
        "}"
        "{"
        "V0@(pr0)	vr10 = sel vmsk0 vr10, vr4;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1;"
        "V1@(pr0)	vr1 = and.u32 vr11, r47;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mov.u32 vr2;"
        "V1@(pr0)	vr6 = mov.u32 vr3;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mov.u32 vr10;"
        "MTI@(pr0)	trf<0> = max vr12;"
        "}"
        "{"
        "MTR@(pr0)	vr12 = pop trf<0>;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 7;"
        "MTI@(pr0)	trf<0> = tsps.single vr12, r34;"
        "}"
        "{"
        "MTR@(pr0)	vr12 = pop trf<0>;"
        "}"
        "{"
        "MTI@(pr0)	trf<0> = max vr12;"
        "}"
        "{"
        "MTR@(pr0)	vr12 = pop trf<0>;"
        "}"
        "{"
        "VS@(pr0)	(vsf) = push vr12;"
        "}"
        "{"
        "S0@(pr0)	r1 = pop;"
        "S1@(pr0)	r2 = mov.u32 r48;"
        "}"
        "{"
        "V0@(pr0)	vr7 = mov.u32 r49;"
        "V1@(pr0)	vr12 = cvtinttof.f32 vr10;"
        "}"
        "{"
        "V0@(pr0)	vr13 = mov.u32 vr3;"
        "V1@(pr0)	vr4 = mov.u32 vr0;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32640;"
        "V0@(pr0)	vr15 = and.u32 vr4, r44;"
        "V1@(pr0)	vmsk6 = eq.s32 vr15, r45;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mul.f32 vr15, r50;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 23007;"
        "pseudo@0	@pseudo imm_1 = 24375;"
        "V0@(pr0)	vr17 = mov.u32 r44;"
        "V1@(pr0)	vr15 = shr.u32 vr15, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16320;"
        "V0@(pr0)	vr15 = sub.s32 vr17, vr15;"
        "V1@(pr0)	vr28 = mov.u32 r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32768;"
        "V0@(pr0)	vr17 = mul.f32 vr15, vr15;"
        "V1@(pr0)	vr30 = and.u32 vr4, r36;"
        "}"
        "{"
        "V0@(pr0)	vr29 = mul.f32 vr16, vr17;"
        "V1@(pr0)	vr29 = sub.f32 vr28, vr29;"
        "}"
        "{"
        "V0@(pr0)	vr29 = mul.f32 vr29, vr15;"
        "V1@(pr0)	vr15 = mov.u32 vr29;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mul.f32 vr15, vr15;"
        "V1@(pr0)	vmsk5 = eq.f32 vr4, r46;"
        "}"
        "{"
        "V0@(pr0)	vr29 = mul.f32 vr16, vr17;"
        "V1@(pr0)	vr29 = sub.f32 vr28, vr29;"
        "}"
        "{"
        "V0@(pr0)	vr29 = mul.f32 vr29, vr15;"
        "V1@(pr0)	vr15 = mov.u32 vr29;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32640;"
        "V0@(pr0)	vr17 = mul.f32 vr15, vr15;"
        "V1@(pr0)	vr31 = or.u32 vr30, r44;"
        "}"
        "{"
        "V0@(pr0)	vr29 = mul.f32 vr16, vr17;"
        "V1@(pr0)	vr29 = sub.f32 vr28, vr29;"
        "}"
        "{"
        "V0@(pr0)	vr29 = mul.f32 vr29, vr15;"
        "V1@(pr0)	vr15 = or.u32 vr30, r46;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mul.f32 vr29, vr29;"
        "V1@(pr0)	vr16 = or.u32 vr30, vr16;"
        "}"
        "{"
        "V0@(pr0)	vr4 = sel vmsk5 vr16, vr31;"
        "V1@(pr0)	vr4 = sel vmsk6 vr4, vr15;"
        "}"
        "{"
        "S0@(pr0)	r2 = add.s32 r2, r48;"
        "S1@(pr0)	pr1 = ls.s32 r2, r1;"
        "V1@(pr0)	vr14 = add.f32 vr7, vr7;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mul.f32 vr4, vr14;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mul.f32 vr14, vr6;"
        "V1@(pr0)	vr14 = sub.f32 vr14, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mov.u32 vr6;"
        "V1@(pr0)	vr6 = mov.u32 vr14;"
        "}"
        "{"
        "V1@(pr0)	vmsk0 = gteq.f32 vr7, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr13 = sel vmsk0 vr6, vr13;"
        "}"
        "{"
        "V1@(pr0)	vr7 = add.f32 vr7, r49;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65528;"
        "S0@(pr1)	(pc) = brrel r32;"
        "}"
        "{"
        "V1@(pr0)	vmsk2 = gt.f32 vr12, vr0;"
        "}"
        "{"
        "V0@(pr0)	vr13 = sel vmsk2 vr13, r46;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mul.f32 vr0, r50;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "S0@(pr0)	r2 = mov.u32 r32;"
        "V0@(pr0)	vr16 = mov.u32 r49;"
        "V1@(pr0)	vr7 = mov.u32 r32;"
        "}"
        "{"
        "V0@(pr0)	vr6 = mov.u32 vr17;"
        "V1@(pr0)	vr5 = mov.u32 vr16;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 r51;"
        "V1@(pr0)	vr15 = mov.u32 vr6;"
        "}"
        "{"
        "S0@(pr0)	r2 = add.s32 r2, r48;"
        "S1@(pr0)	pr1 = lseq.s32 r2, r1;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mul.f32 vr16, vr14;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mul.f32 vr17, vr15;"
        "}"
        "{"
        "V0@(pr0)	vr7 = add.s32 vr7, r48;"
        "V1@(pr0)	vr14 = add.f32 vr14, r49;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = gt.s32 vr7, vr10;"
        "V1@(pr0)	vr5 = sel vmsk0 vr5, vr16;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk0 vr6, vr17;"
        "V1@(pr0)	vr7 = sel vmsk0 vr7, r47;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65529;"
        "S0@(pr1)	(pc) = brrel r32;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32640;"
        "V0@(pr0)	vr15 = and.u32 vr5, r44;"
        "V1@(pr0)	vmsk6 = eq.s32 vr15, r45;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mul.f32 vr15, r50;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 23007;"
        "pseudo@0	@pseudo imm_1 = 24375;"
        "V0@(pr0)	vr17 = mov.u32 r44;"
        "V1@(pr0)	vr15 = shr.u32 vr15, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16320;"
        "V0@(pr0)	vr15 = sub.s32 vr17, vr15;"
        "V1@(pr0)	vr28 = mov.u32 r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32768;"
        "V0@(pr0)	vr17 = mul.f32 vr15, vr15;"
        "V1@(pr0)	vr30 = and.u32 vr5, r36;"
        "}"
        "{"
        "V0@(pr0)	vr29 = mul.f32 vr16, vr17;"
        "V1@(pr0)	vr29 = sub.f32 vr28, vr29;"
        "}"
        "{"
        "V0@(pr0)	vr29 = mul.f32 vr29, vr15;"
        "V1@(pr0)	vr15 = mov.u32 vr29;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mul.f32 vr15, vr15;"
        "V1@(pr0)	vmsk5 = eq.f32 vr5, r46;"
        "}"
        "{"
        "V0@(pr0)	vr29 = mul.f32 vr16, vr17;"
        "V1@(pr0)	vr29 = sub.f32 vr28, vr29;"
        "}"
        "{"
        "V0@(pr0)	vr29 = mul.f32 vr29, vr15;"
        "V1@(pr0)	vr15 = mov.u32 vr29;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32640;"
        "V0@(pr0)	vr17 = mul.f32 vr15, vr15;"
        "V1@(pr0)	vr31 = or.u32 vr30, r44;"
        "}"
        "{"
        "V0@(pr0)	vr29 = mul.f32 vr16, vr17;"
        "V1@(pr0)	vr29 = sub.f32 vr28, vr29;"
        "}"
        "{"
        "V0@(pr0)	vr29 = mul.f32 vr29, vr15;"
        "V1@(pr0)	vr15 = or.u32 vr30, r46;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mul.f32 vr29, vr29;"
        "V1@(pr0)	vr16 = or.u32 vr30, vr16;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk5 vr16, vr31;"
        "V1@(pr0)	vr5 = sel vmsk6 vr5, vr15;"
        "}"
        "{"
        "V0@(pr0)	vr6 = mul.f32 vr6, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 33;"
        "V0@(pr0)	vmsk0 = gt.s32 vr10, r32;"
        "V1@(pr0)	vr6 = sel vmsk0 vr6, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 12416;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr0, r36;"
        "V1@(pr0)	vr6 = sel vmsk1 vr6, r46;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk2 vr6, r46;"
        "V1@(pr0)	vr13 = add.f32 vr6, vr13;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 12416;"
        "V0@(pr0)	vmsk1 = gteq.s32 vr0, r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 48160;"
        "pseudo@0	@pseudo imm_1 = 19646;"
        "V0@(pr0)	vr14 = mov.u32 r44;"
        "}"
        "{"
        "V0@(pr0)	vr12 = sel vmsk1 vr14, vr12;"
        "V1@(pr0)	vr12 = sel vmsk2 vr14, vr12;"
        "}"
        "{"
        "V1@(pr0)	vr14 = add.f32 vr12, vr12;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mul.f32 vr4, r51;"
        "V1@(pr0)	vr28 = mov.u32 r48;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr4, vr14;"
        "V1@(pr0)	vr17 = add.f32 vr5, vr16;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 1024;"
        "S0@(pr0)	r1 = mov.u32 r32;"
        "V0@(pr0)	vr6 = mul.f32 vr5, vr17;"
        "V1@(pr0)	vr6 = sub.f32 vr6, r49;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 27432;"
        "pseudo@0	@pseudo imm_1 = 20078;"
        "V0@(pr0)	vmsk3 = gteq.f32 vr6, r44;"
        "}"
        "{"
        "V0@(pr0)	vr14 = add.s32 vr28, r48;"
        "V1@(pr0)	vr17 = add.f32 vr17, vr16;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mul.f32 vr17, vr6;"
        "V1@(pr0)	vr15 = sub.f32 vr15, vr5;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 27432;"
        "pseudo@0	@pseudo imm_1 = 20078;"
        "V0@(pr0)	vmsk0 = gteq.f32 vr15, r44;"
        "V1@(pr0)	vr5 = mov.u32 vr6;"
        "}"
        "{"
        "V0@(pr0)	vr14 = sel vmsk0 vr14, vr28;"
        "V1@(pr0)	vr7 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr6 = mov.u32 vr15;"
        "V1@(pr0)	vr7 = sel vmsk0 vr7, r49;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65472;"
        "V0@(pr0)	vmsk0 = eq.s32 vr15, r36;"
        "V1@(pr0)	vr7 = sel vmsk0 vr7, r49;"
        "}"
        "{"
        "V1@(pr0)	vr28 = sel vmsk0 vr14, vr28;"
        "}"
        "{"
        "MTI@(pr0)	trf<0> = sum vr7;"
        "}"
        "{"
        "MTR@(pr0)	vr7 = pop trf<0>;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 7;"
        "MTI@(pr0)	trf<0> = tsps.single vr7, r34;"
        "}"
        "{"
        "MTR@(pr0)	vr7 = pop trf<0>;"
        "}"
        "{"
        "MTI@(pr0)	trf<0> = sum vr7;"
        "}"
        "{"
        "MTR@(pr0)	vr7 = pop trf<0>;"
        "}"
        "{"
        "VS@(pr0)	(vsf) = push vr7;"
        "}"
        "{"
        "S0@(pr0)	r2 = pop;"
        "S1@(pr0)	r2 = cvtf32toint32 r2, r56;"
        "}"
        "{"
        "S0@(pr0)	pr1 = ls.s32 r2, r1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65519;"
        "S0@(pr1)	(pc) = brrel r32;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 r48;"
        "V1@(pr0)	vr14 = sel vmsk3 vr14, r46;"
        "}"
        "{"
        "V0@(pr0)	vr28 = add.s32 vr14, vr28;"
        "}"
        "{"
        "V0@(pr0)	vr12 = add.s32 vr10, vr10;"
        "V1@(pr0)	vr7 = add.s32 vr10, vr28;"
        "}"
        "{"
        "V1@(pr0)	vr7 = cvtinttof.f32 vr7;"
        "}"
        "{"
        "V0@(pr0)	vr7 = mul.f32 vr7, r51;"
        "}"
        "{"
        "V0@(pr0)	vr12 = cvtinttof.f32 vr12;"
        "V1@(pr0)	vr14 = mov.u32 r46;"
        "}"
        "{"
        "MTI@(pr0)	trf<0> = max vr7;"
        "}"
        "{"
        "MTR@(pr0)	vr15 = pop trf<0>;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 7;"
        "MTI@(pr0)	trf<0> = tsps.single vr15, r34;"
        "}"
        "{"
        "MTR@(pr0)	vr15 = pop trf<0>;"
        "}"
        "{"
        "MTI@(pr0)	trf<0> = max vr15;"
        "}"
        "{"
        "MTR@(pr0)	vr15 = pop trf<0>;"
        "}"
        "{"
        "VS@(pr0)	(vsf) = push vr15;"
        "}"
        "{"
        "S0@(pr0)	r2 = pop;"
        "S1@(pr0)	r2 = cvtf32toint32 r2, r56;"
        "}"
        "{"
        "MTI@(pr0)	trf<0> = min vr12;"
        "}"
        "{"
        "MTR@(pr0)	vr15 = pop trf<0>;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_2 = 7;"
        "MTI@(pr0)	trf<0> = tsps.single vr15, r34;"
        "}"
        "{"
        "MTR@(pr0)	vr15 = pop trf<0>;"
        "}"
        "{"
        "MTI@(pr0)	trf<0> = min vr15;"
        "}"
        "{"
        "MTR@(pr0)	vr15 = pop trf<0>;"
        "}"
        "{"
        "VS@(pr0)	(vsf) = push vr15;"
        "}"
        "{"
        "S0@(pr0)	r3 = pop;"
        "S1@(pr0)	r3 = cvtf32toint32 r3, r56;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 2;"
        "S0@(pr0)	r2 = sub.s32 r2, r32;"
        "S1@(pr0)	pr1 = gteq.s32 r2, r3;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr7, vr4;"
        "V1@(pr0)	vr5 = sub.f32 vr5, vr14;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32640;"
        "V0@(pr0)	vr15 = and.u32 vr5, r44;"
        "V1@(pr0)	vmsk6 = eq.s32 vr15, r45;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mul.f32 vr15, r50;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 23007;"
        "pseudo@0	@pseudo imm_1 = 24375;"
        "V0@(pr0)	vr17 = mov.u32 r44;"
        "V1@(pr0)	vr15 = shr.u32 vr15, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16320;"
        "V0@(pr0)	vr15 = sub.s32 vr17, vr15;"
        "V1@(pr0)	vr28 = mov.u32 r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32768;"
        "V0@(pr0)	vr17 = mul.f32 vr15, vr15;"
        "V1@(pr0)	vr30 = and.u32 vr5, r36;"
        "}"
        "{"
        "V0@(pr0)	vr29 = mul.f32 vr16, vr17;"
        "V1@(pr0)	vr29 = sub.f32 vr28, vr29;"
        "}"
        "{"
        "V0@(pr0)	vr29 = mul.f32 vr29, vr15;"
        "V1@(pr0)	vr15 = mov.u32 vr29;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mul.f32 vr15, vr15;"
        "V1@(pr0)	vmsk5 = eq.f32 vr5, r46;"
        "}"
        "{"
        "V0@(pr0)	vr29 = mul.f32 vr16, vr17;"
        "V1@(pr0)	vr29 = sub.f32 vr28, vr29;"
        "}"
        "{"
        "V0@(pr0)	vr29 = mul.f32 vr29, vr15;"
        "V1@(pr0)	vr15 = mov.u32 vr29;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32640;"
        "V0@(pr0)	vr17 = mul.f32 vr15, vr15;"
        "V1@(pr0)	vr31 = or.u32 vr30, r44;"
        "}"
        "{"
        "V0@(pr0)	vr29 = mul.f32 vr16, vr17;"
        "V1@(pr0)	vr29 = sub.f32 vr28, vr29;"
        "}"
        "{"
        "V0@(pr0)	vr29 = mul.f32 vr29, vr15;"
        "V1@(pr0)	vr15 = or.u32 vr30, r46;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mul.f32 vr29, vr29;"
        "V1@(pr0)	vr16 = or.u32 vr30, vr16;"
        "}"
        "{"
        "V0@(pr0)	vr5 = sel vmsk5 vr16, vr31;"
        "V1@(pr0)	vr5 = sel vmsk6 vr5, vr15;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = ls.f32 vr7, vr12;"
        "V1@(pr0)	vr14 = sel vmsk0 vr5, vr14;"
        "}"
        "{"
        "V1@(pr0)	vr7 = sub.f32 vr7, r51;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65516;"
        "S0@(pr1)	(pc) = brrel r32;"
        "}"
        "{"
        "S0@(pr0)	r2 = mov.u32 r1;"
        "S1@(pr0)	r2 = sub.s32 r2, r48;"
        "V0@(pr0)	vr7 = mov.u32 vr10;"
        "V1@(pr0)	vr7 = sub.s32 vr7, r48;"
        "}"
        "{"
        "V0@(pr0)	vr7 = cvtinttof.f32 vr7;"
        "}"
        "{"
        "V0@(pr0)	vr6 = mov.u32 r49;"
        "V1@(pr0)	vr28 = mov.u32 vr14;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mov.u32 r49;"
        "V1@(pr0)	vr16 = mov.u32 vr14;"
        "}"
        "{"
        "S0@(pr0)	r2 = sub.s32 r2, r48;"
        "S1@(pr0)	pr1 = gt.s32 r2, r46;"
        "V1@(pr0)	vr15 = add.f32 vr7, vr7;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mul.f32 vr15, vr4;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mul.f32 vr15, vr17;"
        "V1@(pr0)	vr15 = sub.f32 vr15, vr16;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mov.u32 vr17;"
        "V1@(pr0)	vr17 = mov.u32 vr15;"
        "}"
        "{"
        "V0@(pr0)	vr29 = mov.u32 vr17;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32640;"
        "V0@(pr0)	vr0 = and.u32 vr29, r44;"
        "V1@(pr0)	vmsk6 = eq.s32 vr0, r45;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr0, r50;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 23007;"
        "pseudo@0	@pseudo imm_1 = 24375;"
        "V0@(pr0)	vr12 = mov.u32 r44;"
        "V1@(pr0)	vr0 = shr.u32 vr0, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16320;"
        "V0@(pr0)	vr0 = sub.s32 vr12, vr0;"
        "V1@(pr0)	vr14 = mov.u32 r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32768;"
        "V0@(pr0)	vr12 = mul.f32 vr0, vr0;"
        "V1@(pr0)	vr30 = and.u32 vr29, r36;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mul.f32 vr5, vr12;"
        "V1@(pr0)	vr15 = sub.f32 vr14, vr15;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mul.f32 vr15, vr0;"
        "V1@(pr0)	vr0 = mov.u32 vr15;"
        "}"
        "{"
        "V0@(pr0)	vr12 = mul.f32 vr0, vr0;"
        "V1@(pr0)	vmsk5 = eq.f32 vr29, r46;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mul.f32 vr5, vr12;"
        "V1@(pr0)	vr15 = sub.f32 vr14, vr15;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mul.f32 vr15, vr0;"
        "V1@(pr0)	vr0 = mov.u32 vr15;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32640;"
        "V0@(pr0)	vr12 = mul.f32 vr0, vr0;"
        "V1@(pr0)	vr31 = or.u32 vr30, r44;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mul.f32 vr5, vr12;"
        "V1@(pr0)	vr15 = sub.f32 vr14, vr15;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mul.f32 vr15, vr0;"
        "V1@(pr0)	vr0 = or.u32 vr30, r46;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr15, vr15;"
        "V1@(pr0)	vr5 = or.u32 vr30, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr29 = sel vmsk5 vr5, vr31;"
        "V1@(pr0)	vr29 = sel vmsk6 vr29, vr0;"
        "}"
        "{"
        "V0@(pr0)	vr5 = mul.f32 vr16, vr29;"
        "}"
        "{"
        "V0@(pr0)	vr15 = mul.f32 vr28, vr29;"
        "V1@(pr0)	vr29 = mov.u32 r49;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 761;"
        "pseudo@0	@pseudo imm_1 = 20501;"
        "V0@(pr0)	vmsk3 = gt.f32 vr17, r44;"
        "V1@(pr0)	vr16 = sel vmsk3 vr16, vr5;"
        "}"
        "{"
        "V0@(pr0)	vr17 = sel vmsk3 vr17, vr29;"
        "V1@(pr0)	vr15 = sel vmsk3 vr28, vr15;"
        "}"
        "{"
        "V0@(pr0)	vmsk0 = lseq.f32 vr7, r46;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk0 vr17, vr6;"
        "V1@(pr0)	vr28 = sel vmsk0 vr15, vr28;"
        "}"
        "{"
        "V1@(pr0)	vr7 = sub.f32 vr7, r49;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65508;"
        "S0@(pr1)	(pc) = brrel r32;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mov.u32 vr28;"
        "}"
        "{"
        "V0@(pr0)	vr14 = mul.f32 vr14, vr2;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "pseudo@0	@pseudo imm_2 = 0;"
        "pseudo@0	@pseudo imm_3 = 32640;"
        "V0@(pr0)	vr15 = and.u32 vr6, r44;"
        "V1@(pr0)	vmsk6 = eq.s32 vr15, r45;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mul.f32 vr15, r50;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 23007;"
        "pseudo@0	@pseudo imm_1 = 24375;"
        "V0@(pr0)	vr17 = mov.u32 r44;"
        "V1@(pr0)	vr15 = shr.u32 vr15, r48;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 16320;"
        "V0@(pr0)	vr15 = sub.s32 vr17, vr15;"
        "V1@(pr0)	vr28 = mov.u32 r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32768;"
        "V0@(pr0)	vr17 = mul.f32 vr15, vr15;"
        "V1@(pr0)	vr30 = and.u32 vr6, r36;"
        "}"
        "{"
        "V0@(pr0)	vr29 = mul.f32 vr16, vr17;"
        "V1@(pr0)	vr29 = sub.f32 vr28, vr29;"
        "}"
        "{"
        "V0@(pr0)	vr29 = mul.f32 vr29, vr15;"
        "V1@(pr0)	vr15 = mov.u32 vr29;"
        "}"
        "{"
        "V0@(pr0)	vr17 = mul.f32 vr15, vr15;"
        "V1@(pr0)	vmsk5 = eq.f32 vr6, r46;"
        "}"
        "{"
        "V0@(pr0)	vr29 = mul.f32 vr16, vr17;"
        "V1@(pr0)	vr29 = sub.f32 vr28, vr29;"
        "}"
        "{"
        "V0@(pr0)	vr29 = mul.f32 vr29, vr15;"
        "V1@(pr0)	vr15 = mov.u32 vr29;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 0;"
        "pseudo@0	@pseudo imm_1 = 32640;"
        "V0@(pr0)	vr17 = mul.f32 vr15, vr15;"
        "V1@(pr0)	vr31 = or.u32 vr30, r44;"
        "}"
        "{"
        "V0@(pr0)	vr29 = mul.f32 vr16, vr17;"
        "V1@(pr0)	vr29 = sub.f32 vr28, vr29;"
        "}"
        "{"
        "V0@(pr0)	vr29 = mul.f32 vr29, vr15;"
        "V1@(pr0)	vr15 = or.u32 vr30, r46;"
        "}"
        "{"
        "V0@(pr0)	vr16 = mul.f32 vr29, vr29;"
        "V1@(pr0)	vr16 = or.u32 vr30, vr16;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk5 vr16, vr31;"
        "V1@(pr0)	vr6 = sel vmsk6 vr6, vr15;"
        "}"
        "{"
        "V0@(pr0)	vr6 = mul.f32 vr14, vr6;"
        "V1@(pr0)	vr5 = mov.u32 r46;"
        "}"
        "{"
        "V0@(pr0)	vr6 = sel vmsk1 vr5, vr6;"
        "V1@(pr0)	vr6 = sel vmsk2 vr5, vr6;"
        "}"
        "{"
        "V1@(pr0)	vr13 = add.f32 vr6, vr13;"
        "}"
        "{"
        "V0@(pr0)	vr13 = xor.u32 vr13, vr1;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 65535;"
        "pseudo@0	@pseudo imm_1 = 32767;"
        "V0@(pr0)	vr0 = and.u32 vr11, r44;"
        "V1@(pr0)	vmsk0 = eq.s32 vr0, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32640;"
        "pseudo@0	@pseudo imm_1 = 32704;"
        "V0@(pr0)	vr13 = sel vmsk0 vr13, r37;"
        "V1@(pr0)	vmsk1 = eq.s32 vr0, r36;"
        "}"
        "{"
        "V0@(pr0)	vr13 = sel vmsk1 vr13, r46;"
        "V1@(pr0)	vmsk0 = eq.s32 vr10, r48;"
        "}"
        "{"
        "V0@(pr0)	vr13 = sel vmsk0 vr13, vr3;"
        "V1@(pr0)	vmsk1 = eq.s32 vr10, r46;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32640;"
        "V0@(pr0)	vr13 = sel vmsk1 vr13, vr2;"
        "V1@(pr0)	vmsk0 = gt.s32 vr0, r36;"
        "}"
        "{"
        "pseudo@0	@pseudo imm_0 = 32704;"
        "V0@(pr0)	%[res0] = sel vmsk0 vr13, r36;"
        "}"
        : [res0] "=x" (result0)
        : [input0] "x" (a),  [input1] "x" (b)
        :"vr4", "vr30", "vr10", "vr6", "vr12", "vr7", "vr15", "vr31", "vr1", "vr5", "vr0", "vr28", "vr13", "vr29", "vr2", "vr17", "vr16", "vr11", "vr14", "vr3", "r0", "r1", "r3", "r6", "r2", "vmsk3", "vmsk0", "vmsk5", "vmsk4", "vmsk7", "vmsk1", "vmsk2", "vmsk6", "pr1"
        );
    return result0;
}

#endif // _JNF_H_
